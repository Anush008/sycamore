import json
from abc import abstractmethod
from typing import Any, Optional, List, Dict, Tuple

from sycamore.query.operators.count import Count
from sycamore.query.operators.filter import Filter
from sycamore.query.operators.limit import Limit
from sycamore.query.operators.llmextract import LlmExtract
from sycamore.query.operators.llmfilter import LlmFilter
from sycamore.query.operators.llmgenerate import LlmGenerate
from sycamore.query.operators.loaddata import LoadData
from sycamore.query.operators.topk import TopK

from sycamore.query.execution.metrics import LunaLogger

from sycamore.query.execution.operations import (
    llm_generate_operation,
    llm_filter_operation,
    range_filter_operation,
    match_filter_operation,
    count_operation,
    llm_extract_operation,
    top_k_operation,
)
from sycamore.llms import OpenAI, OpenAIModels
from sycamore.utils.cache import S3Cache

from sycamore import DocSet, Context
from sycamore.query.operators.logical_operator import LogicalOperator
from sycamore.query.execution.physical_operator import PhysicalOperator, get_var_name, get_str_for_dict


class SycamoreOperator(PhysicalOperator):
    """
    This interface is a Sycamore platform implementation of a Logical Operator generated by the query planner.
    It serves 2 purposes:
    1. Execute the node using Sycamore tools (possibly lazy)
    2. Return a python script in string form that can be run to achieve the same result

    Args:
        context (Context): The Sycamore context to use.
        logical_node (Operator): The logical query plan node to execute. Contains runtime params based on type.
        query_id (str): Query id
        inputs (List[Any]): List of inputs required to execute the node. Varies based on node type.
    """

    def __init__(
        self,
        context: Context,
        logical_node: LogicalOperator,
        query_id: str,
        inputs: List[Any] = None,
        trace_dir: Optional[str] = None,
    ) -> None:
        super().__init__(logical_node, query_id, inputs)
        self.context = context
        self.trace_dir = trace_dir

    @abstractmethod
    def execute(self) -> Any:
        """
        execute the node
        :return: execution result, can be a Lazy DocSet plan, or executed result like a integer (for count)
        """
        pass

    @abstractmethod
    def script(self, input_var: Optional[str] = None, output_var: Optional[str] = None) -> Tuple[str, List[str]]:
        pass

    def get_node_args(self) -> Dict:
        return {"name": str(self.logical_node.node_id)}

    def get_execute_args(self) -> Dict:
        args = {
            "write_intermediate_data": True,
            "intermediate_datasink": LunaLogger,
            "intermediate_datasink_kwargs": {
                "query_id": self.query_id,
                "node_id": self.logical_node.node_id,
                "path": "none",
            },
        }
        if self.trace_dir:
            args["intermediate_datasink_kwargs"].update(
                {"makedirs": True, "verbose": True, "path": f"{self.trace_dir}/{self.query_id}/"}
            )
        args.update(self.get_node_args())
        return args


class SycamoreLoadData(SycamoreOperator):
    """
    Currently only supports an OpenSearch scan load implementation.
    Args:
        os_client_args (dict): OpenSearch client args passed to OpenSearchScan to initialize the client.
    """

    def __init__(
        self,
        context: Context,
        logical_node: LoadData,
        query_id: str,
        os_client_args: Dict,
        trace_dir: Optional[str] = None,
    ) -> None:
        super().__init__(context=context, logical_node=logical_node, query_id=query_id, trace_dir=trace_dir)
        self.os_client_args = os_client_args

    def execute(self) -> Any:
        assert self.logical_node.data and "index" in self.logical_node.data
        result = self.context.read.opensearch(
            os_client_args=self.os_client_args, index_name=self.logical_node.data["index"]
        )
        return result

    def script(self, input_var: Optional[str] = None, output_var: Optional[str] = None) -> Tuple[str, List[str]]:
        return (
            f"""
os_client_args = {self.os_client_args}
{output_var or get_var_name(self.logical_node)} = OpenSearchScan(
    os_client_args=os_client_args,
    index_name='{self.logical_node.data["index"]}',
    **{self.get_node_args()},
)
""",
            ["from sycamore.connectors.opensearch import OpenSearchScan"],
        )


class SycamoreLlmGenerate(SycamoreOperator):
    """
    Use an LLM to generate a response based on the user input question and provided result set.
    Args:
        s3_cache_path (str): Optional S3 path to use for caching
    """

    def __init__(
        self,
        context: Context,
        logical_node: LlmGenerate,
        query_id: str,
        inputs: List[Any] = None,
        trace_dir: Optional[str] = None,
        s3_cache_path: Optional[str] = None,
    ) -> None:
        super().__init__(context, logical_node, query_id, inputs, trace_dir=trace_dir)
        self.s3_cache_path = s3_cache_path

    def execute(self) -> Any:
        assert self.inputs and len(self.inputs) >= 1, "LlmGenerate requires at least 1 input node"
        result = llm_generate_operation(
            client=OpenAI(OpenAIModels.GPT_4O.value, cache=S3Cache(self.s3_cache_path) if self.s3_cache_path else None),
            question=self.logical_node.data.get("question"),
            result_description=self.logical_node.data["description"],
            result_data=self.inputs[0],
            **self.get_execute_args(),
        )
        return result

    def script(self, input_var: Optional[str] = None, output_var: Optional[str] = None) -> Tuple[str, List[str]]:
        cache_string = ""
        if self.s3_cache_path:
            cache_string = f", cache=S3Cache('{self.s3_cache_path}')"
        result = f"""
{output_var or get_var_name(self.logical_node)} = llm_generate_operation(
    client=OpenAI(OpenAIModels.GPT_4O.value{cache_string}),
    question='{self.logical_node.data.get("question")}',
    result_description='{self.logical_node.data["description"]}',
    result_data={input_var or get_var_name(self.logical_node.dependencies[0])}
)
"""
        return result, ["from sycamore.query.execution.operations import llm_generate_operation"]


class SycamoreLlmFilter(SycamoreOperator):
    """
    Use an LLM to filter records on a Docset.
    Args:
        s3_cache_path (str): Optional S3 path to use for caching
    """

    def __init__(
        self,
        context: Context,
        logical_node: LlmFilter,
        query_id: str,
        inputs: List[Any] = None,
        trace_dir: Optional[str] = None,
        s3_cache_path: Optional[str] = None,
    ) -> None:
        super().__init__(context, logical_node, query_id, inputs, trace_dir=trace_dir)
        self.s3_cache_path = s3_cache_path

    def execute(self) -> Any:
        assert self.inputs and len(self.inputs) == 1, "LlmFilter requires 1 input node"
        assert isinstance(self.inputs[0], DocSet), "LlmFilter requires a DocSet input"

        # load into local vars for Ray serialization magic
        s3_cache_path = self.s3_cache_path
        logical_node = self.logical_node

        result = llm_filter_operation(
            client=OpenAI(OpenAIModels.GPT_4O.value, cache=S3Cache(s3_cache_path) if s3_cache_path else None),
            docset=self.inputs[0],
            filter_question=logical_node.data.get("question"),
            field=logical_node.data.get("field"),
            messages=None,
            threshold=3,
            **self.get_node_args(),
        )
        return result

    def script(self, input_var: Optional[str] = None, output_var: Optional[str] = None) -> Tuple[str, List[str]]:
        cache_string = ""
        if self.s3_cache_path:
            cache_string = f", cache=S3Cache('{self.s3_cache_path}')"
        result = f"""
{output_var or get_var_name(self.logical_node)} = llm_filter_operation(
    client=OpenAI(OpenAIModels.GPT_4O.value{cache_string}),
    docset={input_var or get_var_name(self.logical_node.dependencies[0])},
    filter_question='{self.logical_node.data.get("question")}',
    field='{self.logical_node.data.get("field")}',
    threshold=3,
    **{self.get_node_args()},
)
"""
        return result, ["from sycamore.query.execution.operations import llm_filter_operation"]


class SycamoreFilter(SycamoreOperator):
    """
    Filter a DocSet
    """

    def __init__(
        self,
        context: Context,
        logical_node: Filter,
        query_id: str,
        inputs: List[Any] = None,
        trace_dir: Optional[str] = None,
    ) -> None:
        super().__init__(context, logical_node, query_id, inputs, trace_dir=trace_dir)

    def execute(self) -> Any:
        assert self.inputs and len(self.inputs) == 1, "Filter requires 1 input node"
        assert isinstance(self.inputs[0], DocSet), "Filter requires a DocSet input"
        # load into local vars for Ray serialization magic
        logical_node = self.logical_node

        if logical_node.data.get("rangeFilter"):

            result = self.inputs[0].filter(
                lambda doc: range_filter_operation(
                    doc=doc,
                    field=logical_node.data.get("field"),
                    start=logical_node.data.get("start"),
                    end=logical_node.data.get("end"),
                    date=logical_node.data.get("date"),
                ),
                **self.get_node_args(),
            )
        else:
            result = self.inputs[0].filter(
                lambda doc: match_filter_operation(
                    doc=doc,
                    query=logical_node.data.get("query"),
                    field=logical_node.data.get("field"),
                ),
                **self.get_node_args(),
            )
        return result

    def script(self, input_var: Optional[str] = None, output_var: Optional[str] = None) -> Tuple[str, List[str]]:
        script = ""
        imports = []
        if self.logical_node.data.get("rangeFilter"):
            script = f"""
{output_var or get_var_name(self.logical_node)} = {input_var or get_var_name(self.logical_node.dependencies[0])}.filter(
    lambda doc: range_filter_operation(
        doc=doc,
        field='{self.logical_node.data.get("field")}',
        start='{self.logical_node.data.get("start")}',
        end='{self.logical_node.data.get("end")}',
        date='{self.logical_node.data.get("date")}',
    ),
    **{self.get_node_args()},
)
            """
            imports = ["from sycamore.query.execution.operations import range_filter_operation"]
        else:
            script = f"""
{output_var or get_var_name(self.logical_node)} = {input_var or get_var_name(self.logical_node.dependencies[0])}.filter(
    lambda doc: match_filter_operation(
        doc=doc,
        query='{self.logical_node.data.get("query")}',
        field='{self.logical_node.data.get("field")}',
    ),
    **{self.get_node_args()},
)
"""
            imports = ["from sycamore.query.execution.operations import match_filter_operation"]
        return script, imports


class SycamoreCount(SycamoreOperator):
    """
    Count documents in a DocSet. Can do a unique count optionally.
    """

    def __init__(
        self,
        context: Context,
        logical_node: Count,
        query_id: str,
        inputs: List[Any] = None,
        trace_dir: Optional[str] = None,
    ) -> None:
        super().__init__(context, logical_node, query_id, inputs, trace_dir=trace_dir)

    def execute(self) -> Any:
        assert self.inputs and len(self.inputs) == 1, "Count requires 1 input node"
        assert isinstance(self.inputs[0], DocSet), "Count requires a DocSet input"

        # load into local vars for Ray serialization magic
        logical_node = self.logical_node

        result = count_operation(
            docset=self.inputs[0],
            field=logical_node.data.get("field"),
            primaryField=logical_node.data.get("primaryField"),
            **self.get_execute_args(),
        )
        return result

    def script(self, input_var: Optional[str] = None, output_var: Optional[str] = None) -> Tuple[str, List[str]]:
        script = ""
        imports = ["from sycamore.query.execution.operations import count_operation"]
        script = f"""
{output_var or get_var_name(self.logical_node)} = count_operation(
    docset={input_var or get_var_name(self.logical_node.dependencies[0])},
    """
        if self.logical_node.data.get("field"):
            script += f"""field='{self.logical_node.data.get("field")}',
    """
        if self.logical_node.data.get("primaryField"):
            script += f"""primaryField='{self.logical_node.data.get("primaryField")}',
    """
            script += f"""**{get_str_for_dict(self.get_execute_args())},
)
"""
        return script, imports


class SycamoreLlmExtract(SycamoreOperator):
    """
    Use an LLM to extract information from your data. The data is available for downstream tasks to consume.
    Args:
        s3_cache_path (str): Optional S3 path to use for caching
    """

    def __init__(
        self,
        context: Context,
        logical_node: LlmExtract,
        query_id: str,
        inputs: List[Any] = None,
        trace_dir: Optional[str] = None,
        s3_cache_path: Optional[str] = None,
    ) -> None:
        super().__init__(context, logical_node, query_id, inputs, trace_dir=trace_dir)
        self.s3_cache_path = s3_cache_path

    def execute(self) -> Any:
        assert self.inputs and len(self.inputs) == 1, "LlmExtract requires 1 input node"
        assert isinstance(self.inputs[0], DocSet), "LlmExtract requires a DocSet input"
        # load into local vars for Ray serialization magic
        s3_cache_path = self.s3_cache_path
        logical_node = self.logical_node

        result = self.inputs[0].map(
            lambda doc: llm_extract_operation(
                client=OpenAI(OpenAIModels.GPT_4O.value, cache=S3Cache(s3_cache_path) if s3_cache_path else None),
                doc=doc,
                question=logical_node.data.get("question"),
                new_field=logical_node.data.get("newField"),
                field=logical_node.data.get("field"),
                format=logical_node.data.get("format"),
                discrete=logical_node.data.get("discrete"),
            ),
            **self.get_node_args(),
        )

        # filter out docs with the extracted field labeled as "None"
        def filterNone(doc):
            return doc.properties[logical_node.data.get("newField")] != "None"

        result = result.filter(lambda doc: filterNone(doc))
        return result

    def script(self, input_var: Optional[str] = None, output_var: Optional[str] = None) -> Tuple[str, List[str]]:
        cache_string = ""
        if self.s3_cache_path:
            cache_string = f", cache=S3Cache('{self.s3_cache_path}')"
        result = f"""
{output_var or get_var_name(self.logical_node)} = {input_var or get_var_name(self.logical_node.dependencies[0])}.map(
    lambda doc: llm_extract_operation(
        client=OpenAI(OpenAIModels.GPT_4O.value{cache_string}),
        doc=doc,
        question='{self.logical_node.data.get("question")}',
        new_field='{self.logical_node.data.get("newField")}',
        field='{self.logical_node.data.get("field")}',
        format='{self.logical_node.data.get("format")}',
        discrete='{self.logical_node.data.get("discrete")}',
    ),
    **{self.get_node_args()},
)
"""
        return result, ["from sycamore.query.execution.operations import llm_extract_operation"]


class SycamoreSort(SycamoreOperator):
    """
    Sort a DocSet on a given key.
    """

    def __init__(
        self,
        context: Context,
        logical_node: LogicalOperator,
        query_id: str,
        inputs: List[Any] = None,
        trace_dir: Optional[str] = None,
    ) -> None:
        super().__init__(context, logical_node, query_id, inputs, trace_dir=trace_dir)

    def execute(self) -> Any:
        assert self.inputs and len(self.inputs) == 1, "Sort requires 1 input node"
        assert isinstance(self.inputs[0], DocSet), "Sort requires a DocSet input"

        # load into local vars for Ray serialization magic
        logical_node = self.logical_node

        result = self.inputs[0].sort(
            descending=logical_node.data.get("descending"), field=logical_node.data.get("field")
        )

        return result

    def script(self, input_var: Optional[str] = None, output_var: Optional[str] = None) -> Tuple[str, List[str]]:
        result = f"""
{output_var or get_var_name(self.logical_node)} = {input_var or get_var_name(self.logical_node.dependencies[0])}.sort(
    descending={self.logical_node.data.get("descending")},
    field='{self.logical_node.data.get("field")}'
)
"""
        return result, []


class SycamoreTopK(SycamoreOperator):
    """
    Return the Top-K values from a DocSet
    Args:
        s3_cache_path (str): Optional S3 path to use for caching when using an LLM
    """

    def __init__(
        self,
        context: Context,
        logical_node: TopK,
        query_id: str,
        inputs: List[Any] = None,
        trace_dir: Optional[str] = None,
        s3_cache_path: Optional[str] = None,
    ) -> None:
        super().__init__(context, logical_node, query_id, inputs, trace_dir=trace_dir)
        self.s3_cache_path = s3_cache_path

    def execute(self) -> Any:
        assert self.inputs and len(self.inputs) == 1, "TopK requires 1 input node"
        assert isinstance(self.inputs[0], DocSet), "TopK requires a DocSet input"
        # load into local vars for Ray serialization magic
        s3_cache_path = self.s3_cache_path
        logical_node = self.logical_node

        result = top_k_operation(
            client=OpenAI(OpenAIModels.GPT_4O.value, cache=S3Cache(s3_cache_path) if s3_cache_path else None),
            docset=self.inputs[0],
            field=logical_node.data.get("field"),
            k=logical_node.data.get("K"),
            description=logical_node.data.get("description"),
            descending=logical_node.data.get("descending"),
            use_llm=logical_node.data.get("useLLM"),
            unique_field=logical_node.data.get("primaryField"),
            **self.get_execute_args(),
        )
        return result

    def script(self, input_var: Optional[str] = None, output_var: Optional[str] = None) -> Tuple[str, List[str]]:
        cache_string = ""
        if self.s3_cache_path:
            cache_string = f", cache=S3Cache('{self.s3_cache_path}')"
        result = f"""
{output_var or get_var_name(self.logical_node)} = top_k_operation(
    client=OpenAI(OpenAIModels.GPT_4O.value{cache_string}),
    docset={input_var or get_var_name(self.logical_node.dependencies[0])},
    field='{self.logical_node.data.get("field")}',
    k='{self.logical_node.data.get("K")}',
    description='{self.logical_node.data.get("description")}',
    descending='{self.logical_node.data.get("descending")}',
    use_llm='{self.logical_node.data.get("useLLM")}',
    unique_field='{self.logical_node.data.get("primaryField")}',
    **{self.get_execute_args()},
)
"""
        return result, ["from sycamore.query.execution.operations import top_k_operation"]


class SycamoreLimit(SycamoreOperator):
    """
    Limit the number of results on a DocSet
    """

    def __init__(
        self,
        context: Context,
        logical_node: Limit,
        query_id: str,
        inputs: List[Any] = None,
        trace_dir: Optional[str] = None,
    ) -> None:
        super().__init__(context, logical_node, query_id, inputs, trace_dir=trace_dir)

    def execute(self) -> Any:
        assert self.inputs and len(self.inputs) == 1, "Limit requires 1 input node"
        assert isinstance(self.inputs[0], DocSet), "Limit requires a DocSet input"

        # load into local vars for Ray serialization magic
        logical_node = self.logical_node

        result = self.inputs[0].limit(logical_node.data.get("K"), **self.get_execute_args())
        return result

    def script(self, input_var: Optional[str] = None, output_var: Optional[str] = None) -> Tuple[str, List[str]]:
        result = f"""
{output_var or get_var_name(self.logical_node)} = {input_var or get_var_name(self.logical_node.dependencies[0])}.limit(
    {self.logical_node.data.get("K")},
    **{self.get_execute_args()},
)
"""
        return result, []
