{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dba62559-5150-4e90-a177-1a0acc8322fa",
   "metadata": {},
   "source": [
    "## NTSB demo bolierplate (imports & utilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49fef0f-2ba8-4155-b428-c8e28605ba58",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f742c8f7-c46b-4706-af22-1a271d531926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.data import Document, Table\n",
    "from sycamore.functions import HuggingFaceTokenizer\n",
    "from sycamore.llms import OpenAI, OpenAIModels\n",
    "from sycamore.transforms.extract_schema import OpenAISchemaExtractor, OpenAIPropertyExtractor\n",
    "from sycamore.transforms.extract_entity import OpenAIEntityExtractor\n",
    "#from sycamore.transforms.merge_elements import GreedySectionMerger\n",
    "from sycamore.transforms.partition import UnstructuredPdfPartitioner, SycamorePartitioner\n",
    "from sycamore.transforms.embed import SentenceTransformerEmbedder\n",
    "from sycamore.transforms.summarize_images import SummarizeImages\n",
    "from sycamore.utils.pdf_utils import show_pages\n",
    "\n",
    "from sycamore.data import BoundingBox, Document, Element, TableElement\n",
    "from sycamore.functions.document import split_and_convert_to_image, DrawBoxes\n",
    "import sycamore\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from dateutil import parser\n",
    "\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "from sycamore.transforms.query import OpenSearchQueryExecutor\n",
    "from sycamore.data import OpenSearchQuery\n",
    "from sycamore.utils.time_trace import timetrace\n",
    "\n",
    "import json\n",
    "\n",
    "import os\n",
    "import sys \n",
    "\n",
    "import PIL.Image\n",
    "from io import BytesIO\n",
    "from IPython.display import Image \n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1a084-0810-4cdd-9af2-e6b3f57dafd2",
   "metadata": {},
   "source": [
    "### Location standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8c62093-a5fa-4064-b5a9-a6173281f196",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {\n",
    "    \"AK\": \"Alaska\", \"AL\": \"Alabama\", \"AR\": \"Arkansas\", \"AZ\": \"Arizona\", \"CA\": \"California\",\n",
    "    \"CO\": \"Colorado\", \"CT\": \"Connecticut\", \"DC\": \"District of Columbia\", \"DE\": \"Delaware\",\n",
    "    \"FL\": \"Florida\", \"GA\": \"Georgia\", \"HI\": \"Hawaii\", \"IA\": \"Iowa\", \"ID\": \"Idaho\",\n",
    "    \"IL\": \"Illinois\", \"IN\": \"Indiana\", \"KS\": \"Kansas\", \"KY\": \"Kentucky\", \"LA\": \"Louisiana\",\n",
    "    \"MA\": \"Massachusetts\", \"MD\": \"Maryland\", \"ME\": \"Maine\", \"MI\": \"Michigan\", \"MN\": \"Minnesota\",\n",
    "    \"MO\": \"Missouri\", \"MS\": \"Mississippi\", \"MT\": \"Montana\", \"NC\": \"North Carolina\", \"ND\": \"North Dakota\",\n",
    "    \"NE\": \"Nebraska\", \"NH\": \"New Hampshire\", \"NJ\": \"New Jersey\", \"NM\": \"New Mexico\",\n",
    "    \"NV\": \"Nevada\", \"NY\": \"New York\", \"OH\": \"Ohio\", \"OK\": \"Oklahoma\", \"OR\": \"Oregon\",\n",
    "    \"PA\": \"Pennsylvania\", \"RI\": \"Rhode Island\", \"SC\": \"South Carolina\", \"SD\": \"South Dakota\",\n",
    "    \"TN\": \"Tennessee\", \"TX\": \"Texas\", \"UT\": \"Utah\", \"VA\": \"Virginia\", \"VT\": \"Vermont\",\n",
    "    \"WA\": \"Washington\", \"WI\": \"Wisconsin\", \"WV\": \"West Virginia\", \"WY\": \"Wyoming\"\n",
    "}\n",
    "\n",
    "## standardize timestamps\n",
    "def standardize_date(doc: Document) -> Document:\n",
    "\n",
    "    try:\n",
    "        if 'dateAndTime' in doc.properties['entity']:\n",
    "            doc.properties['entity']['dateTime'] = doc.properties['entity']['dateAndTime']\n",
    "            del doc.properties['entity']['dateAndTime']\n",
    "\n",
    "        raw_date: str = doc.properties['entity']['dateTime']\n",
    "\n",
    "        raw_date = raw_date.replace(\"Local\", \"\")\n",
    "        parsed_date = parser.parse(raw_date)\n",
    "        extracted_date = parsed_date.date()\n",
    "        doc.properties['entity']['day'] = extracted_date\n",
    "        return doc\n",
    "        \n",
    "    except Exception as e:\n",
    "        # date not extracted propoerly, don't do anything\n",
    "        return doc\n",
    "\n",
    "\n",
    "def standardize_location(doc: Document) -> Document:\n",
    "    if \"location\" not in doc.properties['entity']:\n",
    "        return doc\n",
    "\n",
    "    try:\n",
    "        raw_loc: str = doc.properties['entity']['location']\n",
    "        city, state = raw_loc.split(',')\n",
    "        std_loc = city + ', ' + standardize_state(state)\n",
    "        doc.properties['entity']['location'] = std_loc\n",
    "        return doc\n",
    "    except Exception as e:\n",
    "        # location not extracted propoerly, don't do anything\n",
    "        return doc\n",
    "    \n",
    "\n",
    "def standardize_state(state: str) -> str:\n",
    "    clean_state = state.lstrip().rstrip().upper()\n",
    "    if clean_state in state_dict:\n",
    "        return state_dict[clean_state]\n",
    "    else:\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a994956-bad3-4d0f-a51f-641770cfc662",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11c8ef7c-e797-42b9-86ad-ece38ac33229",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m font_path\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# paths for saving processed docs and images\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m pickle_root_sycamore \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/admin/sycamore/data/ntsb/docset-merged/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m img_root_sycamore \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/admin/sycamore/data/ntsb-s/img/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m pickle_root_unstruct \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/admin/sycamore/data/ntsb-u/docset/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "# This font is used for the labels in the visual represenation.\n",
    "font_path= \"/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf\"\n",
    "\n",
    "## pickling utilities\n",
    "def wall_time(fn):\n",
    "    # wall_time(lambda: time.sleep(1))\n",
    "    start = time.time_ns()\n",
    "    fn()\n",
    "    end = time.time_ns()\n",
    "    print(\"Elapsed time:\", (end - start) / 1.0e9)\n",
    "    \n",
    "def pickle_doc(doc: Document) -> bytes:\n",
    "    return pickle.dumps(doc)\n",
    "\n",
    "def pickle_name(doc: Document, extension = None):\n",
    "    return str(doc.doc_id) + \".pickle\"\n",
    "\n",
    "def unpickle_doc(pdoc: Document) -> list[Document]:\n",
    "    doc = convert_schema(pickle.loads(pdoc.binary_representation))\n",
    "    return [doc]\n",
    "\n",
    "def write_out_docset(pickle_root, docset):\n",
    "    wall_time(lambda: docset.write.files(pickle_root, doc_to_bytes_fn=pickle_doc, filename_fn=pickle_name))\n",
    "\n",
    "def read_in_docset(pickle_root, ctx):\n",
    "    pickled_docset = ctx.read.binary(str(pickle_root), binary_format=\"pickle\")\n",
    "    unpickled_docset = pickled_docset.flat_map(unpickle_doc)\n",
    "    wall_time(lambda: unpickled_docset.count())\n",
    "    return unpickled_docset\n",
    "\n",
    "\n",
    "## image generation and saving to disk\n",
    "def image_page_filename(doc: Document):\n",
    "    path = Path(doc.properties[\"path\"])\n",
    "    base_name = \".\".join(path.name.split(\".\")[0:-1])\n",
    "    page_num = doc.properties[\"page_number\"]\n",
    "    return f\"{base_name}_page_{page_num}.png\"\n",
    "\n",
    "\n",
    "def list_files(directory):\n",
    "    # Get a list of all files and directories in the specified directory\n",
    "    all_entries = os.listdir(directory)\n",
    "    \n",
    "    # Filter out only the files\n",
    "    files = [os.path.join(directory, entry) for entry in all_entries if os.path.isfile(os.path.join(directory, entry))]\n",
    "    \n",
    "    return files\n",
    "\n",
    "def enumerate_images_and_tables(m_pages: list[Document]):\n",
    "    num_pages = len(m_pages)\n",
    "    for i in range(0, num_pages):\n",
    "        m_page = m_pages[i]\n",
    "        print(\"Path: \", m_page.properties['path'], \"Page: \", m_page.properties['page_number'])\n",
    "        for e in m_page.elements:\n",
    "            if e.type == \"Image\":\n",
    "                print(\"Image summary: \", e.properties['summary'], \"\\n\")\n",
    "                print()\n",
    "            if e.type == \"table\":\n",
    "                display(HTML(e.table.to_html()))\n",
    "                print()\n",
    "\n",
    "def display_page_and_table_properties(some_pages: list[Document]):\n",
    "    for m_page in some_pages:\n",
    "        print(\"Page props: \")\n",
    "        display(m_page.properties['entity'])\n",
    "        print()\n",
    "#    for k in m_page.properties.keys():\n",
    "#        print(\"Page: \", k)\n",
    "        for e in m_page.elements:\n",
    "            if \"table\" in e.type:\n",
    "                print(\"Element Type: \", e.type)\n",
    "                print(\"Element Properties: \", json.dumps(e.properties, indent=2))\n",
    "                display(HTML(e.text_representation))\n",
    "                #print(e.keys())\n",
    "                #display(HTML(e.table.to_html()))\n",
    "                # print(e.table.to_csv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdf1e2d0-5691-40ea-8b68-dff794f85b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timetrace(\"LLMGen\")\n",
    "def llm_generate_with_retries(llm, prompt_kwargs, llm_kwargs, max_retries=5):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            llm_response = llm.generate(prompt_kwargs=prompt_kwargs, llm_kwargs=llm_kwargs).content\n",
    "            new_props = json.loads(llm_response)\n",
    "            return new_props\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            if attempt == max_retries - 1:\n",
    "                raise e\n",
    "                \n",
    "\"\"\"\n",
    "For table elements, ask the llm to to extract a JSON formatted key-value object from the table's csv string.\n",
    "\"\"\"\n",
    "@timetrace(\"ExtractProp\")\n",
    "def extract_table_as_properties(doc: Document) -> Document:\n",
    "    PROMPT = \"\"\"\n",
    "    You are given a csv representing either a single column, or multi-column table.\n",
    "    Instructions:\n",
    "    1. Parse the table and return a flattened JSON object representing the key-value pairs of properties defined in the table.\n",
    "    2. Do not return nested objects, keep the dictionary only 1 level deep. The only valid value types are numbers, strings, and lists.\n",
    "    3. If you find multiple fields defined in a row, feel free to split them into separate properties.\n",
    "    4. Use camelCase for the key names\n",
    "    5. For fields where the values are in standard measurement units like miles, nautical miles, knots, celsius\n",
    "       - include the unit in the key name and only set the numeric value as the value.\n",
    "       - e.g. \"Wind Speed: 9 knots\" should become windSpeedInKnots: 9, \"Temperature: 3°C\" should become temperatureInC: 3\n",
    "    \"\"\"\n",
    "    llm_kwargs = {\n",
    "        \"response_format\":{ \"type\": \"json_object\" }\n",
    "    }\n",
    "    sys.stderr.write(doc.properties['path'])\n",
    "    if not doc.elements:\n",
    "        return\n",
    "    # we are going to use the first table's properties as document level properties\n",
    "    top_level_table = None\n",
    "    for element in doc.elements:\n",
    "        if element.type != \"table\" or element.table == None:\n",
    "            continue\n",
    "        if not top_level_table:\n",
    "            top_level_table = element\n",
    "        prompt = PROMPT\n",
    "        prompt += \"\\n\" + element.table.to_csv()\n",
    "        prompt_kwargs = {\n",
    "            \"prompt\": prompt\n",
    "        }\n",
    "#        llm_response = llm.generate(prompt_kwargs=prompt_kwargs, llm_kwargs=llm_kwargs).content\n",
    "        new_props = llm_generate_with_retries(llm, prompt_kwargs, llm_kwargs, max_retries=5)\n",
    "#        print(new_props)\n",
    "        if new_props:\n",
    "            element.properties.update(new_props)\n",
    "        else:\n",
    "            element.properties.update({\"Foo\": \"Bar\"})\n",
    "            \n",
    "    doc.properties[\"entity\"] = top_level_table.properties.copy()\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c63c0f-e086-46cc-8eaa-ee6ed5668795",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TIMETRACE'] = \"/tmp/_ntsb_demo_jp\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be259923-c472-42b9-85d9-3c7603cabb53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
