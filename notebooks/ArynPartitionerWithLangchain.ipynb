{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66172e88-fb6a-4dbd-a04e-b3494a4d2beb",
   "metadata": {},
   "source": [
    "##### In this example, we will write the output of the Sycamore from pdf to `FAISS` vector database using Langchain.\n",
    "\n",
    "\n",
    "##### The Aryn Partitioner in this job is configured to use the Aryn Partitioning Service to provide fast, GPU-powered performance. Go to [aryn.ai/sign-up ](aryn.ai/sign-up) to get a free API key for the service. This is the recommended configuration.\n",
    "\n",
    "##### You can also run the Aryn Partitioner locally by setting `use_partitioning_service` to `False`. Though you can use CPU to run the Aryn Partitioner, it is recommended to use an NVIDIA GPU for good performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6513539e-ecdd-4853-a6c7-c4e3811afd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import sycamore \n",
    "from sycamore.data import Document\n",
    "from sycamore.transforms.partition import ArynPartitioner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa29e664-507f-4b78-83a4-af83c069d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements to be added \n",
    "#  faiss-cpu==1.7.4\n",
    "#  langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0552a79-1364-4dec-bd61-b1df6a309171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.utils.aryn_config import ArynConfig, DEFAULT_PATH\n",
    "assert ArynConfig.get_aryn_api_key() != \"\", f\"Unable to find aryn API key.  Looked in {DEFAULT_PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20415414-d5bb-4a33-acc0-6feda0538b49",
   "metadata": {},
   "source": [
    "if the above assertion fails, you can either set the environment variable ARYN_API_KEY and restart jupyter\n",
    "or make a yaml file at the specified path in the assertion error that looks like:\n",
    "\n",
    "```\n",
    "aryn_token: \"YOUR-ARYN-API-KEY\"\n",
    "```\n",
    "\n",
    "It is unsafe, but if neither of those options work, you can put it in this notebook with\n",
    "```\n",
    "import os\n",
    "os.environ[\"ARYN_API_KEY\"] = \"UNSAFE-ARYN-API-KEY-LOCATION\" \n",
    "```\n",
    "\n",
    "but beware that it is easy to accidentally commit the notebook file and have it include your key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8819ce9f-062f-43df-bb39-7029816179f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"/.dockerenv\"):\n",
    "    # Running in Docker.\n",
    "    work_dir = \"/app/work/docker_volume\"\n",
    "else:\n",
    "    # Running outside of docker. This will land under notebooks/data/\n",
    "    work_dir = \"./data\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720c6a38-7ca0-4a79-8831-ba12fc9b47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(work_dir, exist_ok = True)\n",
    "metadata = {}\n",
    "for f in [\"2306.07303\"]:\n",
    "    path = os.path.join(work_dir, f + \".pdf\")\n",
    "    url = os.path.join(\"http://arxiv.org/pdf\", f)\n",
    "    if not Path(path).is_file():\n",
    "        print(\"Downloading {} to {}\".format(url, path))\n",
    "        subprocess.run([\"curl\", \"-o\", path, url])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f1b288-e98a-42fe-89f4-66dc37956a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(pid=31845) INFO:root:Spurious log 1: Verifying that log messages are propogated\n",
      "(raylet) [2024-08-01 00:42:47,160 E 13985 13985] (raylet) node_manager.cc:2963: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b8427498e2278db9e5acca81e16481a65fdd58b4e4c17ff05281581f, IP: 10.0.20.45) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.20.45`\n",
      "(raylet) \n",
      "(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "(pid=709) INFO:root:Spurious log 1: Verifying that log messages are propogated\n",
      "(raylet) [2024-08-01 00:43:47,162 E 13985 13985] (raylet) node_manager.cc:2963: 6 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b8427498e2278db9e5acca81e16481a65fdd58b4e4c17ff05281581f, IP: 10.0.20.45) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.20.45`\n",
      "(raylet) \n",
      "(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "(raylet) [2024-08-01 00:44:47,641 E 13985 13985] (raylet) node_manager.cc:2963: 10 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b8427498e2278db9e5acca81e16481a65fdd58b4e4c17ff05281581f, IP: 10.0.20.45) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.20.45`\n",
      "(raylet) \n",
      "(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "(raylet) [2024-08-01 00:45:47,643 E 13985 13985] (raylet) node_manager.cc:2963: 11 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b8427498e2278db9e5acca81e16481a65fdd58b4e4c17ff05281581f, IP: 10.0.20.45) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.20.45`\n",
      "(raylet) \n",
      "(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "(pid=12522) INFO:root:Spurious log 1: Verifying that log messages are propogated\n",
      "(raylet) [2024-08-01 00:46:47,644 E 13985 13985] (raylet) node_manager.cc:2963: 7 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b8427498e2278db9e5acca81e16481a65fdd58b4e4c17ff05281581f, IP: 10.0.20.45) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.20.45`\n",
      "(raylet) \n",
      "(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "(raylet) [2024-08-01 00:47:47,646 E 13985 13985] (raylet) node_manager.cc:2963: 8 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b8427498e2278db9e5acca81e16481a65fdd58b4e4c17ff05281581f, IP: 10.0.20.45) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.20.45`\n",
      "(raylet) \n",
      "(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "(raylet) [2024-08-01 00:48:48,066 E 13985 13985] (raylet) node_manager.cc:2963: 9 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b8427498e2278db9e5acca81e16481a65fdd58b4e4c17ff05281581f, IP: 10.0.20.45) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.20.45`\n",
      "(raylet) \n",
      "(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "(raylet) [2024-08-01 00:49:48,068 E 13985 13985] (raylet) node_manager.cc:2963: 8 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b8427498e2278db9e5acca81e16481a65fdd58b4e4c17ff05281581f, IP: 10.0.20.45) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.20.45`\n",
      "(raylet) \n",
      "(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "(raylet) [2024-08-01 00:50:48,071 E 13985 13985] (raylet) node_manager.cc:2963: 9 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b8427498e2278db9e5acca81e16481a65fdd58b4e4c17ff05281581f, IP: 10.0.20.45) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.20.45`\n",
      "(raylet) \n",
      "(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "(raylet) [2024-08-01 00:51:49,151 E 13985 13985] (raylet) node_manager.cc:2963: 9 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b8427498e2278db9e5acca81e16481a65fdd58b4e4c17ff05281581f, IP: 10.0.20.45) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.20.45`\n",
      "(raylet) \n",
      "(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "(raylet) [2024-08-01 00:52:49,153 E 13985 13985] (raylet) node_manager.cc:2963: 8 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b8427498e2278db9e5acca81e16481a65fdd58b4e4c17ff05281581f, IP: 10.0.20.45) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.20.45`\n",
      "(raylet) \n",
      "(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "(raylet) [2024-08-01 00:53:49,155 E 13985 13985] (raylet) node_manager.cc:2963: 9 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b8427498e2278db9e5acca81e16481a65fdd58b4e4c17ff05281581f, IP: 10.0.20.45) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.20.45`\n",
      "(raylet) \n",
      "(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "(raylet) [2024-08-01 00:54:49,156 E 13985 13985] (raylet) node_manager.cc:2963: 8 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b8427498e2278db9e5acca81e16481a65fdd58b4e4c17ff05281581f, IP: 10.0.20.45) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.20.45`\n",
      "(raylet) \n",
      "(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "(raylet) [2024-08-01 00:55:49,158 E 13985 13985] (raylet) node_manager.cc:2963: 8 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b8427498e2278db9e5acca81e16481a65fdd58b4e4c17ff05281581f, IP: 10.0.20.45) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.20.45`\n",
      "(raylet) \n",
      "(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "(raylet) [2024-08-01 00:56:49,164 E 13985 13985] (raylet) node_manager.cc:2963: 9 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b8427498e2278db9e5acca81e16481a65fdd58b4e4c17ff05281581f, IP: 10.0.20.45) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.20.45`\n",
      "(raylet) \n",
      "(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "(raylet) [2024-08-01 00:57:49,166 E 13985 13985] (raylet) node_manager.cc:2963: 8 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b8427498e2278db9e5acca81e16481a65fdd58b4e4c17ff05281581f, IP: 10.0.20.45) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.20.45`\n",
      "(raylet) \n",
      "(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "(raylet) [2024-08-01 00:58:49,167 E 13985 13985] (raylet) node_manager.cc:2963: 8 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b8427498e2278db9e5acca81e16481a65fdd58b4e4c17ff05281581f, IP: 10.0.20.45) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.0.20.45`\n",
      "(raylet) \n",
      "(raylet) Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(raylet) The autoscaler failed with the following error:\n",
      "Terminated with signal 15\n",
      "  File \"/home/ec2-user/.cache/pypoetry/virtualenvs/sycamore-monorepo--FwAn7mM-py3.10/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.py\", line 709, in <module>\n",
      "    monitor.run()\n",
      "  File \"/home/ec2-user/.cache/pypoetry/virtualenvs/sycamore-monorepo--FwAn7mM-py3.10/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.py\", line 584, in run\n",
      "    self._run()\n",
      "  File \"/home/ec2-user/.cache/pypoetry/virtualenvs/sycamore-monorepo--FwAn7mM-py3.10/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.py\", line 438, in _run\n",
      "    time.sleep(AUTOSCALER_UPDATE_INTERVAL_S)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = sycamore.init()\n",
    "work_dirs = ['./data/2306.07303.pdf']\n",
    "pdf_docset = context.read.binary(work_dirs, binary_format=\"pdf\")\n",
    "\n",
    "\n",
    "partitioned_docset = pdf_docset.partition(\n",
    "    partitioner=ArynPartitioner(threshold=0.35, extract_table_structure=True) \n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cd8730-5cd8-4100-b9f1-38918f214f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for doc in partitioned_docset.take_all():\n",
    "    for doci in doc.elements:\n",
    "        if doci.type == \"table\":\n",
    "            text +=  doci['table'].to_csv()\n",
    "        elif doci.text_representation:\n",
    "            text +=  doci.text_representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445147c2-4cbd-4a9e-8a2e-0576e869611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_overlap = 200,\n",
    "    chunk_size = 1000,\n",
    "    length_function = len \n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "faiss_index = FAISS.from_texts(chunks, embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae029c87-c20d-41a7-b437-54c6da8be46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def questionAnswering(user_question):\n",
    "    docs = faiss_index.similarity_search(user_question, k=5)\n",
    "    llm = OpenAI()\n",
    "    chain = load_qa_chain(llm, chain_type= \"stuff\")\n",
    "    with get_openai_callback() as cb:\n",
    "        response = chain.run(input_documents=docs, question=user_question)\n",
    "        print(cb)\n",
    "        print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1d3a5b-3427-4066-a4d0-861f83e9c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionAnswering(\"What is transformer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2464bf-77f5-4595-8281-eb695d8f3baa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
