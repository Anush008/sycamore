{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66172e88-fb6a-4dbd-a04e-b3494a4d2beb",
   "metadata": {},
   "source": [
    "##### The Aryn Partitioner is configured to use the Aryn Partitioning Service to provide fast, GPU-powered performance. Go to [aryn.ai/sign-up ](aryn.ai/sign-up) to get a free API key for the service. You can also run the Aryn Partitioner locally by changing `use_partitioning_service` to `False`. Though you can use CPU to run the Aryn Partitioner, it is recommended to use an NVIDIA GPU for good performance.\n",
    "\n",
    "\n",
    "##### In this example, we will write the output of the Sycamore from pdf to Vector Store of Langchain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6513539e-ecdd-4853-a6c7-c4e3811afd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "import os\n",
    "\n",
    "import sycamore \n",
    "from sycamore.data import Document\n",
    "from sycamore.transforms.partition import ArynPartitioner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20415414-d5bb-4a33-acc0-6feda0538b49",
   "metadata": {},
   "source": [
    "Replace the `aryn_api_key` with your key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0552a79-1364-4dec-bd61-b1df6a309171",
   "metadata": {},
   "outputs": [],
   "source": [
    "aryn_api_key = 'aryn-api-key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa29e664-507f-4b78-83a4-af83c069d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements to be added \n",
    "#  faiss-cpu==1.7.4\n",
    "#  langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52f1b288-e98a-42fe-89f4-66dc37956a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Map(BinaryScan._to_document)->MapBatches(_wrap) pid=23614) The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    }
   ],
   "source": [
    "context = sycamore.init()\n",
    "work_dirs = ['./data/2306.07303.pdf']\n",
    "pdf_docset = context.read.binary(work_dirs, binary_format=\"pdf\")\n",
    "\n",
    "\n",
    "partitioned_docset = pdf_docset.partition(\n",
    "    partitioner=ArynPartitioner(threshold=0.35, use_ocr = False ,batch_at_a_time=True,  extract_table_structure=True, aryn_api_key = aryn_api_key) \n",
    "    ,num_gpus=0.1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8cd8730-5cd8-4100-b9f1-38918f214f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 20:38:02,238\tINFO set_read_parallelism.py:115 -- Using autodetected parallelism=8 for stage ReadBinary to satisfy parallelism at least twice the available number of CPUs (4).\n",
      "2024-07-11 20:38:02,239\tINFO set_read_parallelism.py:122 -- To satisfy the requested parallelism of 8, each read task output is split into 8 smaller blocks.\n",
      "2024-07-11 20:38:02,239\tINFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadBinary] -> TaskPoolMapOperator[Map(BinaryScan._to_document)->MapBatches(_wrap)]\n",
      "2024-07-11 20:38:02,240\tINFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2024-07-11 20:38:02,241\tINFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'Page-header', 'bbox': [0.09452253453871783, 0.016898599104447798, 0.703795166015625, 0.04355328646573153], 'properties': {'score': 0.6613875031471252, 'page_number': 1}, 'text_representation': 'This work has been submitted to the Expert Systems With Applications journal (Elsevier) for\\npossible publication\\n'}\n",
      "{'type': 'Title', 'bbox': [0.19320326861213236, 0.0794008359042081, 0.8112643612132353, 0.12149649880149148], 'properties': {'score': 0.6311418414115906, 'page_number': 1}, 'text_representation': 'A COMPREHENSIVE SURVEY ON APPLICATIONS OF\\nTRANSFORMERS FOR DEEP LEARNING TASKS\\n'}\n",
      "{'type': 'Text', 'bbox': [0.10384827557732078, 0.139707294810902, 0.972286376953125, 0.15242800625887784], 'properties': {'score': 0.3755064010620117, 'page_number': 1}}\n",
      "{'type': 'Text', 'bbox': [0.10481727151309743, 0.15526880437677557, 0.7572548540900735, 0.16686823064630682], 'properties': {'score': 0.415859580039978, 'page_number': 1}}\n",
      "{'type': 'Text', 'bbox': [0.10525580911075368, 0.16961482654918325, 0.7587096449908088, 0.18167200261896307], 'properties': {'score': 0.40727871656417847, 'page_number': 1}}\n",
      "{'type': 'Text', 'bbox': [0.10548425113453584, 0.1840617509321733, 0.7556692325367647, 0.19603116122159092], 'properties': {'score': 0.4013771712779999, 'page_number': 1}}\n",
      "{'type': 'Text', 'bbox': [0.10580453311695773, 0.1986296220259233, 0.7346940343520221, 0.21026551680131392], 'properties': {'score': 0.37076273560523987, 'page_number': 1}}\n",
      "{'type': 'Text', 'bbox': [0.10572673124425551, 0.2136015597256747, 0.5948442167394301, 0.2247656943581321], 'properties': {'score': 0.38035058975219727, 'page_number': 1}}\n",
      "{'type': 'Text', 'bbox': [0.10503360523897058, 0.2275618397105824, 0.6537513643152574, 0.23909859397194602], 'properties': {'score': 0.40866413712501526, 'page_number': 1}}\n",
      "{'type': 'Text', 'bbox': [0.10567740047679228, 0.24205463756214488, 0.7848609834558824, 0.2534591119939631], 'properties': {'score': 0.45055830478668213, 'page_number': 1}}\n",
      "{'type': 'Text', 'bbox': [0.10540567734662225, 0.2696520441228693, 0.5323125143612132, 0.2812418157404119], 'properties': {'score': 0.3716304302215576, 'page_number': 1}}\n",
      "{'type': 'Page-header', 'bbox': [0.028710455052992877, 0.27024336381392045, 0.05782888973460478, 0.7066756369850852], 'properties': {'score': 0.8207244277000427, 'page_number': 1}}\n",
      "{'type': 'Text', 'bbox': [0.10394355325137868, 0.2825876409357244, 0.7886025821461397, 0.3087285267223011], 'properties': {'score': 0.37358689308166504, 'page_number': 1}, 'text_representation': '∗Corresponding Author’s Email: jamal.bentahar@concordia.ca\\nContributing Authors’ Emails: saidul.islam@concordia.ca; hanae.elmekki@mail.concordia.ca;\\nahmed.elsebai@outlook.com; n drawe@encs.concordia.ca; g.rjoub@psut.edu.jo; wpedrycz@ualberta.ca\\n'}\n",
      "{'type': 'Text', 'bbox': [0.10492051068474265, 0.2835055819424716, 0.7391643210018383, 0.2947920920632102], 'properties': {'score': 0.3957202434539795, 'page_number': 1}}\n",
      "{'type': 'Text', 'bbox': [0.10439583273494944, 0.2975185879794034, 0.7883463062959559, 0.3088031005859375], 'properties': {'score': 0.40790727734565735, 'page_number': 1}}\n",
      "{'type': 'Text', 'bbox': [0.1043370953728171, 0.32497147993607955, 0.39763111787683825, 0.33660289417613637], 'properties': {'score': 0.8450936079025269, 'page_number': 1}, 'text_representation': 'The authors contributed equally to this work.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.4548382927389706, 0.3821625310724432, 0.5474250344669118, 0.3954760187322443], 'properties': {'score': 0.8709521889686584, 'page_number': 1}, 'text_representation': 'ABSTRACT\\n'}\n",
      "{'type': 'Text', 'bbox': [0.15306638829848346, 0.40778017911044034, 0.8519149959788603, 0.6553741455078125], 'properties': {'score': 0.9355373382568359, 'page_number': 1}, 'text_representation': 'Transformer is a deep neural network that employs a self-attention mechanism to comprehend the con-\\ntextual relationships within sequential data. Unlike conventional neural networks or updated versions of\\nRecurrent Neural Networks (RNNs) such as Long Short-Term Memory (LSTM), transformer models excel\\nin handling long dependencies between input sequence elements and enable parallel processing. As a result,\\ntransformer-based models have attracted substantial interest among researchers in the field of artificial intel-\\nligence. This can be attributed to their immense potential and remarkable achievements, not only in Natural\\nLanguage Processing (NLP) tasks but also in a wide range of domains, including computer vision, audio\\nand speech processing, healthcare, and the Internet of Things (IoT). Although several survey papers have\\nbeen published highlighting the transformer’s contributions in specific fields, architectural differences, or\\nperformance evaluations, there is still a significant absence of a comprehensive survey paper encompassing\\nits major applications across various domains. Therefore, we undertook the task of filling this gap by con-\\nducting an extensive survey of proposed transformer models from 2017 to 2022. Our survey encompasses\\nthe identification of the top five application domains for transformer-based models, namely: NLP, Computer\\nVision, Multi-Modality, Audio and Speech Processing, and Signal Processing. We analyze the impact of\\nhighly influential transformer-based models in these domains and subsequently classify them based on their\\nrespective tasks using a proposed taxonomy. Our aim is to shed light on the existing potential and future\\npossibilities of transformers for enthusiastic researchers, thus contributing to the broader understanding of\\nthis groundbreaking technology.\\nKeywords: Self-attention; Transformer; Deep learning, Recurrent networks; Long short-term memory-\\nLSTM; Multi-modality.\\n'}\n",
      "{'type': 'Text', 'bbox': [0.15291494930491728, 0.6597596324573863, 0.8498211310891544, 0.6853010697798295], 'properties': {'score': 0.6119627356529236, 'page_number': 1}}\n",
      "{'type': 'Section-header', 'bbox': [0.09586862900677849, 0.7096684126420455, 0.25596439137178306, 0.722868818803267], 'properties': {'score': 0.7401106357574463, 'page_number': 1}, 'text_representation': 'INTRODUCTION\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09435796401079963, 0.7340930730646307, 0.9098204130284926, 0.9259696266867897], 'properties': {'score': 0.9384289383888245, 'page_number': 1}, 'text_representation': 'Deep Neural Networks (DNNs) have emerged as the predominant infrastructure and state-of-the-art solution for the majority\\nof learning-based machine intelligence tasks in the field of artificial intelligence. Although various types of DNNs are utilized\\nfor specific tasks, the multilayer perceptron (MLP) represents the classic form of neural network which is characterized by\\nmultiple linear layers and nonlinear activation functions (Murtagh, 1990). For instance, in computer vision, convolutional\\nneural networks incorporate convolutional layers to process images, while recurrent neural networks employ recurrent cells\\nto process sequential data, particularly in Natural Language Processing (NLP) (O’Shea & Nash, 2015, Mikolov et al., 2010).\\nDespite the wide use of recurrent neural networks, they exhibit certain limitations. One of the major issues with conventional\\nnetworks is that they have short-term dependencies associated with exploding and vanishing gradients. In contrast, to achieve\\ngood results in NLP, long-term dependencies must be captured. Additionally, recurrent neural networks are slow to train\\ndue to their sequential data processing and computational approach (Giles et al., 1995). To address these issues, the long-\\nshort-term memory (LSTM) version of recurrent networks was developed, which improves the gradient descent problem\\nof recurrent neural networks and increases the memory range of NLP tasks (Hochreiter & Schmidhuber, 1997). However,\\nLSTMs still struggle with the problem of sequential processing, which hinders the extraction of the actual meaning of the\\ncontext. To tackle this challenge, bidirectional LSTMs were introduced, which process natural language from both directions,\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09351798562442555, 0.1006673847545277, 0.9121100930606618, 0.29534482088955966], 'properties': {'score': 0.9209118485450745, 'page_number': 2}}\n",
      "{'type': 'Text', 'bbox': [0.09358143525965074, 0.10094695351340555, 0.9094453699448529, 0.12836395263671874], 'properties': {'score': 0.5057174563407898, 'page_number': 2}}\n",
      "{'type': 'Text', 'bbox': [0.09402090633616728, 0.29645898992365055, 0.9117412971047794, 0.39289708917791194], 'properties': {'score': 0.7747340202331543, 'page_number': 2}}\n",
      "{'type': 'Text', 'bbox': [0.09339240579044118, 0.3941249500621449, 0.9103861012178309, 0.5187220348011363], 'properties': {'score': 0.7845802307128906, 'page_number': 2}}\n",
      "{'type': 'Section-header', 'bbox': [0.09501050163717831, 0.5465038507634943, 0.3884716078814338, 0.5604386763139204], 'properties': {'score': 0.7921656370162964, 'page_number': 2}, 'text_representation': '1.1 CONTRIBUTIONS AND MOTIVATIONS\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09391143798828125, 0.5681029163707386, 0.9118991986443015, 0.7893427068536932], 'properties': {'score': 0.9361646175384521, 'page_number': 2}, 'text_representation': 'Although several survey articles on the topic of transformers already exist in the literature, our motivations for conducting\\nthis survey stem from two essential observations. First, most of these studies have focused on transformer architecture, model\\nefficiency, and specific artificial intelligence fields, such as NLP, computer vision, multi-modality, audio & speech, and signal\\nprocessing. They have often neglected other crucial aspects, such as the transformer-based model’s execution in deep learning\\ntasks across multiple application domains. We aim in this survey to cover all major fields of application and present significant\\nmodels for different task executions. The second motivation is the absence of a comprehensive and methodical analysis\\nencompassing various prevalent application domains, and their corresponding utilization of transformer-based models, in\\nrelation to diverse deep learning tasks within distinct fields of application. We propose a high-level classification framework\\nfor transformer models, which is based on their most prominent fields of application. The prominent models are categorized\\nand evaluated based on their task performance within the respective fields. In this survey, we highlight the application domains\\nof transformers that have received comparatively greater or lesser attention from researchers. To the best of our knowledge,\\nthis is the first review paper that presents a high-level classification scheme for the transformer-based models and provides\\na collection of criteria that aim to achieve two objectives: (1) assessing the effectiveness of transformer models in various\\napplications; and (2) assisting researchers interested in exploring and extending the capabilities of transformer-based models\\nto new domains. Moreover, the paper provides valuable insights into potential future applications and highlights unresolved\\nchallenges within this field.\\nThe remainder of the paper is organized as follows. Preliminary concepts important for the rest of the paper are explained in\\nSection 2. A detailed description of the systematic methodology used to search for relevant research articles is provided in\\nSection 3. Section 4 presents related review papers and discusses similarities and differences with the current survey paper,\\nwhich helps us identify the unique characteristics and the value added of our survey. Section 5 identifies the the transformer\\nmodels proposed so far across different fields of application. A Classification of the selected scientific articles on section\\n6. Section 7 outlines potential directions for future work. Finally, Section 8 concludes the paper and summarizes the key\\nfindings and contributions of the study.\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09340518727022058, 0.7900946599786932, 0.9111986586626838, 0.8869691606001421], 'properties': {'score': 0.8256642818450928, 'page_number': 2}}\n",
      "{'type': 'Section-header', 'bbox': [0.09445491117589613, 0.9065516801313921, 0.25855711095473344, 0.9226465953480114], 'properties': {'score': 0.8293236494064331, 'page_number': 2}, 'text_representation': '2 PRELIMINARIES\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09342312981100644, 0.9345033957741478, 0.8766544117647059, 0.9481615101207387], 'properties': {'score': 0.8720320463180542, 'page_number': 2}, 'text_representation': 'Before delving into the literature of transformers, let us describe some concepts that will be used throughout this article.\\n'}\n",
      "{'type': 'Image', 'bbox': [0.2578384219898897, 0.09673316955566406, 0.7501156077665441, 0.3700851717862216], 'properties': {'score': 0.917018473148346, 'image_size': None, 'image_mode': None, 'image_format': None, 'page_number': 3}}\n",
      "{'type': 'Caption', 'bbox': [0.22704902200137866, 0.3865479347922585, 0.7736280014935661, 0.399996337890625], 'properties': {'score': 0.8667908906936646, 'page_number': 3}, 'text_representation': 'Figure 1: Multi-head attention & scaled dot product attention (Vaswani et al., 2017)\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09449680103975183, 0.43594815340909093, 0.3564038444967831, 0.4489339377663352], 'properties': {'score': 0.6649539470672607, 'page_number': 3}, 'text_representation': '2.1 TRANSFORMER ARCHITECTURE\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09395018633674172, 0.45466877330433236, 0.9110733570772059, 0.5654850075461648], 'properties': {'score': 0.9355127215385437, 'page_number': 3}, 'text_representation': 'The transformer model was first proposed in 2017 for a machine translation task, and since then, numerous models have\\nbeen developed based on the inspiration of the original transformer model to address a variety of tasks across different fields.\\nWhile some models have utilized the vanilla transformer architecture as is, others have leveraged only the encoder or decoder\\nmodule of the transformer model. As a result, the task and performance of transformer-based models can vary depending on\\nthe specific architecture employed. Nonetheless, a key and widely used component of transformer models is self-attention,\\nwhich is essential to their functionality. All transformer-based models employ the self-attention mechanism and multi-head\\nattention, which typically forms the primary learning layer of the architecture. Given the significance of self-attention, the\\nrole of the attention mechanism is crucial in transformer models (Vaswani et al., 2017)\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09377111098345588, 0.5752151211825284, 0.32311261345358455, 0.5883405095880682], 'properties': {'score': 0.6938475370407104, 'page_number': 3}}\n",
      "{'type': 'Text', 'bbox': [0.09357854506548713, 0.593124833540483, 0.910702335133272, 0.6759944291548295], 'properties': {'score': 0.8972923755645752, 'page_number': 3}}\n",
      "{'type': 'Text', 'bbox': [0.09351315666647518, 0.6768390447443182, 0.911171444163603, 0.8013487659801136], 'properties': {'score': 0.8578436970710754, 'page_number': 3}, 'text_representation': '2.1.1 ATTENTION MECHANISM\\nThe attention mechanism has garnered significant recognition since its introduction in the 1990s, owing to its ability to\\nconcentrate on critical pieces of information. In image processing, certain regions of images were found to be more pertinent\\nthan others. Consequently, the attention mechanism was introduced as a novel approach in computer vision tasks, aiming to\\nemphasize important parts based on their contextual relevance in the application. This technique yielded significant outcomes\\nwhen implemented in computer vision, thereby promoting its widespread adoption in various other fields such as language\\nprocessing.\\nIn 2017, a novel attention-based neural network, named “Transformer”, was introduced to address the limitations of other\\nneural networks (such as A recurrent neural network (RNN)) in encoding long-range dependencies in sequences, particularly\\nin language translation tasks (Vaswani et al., 2017). The incorporation of a self-attention mechanism in the transformer model\\nimproved the performance of the attention mechanism by better capturing local features and reducing the reliance on external\\ninformation. In the original transformer architecture, the attention technique is implemented through the “Scaled Dot Product\\nAttention”, which is based on three primary parameter matrices: Query (Q), Key (K), and Value (V). Each of these matrices\\ncarries an encoded representation of each input in the sequence (Vaswani et al., 2017). The SoftMax function is applied to\\nobtain the final output of the attention process, which is a probability score computed from the combination of the weights of\\nthe three matrices (see Figure 1). Mathematically, the scaled dot product attention function is computed as follows:\\n'}\n",
      "{'type': 'Formula', 'bbox': [0.3466021369485294, 0.810253739790483, 0.6535179946001838, 0.8448996803977272], 'properties': {'score': 0.7647605538368225, 'page_number': 3}, 'text_representation': 'V\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09334837969611673, 0.8510498601740056, 0.9076142434512867, 0.8789730557528409], 'properties': {'score': 0.8915411829948425, 'page_number': 3}, 'text_representation': 'The matrices Q and K represent the Query and Key vectors respectively, both having a dimension of dk, while the matrix V\\nrepresents the values vectors.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09370784086339613, 0.8892465487393466, 0.3237228932100184, 0.9023079057173296], 'properties': {'score': 0.6097784638404846, 'page_number': 3}}\n",
      "{'type': 'Text', 'bbox': [0.09401709163890165, 0.9061798095703125, 0.9090484978170956, 0.9480183549360796], 'properties': {'score': 0.8661753535270691, 'page_number': 3}, 'text_representation': '2.1.2 MULTI-HEAD ATTENTION\\nThe application of the scaled dot-product attention function in parallel within the multi-head Attention module is essential\\nfor extracting the maximum dependencies among different segments in the input sequence. Each head denoted by k performs\\nthe attention mechanism based on its own learnable weights W kQ, W kK, and W kv. The attention outputs calculated by each\\n'}\n",
      "{'type': 'Image', 'bbox': [0.28240769330193016, 0.09821759310635654, 0.7161058134191176, 0.5026354425603693], 'properties': {'score': 0.9177677035331726, 'image_size': None, 'image_mode': None, 'image_format': None, 'page_number': 4}}\n",
      "{'type': 'Caption', 'bbox': [0.3131997860179228, 0.5118587424538352, 0.6872310862821691, 0.5258661998401989], 'properties': {'score': 0.8677524328231812, 'page_number': 4}, 'text_representation': 'Figure 2: Transformer architecture (Vaswani et al., 2017)\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09410664277918199, 0.5526947576349431, 0.9077570657169117, 0.5807035688920454], 'properties': {'score': 0.9169228672981262, 'page_number': 4}, 'text_representation': 'head are subsequently concatenated and linearly transformed into a single matrix with the expected dimension (Vaswani et al.,\\n2017).\\n'}\n",
      "{'type': 'Formula', 'bbox': [0.2877974207261029, 0.5988887162642046, 0.7140322696461398, 0.6351222367720171], 'properties': {'score': 0.4005083441734314, 'page_number': 4}, 'text_representation': 'headk = Attention(QW kQ, KW kK, V W kV )\\nM ultiHead(Q, K, V ) = Concat(head1, head2, ....headH)W 0\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09419458725873162, 0.6427860884232954, 0.9095970961626838, 0.6978693736683239], 'properties': {'score': 0.9357970356941223, 'page_number': 4}, 'text_representation': 'The utilization of multi-head attention facilitates the neural network in learning and capturing diverse characteristics of the\\ninput sequential data. Consequently, this enhances the representation of the input contexts, as it merges information from\\ndistinct features of the attention mechanism within a specific range, which could be either short or long. This approach allows\\nthe attention mechanism to jointly function, which results in better network performance (Vaswani et al., 2017).\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09435296451344209, 0.7082990056818181, 0.4667657470703125, 0.7217502108487216], 'properties': {'score': 0.5391800403594971, 'page_number': 4}, 'text_representation': '2.2 ARCHITECTURE OF THE TRANSFORMER MODEL\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09433340633616728, 0.7273982377485796, 0.9103820082720588, 0.7965253240411931], 'properties': {'score': 0.8873066902160645, 'page_number': 4}}\n",
      "{'type': 'Text', 'bbox': [0.09415444766773898, 0.7974381325461648, 0.9104746380974265, 0.8666281960227272], 'properties': {'score': 0.7824925184249878, 'page_number': 4}}\n",
      "{'type': 'Section-header', 'bbox': [0.09430780747357537, 0.8757083962180398, 0.2812424244600184, 0.8885109086470171], 'properties': {'score': 0.5039551854133606, 'page_number': 4}}\n",
      "{'type': 'Text', 'bbox': [0.09454851935891544, 0.8927373157848011, 0.9108340992647059, 0.9481400923295454], 'properties': {'score': 0.9265841245651245, 'page_number': 4}, 'text_representation': '2.2.1 ENCODER MODULE\\nThe stacked module within the transformer architecture comprises two fundamental layers, namely the Feed-Forward Layer\\nand Multi-Head Attention Layer. In addition, it incorporates Residual connections around both layers, as well as two Add\\nand Norm layers, which play a pivotal role (Vaswani et al., 2017). In the case of text translation, the Encoder module receives\\nan embedding input that is generated based on the input’s meaning and position information via the Embedding and Position\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09361794864430147, 0.10059684753417969, 0.912563907398897, 0.18428343339399858], 'properties': {'score': 0.9417975544929504, 'page_number': 5}, 'text_representation': 'Encoding layers. From the embedding input, three parameter matrices are created, namely the Query (Q), Key (K), and Value\\n(V ) matrices, along with positional information, which are passed through the “Multi-Head Attention” layer. Following this\\nstep, the Feed-Forward layer addresses the issue of rank collapse that can arise during the computation process. Additionally,\\na normalization layer is applied to each step, which reduces the dependencies between layers by normalizing the weights used\\nin gradient computation within each layer. To address the issue of vanishing gradients, the Residual Connection is applied to\\nevery output of both the attention and feed-forward layers, as illustrated in Figure 2.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09395393820369945, 0.19674034812233665, 0.2823418112362132, 0.21050084894353693], 'properties': {'score': 0.7288676500320435, 'page_number': 5}, 'text_representation': '2.2.2 DECODER MODULE\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09380340576171875, 0.21573674982244317, 0.9129925896139706, 0.34111530650745736], 'properties': {'score': 0.9457911849021912, 'page_number': 5}, 'text_representation': 'The Decoder module in the transformer architecture is similar to the Encoder module, with the inclusion of additional layers\\nsuch as Masked Multi-Head Attention. In addition to the Feed-Forward, Multi-Head Attention, Residual connection, and Add\\nand Norm layers, the Decoder also contains Masked Multi-Head Attention layers. These layers use the scaled dot product\\nand Mask Operations to exclude future predictions and consider only previous outputs. The Attention mechanism is applied\\ntwice in the Decoder: one for computing attention between elements of the targeted output and another for finding attention\\nbetween the encoding inputs and targeted output. Each attention vector is then passed through the feed-forward unit to make\\nthe output more comprehensible to the layers. The generated decoding result is then caught by Linear and SoftMax layers\\nat the top of the Decoder to compute the final output of the transformer architecture. This process is repeated multiple times\\nuntil the last token of a sentence is found (Vaswani et al., 2017), as illustrated in Figure 2.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09418988396139706, 0.35800922740589486, 0.3568084357766544, 0.3748151189630682], 'properties': {'score': 0.870897114276886, 'page_number': 5}, 'text_representation': '3 RESEARCH METHODOLOGY\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09376004387350644, 0.3853788896040483, 0.9128376321231617, 0.46959239612926135], 'properties': {'score': 0.9409381747245789, 'page_number': 5}, 'text_representation': 'In this survey, we collect and analyze the most recent surveys on transformers that have been published in refereed journals\\nand conferences with the aim of studying their contributions and limitations. To gather the relevant papers, we employed a\\ntwo-fold strategy: (1) searching using several established search engines and selected papers based on the keywords “survey”,\\n“review”, “Transformer”, “attention”, “self-attention”, “artificial intelligence”, and “deep learning; and (2) evaluating the\\nselected papers and eliminated those that were deemed irrelevant for our study. A detailed organization of our survey is\\ndepicted in Figure 3.\\n'}\n",
      "{'type': 'Image', 'bbox': [0.06940659915699679, 0.48601801091974434, 0.9243362247242647, 0.7539928644353693], 'properties': {'score': 0.9079911708831787, 'image_size': None, 'image_mode': None, 'image_format': None, 'page_number': 5}}\n",
      "{'type': 'Caption', 'bbox': [0.37920306037454043, 0.7601444313742898, 0.6213225959329044, 0.7746867786754261], 'properties': {'score': 0.8992475867271423, 'page_number': 5}, 'text_representation': 'Figure 3: Methodology of the survey\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09363090963924632, 0.7943286687677557, 0.910548095703125, 0.905689697265625], 'properties': {'score': 0.8974995613098145, 'page_number': 5}, 'text_representation': 'Indeed, by means of a comprehensive examination of survey papers and expert discussions on deep learning, we have iden-\\ntified the top five domains of application for transformer-based models, these are: (i) NLP, (ii) computer vision, (iii) multi-\\nmodality, (iv) audio/speech, and (v) signal processing. Subsequently, we performed a systematic search for journal and\\nconference papers that presented transformer-based models in each of the aforementioned fields of application, utilizing the\\nkeywords presented in Table 1. Our search yielded a substantial number of papers for each field, which we thoroughly re-\\nviewed and evaluated. We selected papers that proposed novel transformer-based or transformer-inspired models for deep\\nlearning tasks, while disregarding others. Through our examination of this extensive collection of models, we have identified\\nprevalent deep-learning tasks associated with each field of application.\\nAs we have examined more than 600 transformer models during this process, it has become exceedingly difficult to classify\\nsuch a large number of models and conduct thorough analyses of each task within every field of application. Therefore, we\\nhave opted to perform a more comprehensive analysis of a number of transformer models for each task within every field of\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09381830552045037, 0.9065298184481534, 0.9103453153722426, 0.9478355823863637], 'properties': {'score': 0.7039918899536133, 'page_number': 5}}\n",
      "{'type': 'Text', 'bbox': [0.09386071597828585, 0.10066274469549005, 0.9090404555376839, 0.12821985418146306], 'properties': {'score': 0.8331626057624817, 'page_number': 6}, 'text_representation': 'application. These models were selected based on specific criteria and an in-depth analysis was carried out accordingly. The\\nselected models are as follows:\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.13175440171185662, 0.13543217052112927, 0.9090135282628676, 0.16347525856711648], 'properties': {'score': 0.9004900455474854, 'page_number': 6}, 'text_representation': '1. The transformer-based models that have been proposed to execute a deep learning task for the first time and opened\\n up new path for research in the field of transformer applications.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.13155779670266543, 0.16797565806995737, 0.909041317210478, 0.20979664195667613], 'properties': {'score': 0.9017342925071716, 'page_number': 6}, 'text_representation': '2. The models that have proposed alternative or novel approaches to implementing the transformer’s attention mech-\\nanism, as compared to the vanilla architecture, such as introducing a new attention mechanism or enhancing the\\nposition encoding module.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.13130411484662224, 0.2149071017178622, 0.9101418169806985, 0.25656938032670457], 'properties': {'score': 0.9114456176757812, 'page_number': 6}, 'text_representation': '3. The transformer models have had a significant impact in the field, with higher citation rates, and have been widely\\naccepted by the scientific community. Models that have also contributed to breakthroughs in the advancement of\\ntransformer applications.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.13108021455652574, 0.2613983154296875, 0.9094901769301471, 0.30267764004794034], 'properties': {'score': 0.8889461755752563, 'page_number': 6}, 'text_representation': '4. The models and their variants have been proposed for the purpose of applying the transformer technology to real-\\nworld applications, with the aim of achieving superior performance results in comparison to other deep learning\\nmethods.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.13115641874425552, 0.3076609940962358, 0.9099319996553309, 0.3357906827059659], 'properties': {'score': 0.8610333800315857, 'page_number': 6}, 'text_representation': '5. The transformer-based models generated a significant buzz within the theoretical and applied artificial intelligence\\n community.\\n'}\n",
      "{'type': 'table', 'bbox': [0.05068528568043428, 0.36155542547052555, 0.9448694565716912, 0.9499106667258522], 'properties': {'score': 0.8498632907867432, 'title': None, 'columns': None, 'rows': None, 'page_number': 6}, 'text_representation': 'Fields of\\nApplication\\n Natural Language\\nProcessing\\n Keywords for Paper Search\\n Tasks Of Application\\n Number of papers\\n “Natural Language Processing”,\\n“NLP”,“Text”,“Text Processing”,\\n“Transformer”, “Attention”,\\n“Self-attention”, “multi-head\\nattention”, “Language model”.\\n Relevant models\\nusing keywords\\n Selected models\\nfor Taxonomy\\n Language Translation\\n 257\\n 25\\n Text Classification & Segmentation\\nQuestion Answering\\nText Summarization\\nText Generation\\nNatural Language Reasoning\\nAutomated Symbolic\\nReasoning\\n Computer Vision\\n “Transformer”,“Attention”,\\n“Self-attention”,“Image”,\\n“Natural image”,“medical\\nimage”,“Biomedical”,\\n“health”,“Image processing”,\\n“Computer vision”,“Vision”.\\n Natural Image\\nProcessing\\n Image\\nClassification\\n 197\\n 27\\n Medical Image\\nProcessing\\n Recognition &\\nObject Detection\\nImage\\nSegmentation\\nImage Generation\\nImage\\nSegmentation\\nImage\\nClassification\\nImage\\nTranslation\\n Multi-modal\\n “Transformer”,“Attention”,\\n“Self-attention”,“multi-head\\nattention”,“multimodal”,\\n“multi-modality”,“text-image”,\\n“image-text”,“ video-audio-\\ntext, “text-audio”,“audio-text”,\\n“vision-language”,\\n“language-vision”.\\n Classification &\\nSegmentation\\n 94\\n 20\\n Continued on next page\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e04664a0>, 'tokens': [{'text': 'Fields of\\nApplication\\n', 'bbox': {'x1': 0.0802467320261438, 'y1': 0.36479366590909096, 'x2': 0.16124983267973855, 'y2': 0.3912098275252526}}, {'text': 'Natural Language\\nProcessing\\n', 'bbox': {'x1': 0.06285130718954249, 'y1': 0.4311102545454545, 'x2': 0.17992813594771243, 'y2': 0.45752641616161616}}, {'text': 'Keywords for Paper Search\\n', 'bbox': {'x1': 0.20723692810457514, 'y1': 0.37171285782828284, 'x2': 0.3988541905228759, 'y2': 0.38429189823232324}}, {'text': 'Tasks Of Application\\n', 'bbox': {'x1': 0.4754803921568628, 'y1': 0.37171285782828284, 'x2': 0.6219241006535948, 'y2': 0.38429189823232324}}, {'text': 'Number of papers\\n', 'bbox': {'x1': 0.748859477124183, 'y1': 0.37171285782828284, 'x2': 0.8754593794117647, 'y2': 0.38429189823232324}}, {'text': '“Natural Language Processing”,\\n“NLP”,“Text”,“Text Processing”,\\n“Transformer”, “Attention”,\\n“Self-attention”, “multi-head\\nattention”, “Language model”.\\n', 'bbox': {'x1': 0.19883006535947712, 'y1': 0.4207327292929293, 'x2': 0.41667240718954257, 'y2': 0.48866025454545453}}, {'text': 'Relevant models\\nusing keywords\\n', 'bbox': {'x1': 0.6901437908496731, 'y1': 0.39246790833333334, 'x2': 0.8041276555555555, 'y2': 0.41888406994949495}}, {'text': 'Selected models\\nfor Taxonomy\\n', 'bbox': {'x1': 0.8243120915032679, 'y1': 0.39246790833333334, 'x2': 0.9341774303921568, 'y2': 0.41888406994949495}}, {'text': 'Language Translation\\n', 'bbox': {'x1': 0.4777843137254902, 'y1': 0.4347592444444444, 'x2': 0.6196211336601307, 'y2': 0.4473382848484848}}, {'text': '257\\n', 'bbox': {'x1': 0.7455996732026144, 'y1': 0.4347592444444444, 'x2': 0.7700178104575164, 'y2': 0.4473382848484848}}, {'text': '25\\n', 'bbox': {'x1': 0.8797189542483661, 'y1': 0.4347592444444444, 'x2': 0.8959977124183008, 'y2': 0.4473382848484848}}, {'text': 'Text Classification & Segmentation\\nQuestion Answering\\nText Summarization\\nText Generation\\nNatural Language Reasoning\\nAutomated Symbolic\\nReasoning\\n', 'bbox': {'x1': 0.4330751633986928, 'y1': 0.48991833535353535, 'x2': 0.6643312019607843, 'y2': 0.5862738909090909}}, {'text': 'Computer Vision\\n', 'bbox': {'x1': 0.06493627450980392, 'y1': 0.6055206080808081, 'x2': 0.17655971928104577, 'y2': 0.6180996484848484}}, {'text': '“Transformer”,“Attention”,\\n“Self-attention”,“Image”,\\n“Natural image”,“medical\\nimage”,“Biomedical”,\\n“health”,“Image processing”,\\n“Computer vision”,“Vision”.\\n', 'bbox': {'x1': 0.2074411764705882, 'y1': 0.5880344969696969, 'x2': 0.39865146993464057, 'y2': 0.6697991434343434}}, {'text': 'Natural Image\\nProcessing\\n', 'bbox': {'x1': 0.4274460784313725, 'y1': 0.6018716181818182, 'x2': 0.5210163803921568, 'y2': 0.6282877797979798}}, {'text': 'Image\\nClassification\\n', 'bbox': {'x1': 0.5607941176470588, 'y1': 0.6018716181818182, 'x2': 0.6494319558823528, 'y2': 0.6282877797979798}}, {'text': '197\\n', 'bbox': {'x1': 0.7455996732026144, 'y1': 0.6055206080808081, 'x2': 0.7700178104575164, 'y2': 0.6180996484848484}}, {'text': '27\\n', 'bbox': {'x1': 0.8797189542483661, 'y1': 0.6055206080808081, 'x2': 0.8959977124183008, 'y2': 0.6180996484848484}}, {'text': 'Medical Image\\nProcessing\\n', 'bbox': {'x1': 0.4274460784313725, 'y1': 0.7402415676767676, 'x2': 0.5255418751633987, 'y2': 0.7666577292929293}}, {'text': 'Recognition &\\nObject Detection\\nImage\\nSegmentation\\nImage Generation\\nImage\\nSegmentation\\nImage\\nClassification\\nImage\\nTranslation\\n', 'bbox': {'x1': 0.5402679738562092, 'y1': 0.6710572242424242, 'x2': 0.6604926967320262, 'y2': 0.8220062141414142}}, {'text': 'Multi-modal\\n', 'bbox': {'x1': 0.07959640522875817, 'y1': 0.8481708606060606, 'x2': 0.16190180653594774, 'y2': 0.8607499010101011}}, {'text': '“Transformer”,“Attention”,\\n“Self-attention”,“multi-head\\nattention”,“multimodal”,\\n“multi-modality”,“text-image”,\\n“image-text”,“ video-audio-\\ntext, “text-audio”,“audio-text”,\\n“vision-language”,\\n“language-vision”.\\n', 'bbox': {'x1': 0.20098692810457516, 'y1': 0.8237668202020202, 'x2': 0.40510627679738576, 'y2': 0.9332057090909092}}, {'text': 'Classification &\\nSegmentation\\n', 'bbox': {'x1': 0.49601633986928106, 'y1': 0.8408741434343434, 'x2': 0.601388741503268, 'y2': 0.8672903050505051}}, {'text': '94\\n', 'bbox': {'x1': 0.7496699346405229, 'y1': 0.8481708606060606, 'x2': 0.7659486928104574, 'y2': 0.8607499010101011}}, {'text': '20\\n', 'bbox': {'x1': 0.8797189542483661, 'y1': 0.8481708606060606, 'x2': 0.8959977124183008, 'y2': 0.8607499010101011}}, {'text': 'Continued on next page\\n', 'bbox': {'x1': 0.7793333333333333, 'y1': 0.9349663151515151, 'x2': 0.9341768810457516, 'y2': 0.9475453555555555}}]}\n",
      "{'type': 'Section-header', 'bbox': [0.3616466208065257, 0.09792782176624645, 0.637420654296875, 0.11205234874378552], 'properties': {'score': 0.4395604133605957, 'page_number': 7}, 'text_representation': 'Table 1 – continued from previous page\\n'}\n",
      "{'type': 'table', 'bbox': [0.050577612484202665, 0.11224247325550427, 0.945206298828125, 0.47636957341974434], 'properties': {'score': 0.872572124004364, 'title': None, 'columns': None, 'rows': None, 'page_number': 7}, 'text_representation': 'Fields of\\nApplication\\n Keywords for Paper Search\\n Tasks Of Application\\n Number of papers\\n Relevant models\\nusing keywords\\n Selected models\\nfor Taxonomy\\n Visual Question Answering\\nVisual Captioning\\nVisual Common-sense\\nReasoning\\nText/Image/Video/Speech\\nGeneration\\nCloud Task Computing\\n Audio & Speech\\n Signal Pro-\\ncessing\\n “Transformer”,“Attention”,\\n“Self-attention”,“multi-head\\nattention”,“audio”,“Speech”,\\n“audio processing”,“speech\\nprocessing”,\\n “Transformer”, “Attention”,\\n“Self-attention”, “multi-head\\nattention”, “signal”, “signal\\nprocessing” , “wireless”,\\n“wireless signal”, “wireless\\nnetwork”, “biosignal”, “medical\\nsignal”.\\n Audio & Speech Recognition\\n 70\\n 16\\n Audio & Speech Separation\\nAudio & Speech Classification\\n Wireless network Signal\\nprocessing\\n Medical Signal Processing\\n 23\\n 11\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e04675e0>, 'tokens': [{'text': 'Fields of\\nApplication\\n', 'bbox': {'x1': 0.0802467320261438, 'y1': 0.11354619116161614, 'x2': 0.16124983267973855, 'y2': 0.13996235277777763}}, {'text': 'Keywords for Paper Search\\n', 'bbox': {'x1': 0.20723692810457514, 'y1': 0.12046538308080808, 'x2': 0.3988541905228759, 'y2': 0.13304442348484843}}, {'text': 'Tasks Of Application\\n', 'bbox': {'x1': 0.4754803921568628, 'y1': 0.12046538308080808, 'x2': 0.6219241006535948, 'y2': 0.13304442348484843}}, {'text': 'Number of papers\\n', 'bbox': {'x1': 0.748859477124183, 'y1': 0.12046538308080808, 'x2': 0.8754593794117647, 'y2': 0.13304442348484843}}, {'text': 'Relevant models\\nusing keywords\\n', 'bbox': {'x1': 0.6901437908496731, 'y1': 0.14122043358585856, 'x2': 0.8041276555555555, 'y2': 0.16763659520202007}}, {'text': 'Selected models\\nfor Taxonomy\\n', 'bbox': {'x1': 0.8243120915032679, 'y1': 0.14122043358585856, 'x2': 0.9341774303921568, 'y2': 0.16763659520202007}}, {'text': 'Visual Question Answering\\nVisual Captioning\\nVisual Common-sense\\nReasoning\\nText/Image/Video/Speech\\nGeneration\\nCloud Task Computing\\n', 'bbox': {'x1': 0.4587630718954248, 'y1': 0.16948525454545452, 'x2': 0.6386433496732026, 'y2': 0.2650870222222223}}, {'text': 'Audio & Speech\\n', 'bbox': {'x1': 0.06649019607843137, 'y1': 0.2808741434343434, 'x2': 0.17500439803921572, 'y2': 0.2934531838383839}}, {'text': 'Signal Pro-\\ncessing\\n', 'bbox': {'x1': 0.0838937908496732, 'y1': 0.3782365171717172, 'x2': 0.15760400784313727, 'y2': 0.4046514161616162}}, {'text': '“Transformer”,“Attention”,\\n“Self-attention”,“multi-head\\nattention”,“audio”,“Speech”,\\n“audio processing”,“speech\\nprocessing”,\\n', 'bbox': {'x1': 0.20879248366013073, 'y1': 0.26684762828282826, 'x2': 0.39730050326797395, 'y2': 0.3347751535353533}}, {'text': '“Transformer”, “Attention”,\\n“Self-attention”, “multi-head\\nattention”, “signal”, “signal\\nprocessing” , “wireless”,\\n“wireless signal”, “wireless\\nnetwork”, “biosignal”, “medical\\nsignal”.\\n', 'bbox': {'x1': 0.19883006535947712, 'y1': 0.364210002020202, 'x2': 0.40843535555555566, 'y2': 0.4598117696969697}}, {'text': 'Audio & Speech Recognition\\n', 'bbox': {'x1': 0.4274460784313725, 'y1': 0.2808741434343434, 'x2': 0.619616818627451, 'y2': 0.2934531838383839}}, {'text': '70\\n', 'bbox': {'x1': 0.7496699346405229, 'y1': 0.2808741434343434, 'x2': 0.7659486928104574, 'y2': 0.2934531838383839}}, {'text': '16\\n', 'bbox': {'x1': 0.8797189542483661, 'y1': 0.2808741434343434, 'x2': 0.8959977124183008, 'y2': 0.2934531838383839}}, {'text': 'Audio & Speech Separation\\nAudio & Speech Classification\\n', 'bbox': {'x1': 0.4274460784313725, 'y1': 0.33603323434343424, 'x2': 0.6286678081699346, 'y2': 0.36244939595959597}}, {'text': 'Wireless network Signal\\nprocessing\\n', 'bbox': {'x1': 0.46908333333333335, 'y1': 0.3778589919191919, 'x2': 0.628322145751634, 'y2': 0.4042751535353535}}, {'text': 'Medical Signal Processing\\n', 'bbox': {'x1': 0.4618872549019608, 'y1': 0.4610685878787878, 'x2': 0.6355164895424836, 'y2': 0.47364762828282825}}, {'text': '23\\n', 'bbox': {'x1': 0.7496699346405229, 'y1': 0.3851544464646465, 'x2': 0.7659486928104574, 'y2': 0.39773348686868687}}, {'text': '11\\n', 'bbox': {'x1': 0.8797189542483661, 'y1': 0.3851544464646465, 'x2': 0.8959977124183008, 'y2': 0.39773348686868687}}]}\n",
      "{'type': 'Text', 'bbox': [0.0936464646283318, 0.4872634055397727, 0.9121343635110294, 0.5147677334872159], 'properties': {'score': 0.8309272527694702, 'page_number': 7}, 'text_representation': 'Table 1: Transformer models’ field of application, used keywords for paper search, popular deep learning tasks, number of\\nrelevant papers by search, and number of selected models for taxonomy and further discussion.\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09409535127527574, 0.534273515181108, 0.9130512551700367, 0.6312036687677557], 'properties': {'score': 0.8954193592071533, 'page_number': 7}, 'text_representation': 'In the field of application, we have classified the selected models based on their task execution and developed a taxonomy of\\ntransformer applications. Our analysis involved a comprehensive examination of the models, including their structures, char-\\nacteristics, operational methods, and datasets, among others. Based on this investigation, we provide an in-depth discussion\\nof the potential future applications of transformers. To conduct this research, we consulted various prominent research repos-\\nitories, such as “AAAI”, “ACM”, “ACL”, “CVPR”, “ICML”, “ICLR”, “ICCV”, “NeurlIPS”, “LREC”, “IEEE”, “PMLR”,\\n”National Library of medicine”,“SCOPUS”, “MDPI”, “ScienceDirect”, and “Cornell University-arxiv library”. Table 1 de-\\npicts the category of the selected models.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09423269832835478, 0.6452532958984375, 0.26396697998046875, 0.6608528830788353], 'properties': {'score': 0.7360638380050659, 'page_number': 7}, 'text_representation': '4 RELATED WORK\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09379741893095128, 0.6693968616832386, 0.9125263528262868, 0.808954023881392], 'properties': {'score': 0.49636510014533997, 'page_number': 7}}\n",
      "{'type': 'Text', 'bbox': [0.09376036700080423, 0.8096636408025568, 0.9108514763327206, 0.8929302423650568], 'properties': {'score': 0.4521206319332123, 'page_number': 7}}\n",
      "{'type': 'Text', 'bbox': [0.09409496531767003, 0.8932132235440341, 0.9104991957720588, 0.9482729270241478], 'properties': {'score': 0.518610417842865, 'page_number': 7}}\n",
      "{'type': 'Text', 'bbox': [0.09406474393956801, 0.10079480257901278, 0.9101991900275735, 0.14214354081587358], 'properties': {'score': 0.8786973357200623, 'page_number': 8}}\n",
      "{'type': 'Text', 'bbox': [0.09396082261029412, 0.14320311112837358, 0.9131692325367647, 0.25412880637428975], 'properties': {'score': 0.8725324273109436, 'page_number': 8}}\n",
      "{'type': 'Text', 'bbox': [0.09375885009765625, 0.25481184525923295, 0.9116805491727941, 0.35190770929509946], 'properties': {'score': 0.7960163950920105, 'page_number': 8}}\n",
      "{'type': 'Text', 'bbox': [0.09382223690257353, 0.3524943126331676, 0.9110305606617647, 0.4491587968306108], 'properties': {'score': 0.7744642496109009, 'page_number': 8}}\n",
      "{'type': 'Text', 'bbox': [0.09359919828527113, 0.4499279230291193, 0.9118627211626839, 0.5606490256569602], 'properties': {'score': 0.7966157793998718, 'page_number': 8}}\n",
      "{'type': 'table', 'bbox': [0.049532295675838695, 0.5810160134055398, 0.9553049603630515, 0.892227616743608], 'properties': {'score': 0.7963489890098572, 'title': None, 'columns': None, 'rows': None, 'page_number': 8}, 'text_representation': 'Approach\\n Fields of\\nApplication\\n Similarities\\n Differences\\n Q Fournier\\net al.\\n(Fournier\\net al., 2021)\\n Performance\\n/Archi-\\ntecture\\n • A classification of the trans-\\n formers is suggested, and this\\nclassification is based on atten-\\ntion mechanism modification or\\narchitecture modification\\n • This pare surveyed the different alternatives of the standard\\n transformers that are more efficient in terms of time and mem-\\nory complexities, and these alternatives are categorized by ei-\\nther modifying the attention mechanism or the network archi-\\ntecture. Their classification is based on the change in architec-\\nture or change in attention mechanism, while our classification\\nis driven by application areas.\\n T. Lin et\\nal. (Lin\\net al., 2022)\\n Performance\\n/Archi-\\ntecture\\n • Proposed taxonomy of X-\\n formers covering several fields\\n • This existing survey compared X-formers from architectural\\n modification, pre-training, and a very small range of application\\nperspectives, while our survey deeply focuses on popular tasks\\nunder each field of application.\\n • The wireless/medical signal processing and cloud computing\\n tasks application were missing in this exciting survey, while our\\nsurvey covers these tasks and applications.\\n Continued on next page\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e0494910>, 'tokens': [{'text': 'Approach\\n', 'bbox': {'x1': 0.07006699346405229, 'y1': 0.5902204335858586, 'x2': 0.1390075343137255, 'y2': 0.602799473989899}}, {'text': 'Fields of\\nApplication\\n', 'bbox': {'x1': 0.16759313725490196, 'y1': 0.5833012416666667, 'x2': 0.24859623790849675, 'y2': 0.6097174032828283}}, {'text': 'Similarities\\n', 'bbox': {'x1': 0.3413366013071895, 'y1': 0.5902204335858586, 'x2': 0.4209234499999999, 'y2': 0.602799473989899}}, {'text': 'Differences\\n', 'bbox': {'x1': 0.6886797385620915, 'y1': 0.5902204335858586, 'x2': 0.7670294016339868, 'y2': 0.602799473989899}}, {'text': 'Q Fournier\\net al.\\n(Fournier\\net al., 2021)\\n', 'bbox': {'x1': 0.06565522875816995, 'y1': 0.6486746484848485, 'x2': 0.14341885653594771, 'y2': 0.7027650525252526}}, {'text': 'Performance\\n/Archi-\\ntecture\\n', 'bbox': {'x1': 0.16651143790849673, 'y1': 0.6555938404040405, 'x2': 0.24967961339869282, 'y2': 0.6958471232323232}}, {'text': '• A classification of the trans-\\n', 'bbox': {'x1': 0.27002450980392156, 'y1': 0.6442718707070707, 'x2': 0.46742073137254914, 'y2': 0.656850911111111}}, {'text': 'formers is suggested, and this\\nclassification is based on atten-\\ntion mechanism modification or\\narchitecture modification\\n', 'bbox': {'x1': 0.28386111111111106, 'y1': 0.6581089919191919, 'x2': 0.49141527777777777, 'y2': 0.7121993959595959}}, {'text': '• This pare surveyed the different alternatives of the standard\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.630436012121212, 'x2': 0.9127003810457519, 'y2': 0.6430150525252525}}, {'text': 'transformers that are more efficient in terms of time and mem-\\nory complexities, and these alternatives are categorized by ei-\\nther modifying the attention mechanism or the network archi-\\ntecture. Their classification is based on the change in architec-\\nture or change in attention mechanism, while our classification\\nis driven by application areas.\\n', 'bbox': {'x1': 0.526372549019608, 'y1': 0.6442718707070707, 'x2': 0.9346438039215689, 'y2': 0.7260365171717172}}, {'text': 'T. Lin et\\nal. (Lin\\net al., 2022)\\n', 'bbox': {'x1': 0.06565522875816993, 'y1': 0.7881771737373737, 'x2': 0.14341885653594771, 'y2': 0.8284304565656566}}, {'text': 'Performance\\n/Archi-\\ntecture\\n', 'bbox': {'x1': 0.16651143790849673, 'y1': 0.7881771737373737, 'x2': 0.24967961339869282, 'y2': 0.8284304565656566}}, {'text': '• Proposed taxonomy of X-\\n', 'bbox': {'x1': 0.27002450980392156, 'y1': 0.7976115171717172, 'x2': 0.452509388888889, 'y2': 0.8101905575757576}}, {'text': 'formers covering several fields\\n', 'bbox': {'x1': 0.28386111111111106, 'y1': 0.8114486383838384, 'x2': 0.48389449150326797, 'y2': 0.8240276787878787}}, {'text': '• This existing survey compared X-formers from architectural\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.7605029313131313, 'x2': 0.918951424183007, 'y2': 0.7730819717171717}}, {'text': 'modification, pre-training, and a very small range of application\\nperspectives, while our survey deeply focuses on popular tasks\\nunder each field of application.\\n', 'bbox': {'x1': 0.526372549019608, 'y1': 0.7743400525252526, 'x2': 0.9429785281045754, 'y2': 0.8145933353535354}}, {'text': '• The wireless/medical signal processing and cloud computing\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.8208829818181819, 'x2': 0.9233466888888893, 'y2': 0.8334620222222222}}, {'text': 'tasks application were missing in this exciting survey, while our\\nsurvey covers these tasks and applications.\\n', 'bbox': {'x1': 0.526372549019608, 'y1': 0.834720103030303, 'x2': 0.9423762140522876, 'y2': 0.8611362646464646}}, {'text': 'Continued on next page\\n', 'bbox': {'x1': 0.7883888888888889, 'y1': 0.8772377797979797, 'x2': 0.9432324366013072, 'y2': 0.8898168202020202}}]}\n",
      "{'type': 'Section-header', 'bbox': [0.36648236443014703, 0.09749876542524857, 0.6415929457720588, 0.11206511064009234], 'properties': {'score': 0.5389121174812317, 'page_number': 9}, 'text_representation': 'Table 2 – continued from previous page\\n'}\n",
      "{'type': 'table', 'bbox': [0.049542994779698986, 0.11090212735262785, 0.9549565573299632, 0.831591463955966], 'properties': {'score': 0.8215442299842834, 'title': None, 'columns': None, 'rows': None, 'page_number': 9}, 'text_representation': 'Approach\\n Fields of\\nApplication\\n Similarities\\n Differences\\n Y. Tay et\\nal. (Tay\\net al., 2023)\\n Performance\\n/Archi-\\ntecture\\n • Proposed a taxonomy consid-\\nering the primary use case of\\ntransformer models in language\\nand vision domains .\\n A. M. P.\\nBras¸oveanu\\net al.\\n(Brasoveanu\\n& Andonie,\\n2020)\\n Natural\\nlanguage\\nProcessing-\\nNLP\\n • Explain transformer architec-\\nture and explain its features.\\n • This existing survey compared the computational power and\\n memory efficiency of transformer models, whereas our survey\\nfocuses on deep learning tasks and applications.\\n • This exciting survey focused on language and vision domain\\n only, while we cover other top five fields of transformer appli-\\ncations: NLP, computer vision, multi-modality, audio/speech,\\nand signal processing.\\n • Our survey describes the transformer model and the significant\\nmodels’ working processing for a range of tasks. However, this\\nexisting paper focused on visualization techniques used to ex-\\nplain the most recent transformer architectures and explored\\ntwo large tool classes to explain the inner workings of Trans-\\nformers.\\n • we covered five fields of transformer applications: NLP, com-\\nputer vision, multi-modality, audio/speech, and signal process-\\ning and this exciting survey focused on the models for NLP\\nonly.\\n W Guan et\\nal. (Wang\\net al.,\\n2020a)\\n Natural\\nlanguage\\nProcessing-\\nNLP\\n • Survey an application area of\\ntransformers, which is text\\nsummarization, which is one\\nof the application areas covered\\nin our survey\\n • The authors propose a transformer-based summarizer that\\n solves the issues of standard transformers that cannot take a\\nlong text as an input. They survey different use cases of apply-\\ning transformers to different text summarization tasks and they\\nonly cover text summarization. no proposed transformers have\\nbeen built in our survey.\\n R Kumar\\n(Kaliyar,\\n2020)\\n Natural\\nlanguage\\nProcessing-\\nNLP\\n • Discussion of different NLP\\ndownstream tasks that BERT\\nperforms. BERT is covered\\nin our survey as well as the\\ndifferent NLP tasks\\n • Survey different techniques on using BERT as a word-\\n embedder against traditional word-embedding techniques.\\nTheir survey is only focused as using transformers as a tool\\nfor embedding text\\n F Acheam-\\npong et al.\\n(Acheam-\\npong et al.,\\n2021)\\n Natural\\nlanguage\\nProcessing-\\nNLP\\n • Survey different transformer\\narchitectures that accomplish\\nthe emotion detection task. We\\ndo the same, the application of\\ndifferent transformers to the\\nsame type if task\\n • Survey the application of transformer architecture to a single\\n application area but in too much detail, which is emotion detec-\\ntion from text-based data, a form of sentiment analysis but the\\ngoal is to extract fine-grained emotion from the data. The task\\nof sentiment analysis is covered in our survey, but we didn’t\\ncover especially the task of detecting emotions on different lev-\\nels and not just as a binary classification task as usually done in\\nsentiment analysis\\n Continued on next page\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e0495180>, 'tokens': [{'text': 'Approach\\n', 'bbox': {'x1': 0.07006699346405229, 'y1': 0.12046538308080808, 'x2': 0.1390075343137255, 'y2': 0.13304442348484843}}, {'text': 'Fields of\\nApplication\\n', 'bbox': {'x1': 0.16759313725490196, 'y1': 0.11354619116161614, 'x2': 0.24859623790849675, 'y2': 0.13996235277777763}}, {'text': 'Similarities\\n', 'bbox': {'x1': 0.3413366013071895, 'y1': 0.12046538308080808, 'x2': 0.4209234499999999, 'y2': 0.13304442348484843}}, {'text': 'Differences\\n', 'bbox': {'x1': 0.6886797385620915, 'y1': 0.12046538308080808, 'x2': 0.7670294016339868, 'y2': 0.13304442348484843}}, {'text': 'Y. Tay et\\nal. (Tay\\net al., 2023)\\n', 'bbox': {'x1': 0.06565522875816994, 'y1': 0.18835394141414133, 'x2': 0.14341885653594771, 'y2': 0.2286072242424241}}, {'text': 'Performance\\n/Archi-\\ntecture\\n', 'bbox': {'x1': 0.16651143790849673, 'y1': 0.18835394141414133, 'x2': 0.24967961339869282, 'y2': 0.2286072242424241}}, {'text': '• Proposed a taxonomy consid-\\nering the primary use case of\\ntransformer models in language\\nand vision domains .\\n', 'bbox': {'x1': 0.27002450980392156, 'y1': 0.1839524262626262, 'x2': 0.4904711098039216, 'y2': 0.2380415676767675}}, {'text': 'A. M. P.\\nBras¸oveanu\\net al.\\n(Brasoveanu\\n& Andonie,\\n2020)\\n', 'bbox': {'x1': 0.06364379084967321, 'y1': 0.3234536888888888, 'x2': 0.14542827189542484, 'y2': 0.40521833535353524}}, {'text': 'Natural\\nlanguage\\nProcessing-\\nNLP\\n', 'bbox': {'x1': 0.17011601307189544, 'y1': 0.33729081010101014, 'x2': 0.24607269869281048, 'y2': 0.3913812141414142}}, {'text': '• Explain transformer architec-\\nture and explain its features.\\n', 'bbox': {'x1': 0.27002450980392156, 'y1': 0.3536443454545455, 'x2': 0.4755275529411766, 'y2': 0.38005924444444444}}, {'text': '• This existing survey compared the computational power and\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.16068096161616163, 'x2': 0.9190328179738566, 'y2': 0.17326000202020214}}, {'text': 'memory efficiency of transformer models, whereas our survey\\nfocuses on deep learning tasks and applications.\\n', 'bbox': {'x1': 0.526372549019608, 'y1': 0.1745180828282828, 'x2': 0.9311927071895428, 'y2': 0.2009329818181818}}, {'text': '• This exciting survey focused on language and vision domain\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.20722262828282814, 'x2': 0.9208072026143794, 'y2': 0.21980166868686865}}, {'text': 'only, while we cover other top five fields of transformer appli-\\ncations: NLP, computer vision, multi-modality, audio/speech,\\nand signal processing.\\n', 'bbox': {'x1': 0.526372549019608, 'y1': 0.2210597494949493, 'x2': 0.9306229506535949, 'y2': 0.26131303232323205}}, {'text': '• Our survey describes the transformer model and the significant\\nmodels’ working processing for a range of tasks. However, this\\nexisting paper focused on visualization techniques used to ex-\\nplain the most recent transformer architectures and explored\\ntwo large tool classes to explain the inner workings of Trans-\\nformers.\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.29577944646464643, 'x2': 0.9382414094771243, 'y2': 0.3775440929292927}}, {'text': '• we covered five fields of transformer applications: NLP, com-\\nputer vision, multi-modality, audio/speech, and signal process-\\ning and this exciting survey focused on the models for NLP\\nonly.\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.3838337393939392, 'x2': 0.9345135738562093, 'y2': 0.4379241434343432}}, {'text': 'W Guan et\\nal. (Wang\\net al.,\\n2020a)\\n', 'bbox': {'x1': 0.06927777777777779, 'y1': 0.4837112646464647, 'x2': 0.13979735816993466, 'y2': 0.5378016686868687}}, {'text': 'Natural\\nlanguage\\nProcessing-\\nNLP\\n', 'bbox': {'x1': 0.17011601307189544, 'y1': 0.4837112646464647, 'x2': 0.24607269869281048, 'y2': 0.5378016686868687}}, {'text': '• Survey an application area of\\ntransformers, which is text\\nsummarization, which is one\\nof the application areas covered\\nin our survey\\n', 'bbox': {'x1': 0.27002450980392156, 'y1': 0.4793084868686868, 'x2': 0.49040599477124186, 'y2': 0.5472360121212121}}, {'text': '• The authors propose a transformer-based summarizer that\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.47239055757575765, 'x2': 0.9021680245098044, 'y2': 0.484969597979798}}, {'text': 'solves the issues of standard transformers that cannot take a\\nlong text as an input. They survey different use cases of apply-\\ning transformers to different text summarization tasks and they\\nonly cover text summarization. no proposed transformers have\\nbeen built in our survey.\\n', 'bbox': {'x1': 0.526372549019608, 'y1': 0.48622767878787876, 'x2': 0.935099609150327, 'y2': 0.5541539414141414}}, {'text': 'R Kumar\\n(Kaliyar,\\n2020)\\n', 'bbox': {'x1': 0.07459150326797385, 'y1': 0.5999423252525252, 'x2': 0.1344810545751634, 'y2': 0.640195608080808}}, {'text': 'Natural\\nlanguage\\nProcessing-\\nNLP\\n', 'bbox': {'x1': 0.17011601307189544, 'y1': 0.5930243959595959, 'x2': 0.24607269869281048, 'y2': 0.6471135373737373}}, {'text': '• Discussion of different NLP\\ndownstream tasks that BERT\\nperforms. BERT is covered\\nin our survey as well as the\\ndifferent NLP tasks\\n', 'bbox': {'x1': 0.27002450980392156, 'y1': 0.5886216181818181, 'x2': 0.4728412147058823, 'y2': 0.6565478808080808}}, {'text': '• Survey different techniques on using BERT as a word-\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.5955395474747475, 'x2': 0.8815102803921568, 'y2': 0.6081185878787879}}, {'text': 'embedder against traditional word-embedding techniques.\\nTheir survey is only focused as using transformers as a tool\\nfor embedding text\\n', 'bbox': {'x1': 0.526372549019608, 'y1': 0.6093766686868687, 'x2': 0.9128953830065362, 'y2': 0.6496299515151515}}, {'text': 'F Acheam-\\npong et al.\\n(Acheam-\\npong et al.,\\n2021)\\n', 'bbox': {'x1': 0.0681454248366013, 'y1': 0.709254193939394, 'x2': 0.1409277526143791, 'y2': 0.7771817191919193}}, {'text': 'Natural\\nlanguage\\nProcessing-\\nNLP\\n', 'bbox': {'x1': 0.17011601307189544, 'y1': 0.7161733858585859, 'x2': 0.24607269869281048, 'y2': 0.77026378989899}}, {'text': '• Survey different transformer\\narchitectures that accomplish\\nthe emotion detection task. We\\ndo the same, the application of\\ndifferent transformers to the\\nsame type if task\\n', 'bbox': {'x1': 0.27002450980392156, 'y1': 0.7048514161616162, 'x2': 0.485636318627451, 'y2': 0.7866160626262626}}, {'text': '• Survey the application of transformer architecture to a single\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.6910155575757576, 'x2': 0.9219141581699349, 'y2': 0.703594597979798}}, {'text': 'application area but in too much detail, which is emotion detec-\\ntion from text-based data, a form of sentiment analysis but the\\ngoal is to extract fine-grained emotion from the data. The task\\nof sentiment analysis is covered in our survey, but we didn’t\\ncover especially the task of detecting emotions on different lev-\\nels and not just as a binary classification task as usually done in\\nsentiment analysis\\n', 'bbox': {'x1': 0.526372549019608, 'y1': 0.7048514161616162, 'x2': 0.9406506656862748, 'y2': 0.8004531838383839}}, {'text': 'Continued on next page\\n', 'bbox': {'x1': 0.7883888888888889, 'y1': 0.816554698989899, 'x2': 0.9432324366013072, 'y2': 0.8291337393939393}}]}\n",
      "{'type': 'Text', 'bbox': [0.7863330796185661, 0.8171506569602273, 0.9468753590303309, 0.8294523481889204], 'properties': {'score': 0.4578110873699188, 'page_number': 9}, 'text_representation': 'Continued on next page\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.36661850873161766, 0.09787196766246449, 0.6416806209788602, 0.11191655245694247], 'properties': {'score': 0.552659809589386, 'page_number': 10}, 'text_representation': 'Table 2 – continued from previous page\\n'}\n",
      "{'type': 'table', 'bbox': [0.04990065181956572, 0.11081066478382458, 0.9540403837316176, 0.8121501020951705], 'properties': {'score': 0.8430319428443909, 'title': None, 'columns': None, 'rows': None, 'page_number': 10}, 'text_representation': 'Approach\\n Fields of\\nApplication\\n Similarities\\n Differences\\n R Gruet-\\nzemacher et\\nal. (Gruet-\\nzemacher\\n& Paradice,\\n2022)\\n Natural\\nlanguage\\nProcessing-\\nNLP\\n • Survey the progress of trans-\\n formers in the text-mining ap-\\nplication area. We do cover\\nin our survey the progress of\\ntransformers on a wide variety\\nof tasks\\n • Tackle the different transformers on how they can be used as\\ntext miners for organizations that have huge amounts of un-\\nstructured data against traditional NLP text-mining techniques\\n J. Selva et\\nal. (Selva\\net al., 2023)\\n Computer\\nVision\\n • This paper is an overview of\\ntransformers developed for\\nmodeling images and video\\ndata\\n • This survey focuses solely on image and video data. Models\\nare compared based on their performance in video classifica-\\ntion, it does not cover any other applications. The paper pro-\\nposes a taxonomy of various transformer models based on their\\nrecurrence properties, memory capacities, and architectural de-\\nsign\\n K. S.\\nKalyan et\\nal. (Subra-\\nmanyam\\net al.,\\n2021a)\\n Natural\\nlanguage\\nProcessing-\\nMedical\\n • This paper provides an\\n overview of the developed\\ntransformer-based BPLMs for\\na wide range of NLP tasks,\\nincluding Natural language\\ninference, Entity extraction,\\nRelation extraction, Semantic\\ntextual similarity, Text classi-\\nfication, Question answering,\\nand Text summarization\\n • This survey addresses only transformer-based biomedical pre-\\ntrained language models, which restricts its scope to the spe-\\ncific field of biomedical natural language processing. The tax-\\nonomy does not distinguish models based on the type of ap-\\nplication they are used for, but rather based on the dataset of\\npre-training, the embedding type, and other criteria such as the\\ntargeted language\\n K. Han et\\nal. (Han\\net al., 2023)\\n Computer\\nVision\\n • Categorized vision transformer\\nmodels based on different tasks\\n • This existing paper analyzed transformer models’ advantages\\nand disadvantages, and efficient transformer methods for the\\nbackbone network, while our survey categorizes transformer\\nmodels based on tasks and summarize downstream tasks and\\ncommonly used dataset.\\n • While our survey paper classified computer vision tasks into\\n two segments: natural image processing & medical image pro-\\ncessing and then focused on popular computer vision like vi-\\nsual question answering, classification, segmentation, ques-\\ntion answering, and so on, then this existing paper focused on\\nhigh/mid-level vision, low-level vision, and video processing\\ncomputer vision tasks.\\n • This survey focused on computer vision tasks only, while we\\ncovered other four fields of applications-NLP, Multi-modal,\\nAudio/speech, and signal processing besides computer vision\\n Continued on next page\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e0496080>, 'tokens': [{'text': 'Approach\\n', 'bbox': {'x1': 0.07006699346405229, 'y1': 0.12046538308080808, 'x2': 0.1390075343137255, 'y2': 0.13304442348484843}}, {'text': 'Fields of\\nApplication\\n', 'bbox': {'x1': 0.16759313725490196, 'y1': 0.11354619116161614, 'x2': 0.24859623790849675, 'y2': 0.13996235277777763}}, {'text': 'Similarities\\n', 'bbox': {'x1': 0.3413366013071895, 'y1': 0.12046538308080808, 'x2': 0.4209234499999999, 'y2': 0.13304442348484843}}, {'text': 'Differences\\n', 'bbox': {'x1': 0.6886797385620915, 'y1': 0.12046538308080808, 'x2': 0.7670294016339868, 'y2': 0.13304442348484843}}, {'text': 'R Gruet-\\nzemacher et\\nal. (Gruet-\\nzemacher\\n& Paradice,\\n2022)\\n', 'bbox': {'x1': 0.06544444444444444, 'y1': 0.15816454747474742, 'x2': 0.14363131993464054, 'y2': 0.23992919393939363}}, {'text': 'Natural\\nlanguage\\nProcessing-\\nNLP\\n', 'bbox': {'x1': 0.17011601307189544, 'y1': 0.1720016686868686, 'x2': 0.24607269869281048, 'y2': 0.2260920727272725}}, {'text': '• Survey the progress of trans-\\n', 'bbox': {'x1': 0.27002450980392156, 'y1': 0.16068096161616163, 'x2': 0.47144158464052294, 'y2': 0.17326000202020214}}, {'text': 'formers in the text-mining ap-\\nplication area. We do cover\\nin our survey the progress of\\ntransformers on a wide variety\\nof tasks\\n', 'bbox': {'x1': 0.28386111111111106, 'y1': 0.1745180828282828, 'x2': 0.4823642882352941, 'y2': 0.24244434545454527}}, {'text': '• Tackle the different transformers on how they can be used as\\ntext miners for organizations that have huge amounts of un-\\nstructured data against traditional NLP text-mining techniques\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.18143601212121213, 'x2': 0.9320717601307191, 'y2': 0.2216892949494949}}, {'text': 'J. Selva et\\nal. (Selva\\net al., 2023)\\n', 'bbox': {'x1': 0.06565522875816993, 'y1': 0.2951506585858586, 'x2': 0.14341885653594771, 'y2': 0.33540394141414137}}, {'text': 'Computer\\nVision\\n', 'bbox': {'x1': 0.1755375816993464, 'y1': 0.3020698505050504, 'x2': 0.24065261437908497, 'y2': 0.32848601212121203}}, {'text': '• This paper is an overview of\\ntransformers developed for\\nmodeling images and video\\ndata\\n', 'bbox': {'x1': 0.27002450980392156, 'y1': 0.2907478808080807, 'x2': 0.4687718683006536, 'y2': 0.34483828484848467}}, {'text': '• This survey focuses solely on image and video data. Models\\nare compared based on their performance in video classifica-\\ntion, it does not cover any other applications. The paper pro-\\nposes a taxonomy of various transformer models based on their\\nrecurrence properties, memory capacities, and architectural de-\\nsign\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.27691075959595957, 'x2': 0.9387460509803923, 'y2': 0.3586754060606058}}, {'text': 'K. S.\\nKalyan et\\nal. (Subra-\\nmanyam\\net al.,\\n2021a)\\n', 'bbox': {'x1': 0.06969281045751634, 'y1': 0.4183009111111111, 'x2': 0.13938217418300655, 'y2': 0.500064294949495}}, {'text': 'Natural\\nlanguage\\nProcessing-\\nMedical\\n', 'bbox': {'x1': 0.17011601307189544, 'y1': 0.4321367696969696, 'x2': 0.24607269869281048, 'y2': 0.48622717373737373}}, {'text': '• This paper provides an\\n', 'bbox': {'x1': 0.27002450980392156, 'y1': 0.39314182020202015, 'x2': 0.4323562862745097, 'y2': 0.40572086060606055}}, {'text': 'overview of the developed\\ntransformer-based BPLMs for\\na wide range of NLP tasks,\\nincluding Natural language\\ninference, Entity extraction,\\nRelation extraction, Semantic\\ntextual similarity, Text classi-\\nfication, Question answering,\\nand Text summarization\\n', 'bbox': {'x1': 0.28386111111111106, 'y1': 0.4069789414141414, 'x2': 0.4797434081699347, 'y2': 0.5302536888888889}}, {'text': '• This survey addresses only transformer-based biomedical pre-\\ntrained language models, which restricts its scope to the spe-\\ncific field of biomedical natural language processing. The tax-\\nonomy does not distinguish models based on the type of ap-\\nplication they are used for, but rather based on the dataset of\\npre-training, the embedding type, and other criteria such as the\\ntargeted language\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.41389813333333336, 'x2': 0.9346112464052291, 'y2': 0.5094986383838384}}, {'text': 'K. Han et\\nal. (Han\\net al., 2023)\\n', 'bbox': {'x1': 0.06565522875816994, 'y1': 0.6502592444444445, 'x2': 0.14341885653594771, 'y2': 0.6905112646464646}}, {'text': 'Computer\\nVision\\n', 'bbox': {'x1': 0.1755375816993464, 'y1': 0.6571771737373738, 'x2': 0.24065261437908497, 'y2': 0.6835933353535354}}, {'text': '• Categorized vision transformer\\nmodels based on different tasks\\n', 'bbox': {'x1': 0.27002450980392156, 'y1': 0.6596935878787878, 'x2': 0.4887130039215687, 'y2': 0.6861084868686869}}, {'text': '• This existing paper analyzed transformer models’ advantages\\nand disadvantages, and efficient transformer methods for the\\nbackbone network, while our survey categorizes transformer\\nmodels based on tasks and summarize downstream tasks and\\ncommonly used dataset.\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.5647213656565657, 'x2': 0.925365254901961, 'y2': 0.6326476282828283}}, {'text': '• While our survey paper classified computer vision tasks into\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.6389372747474747, 'x2': 0.9192444418300656, 'y2': 0.6515163151515152}}, {'text': 'two segments: natural image processing & medical image pro-\\ncessing and then focused on popular computer vision like vi-\\nsual question answering, classification, segmentation, ques-\\ntion answering, and so on, then this existing paper focused on\\nhigh/mid-level vision, low-level vision, and video processing\\ncomputer vision tasks.\\n', 'bbox': {'x1': 0.526372549019608, 'y1': 0.6527743959595961, 'x2': 0.9347577552287584, 'y2': 0.7345390424242425}}, {'text': '• This survey focused on computer vision tasks only, while we\\ncovered other four fields of applications-NLP, Multi-modal,\\nAudio/speech, and signal processing besides computer vision\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.740828688888889, 'x2': 0.9256090931372553, 'y2': 0.7810807090909092}}, {'text': 'Continued on next page\\n', 'bbox': {'x1': 0.7883888888888889, 'y1': 0.7971822242424241, 'x2': 0.9432324366013072, 'y2': 0.8097612646464647}}]}\n",
      "{'type': 'Text', 'bbox': [0.7876283174402574, 0.798456864790483, 0.9456821576286765, 0.8098092928799716], 'properties': {'score': 0.4984877109527588, 'page_number': 10}, 'text_representation': 'Continued on next page\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.366372465245864, 0.09744531111283736, 0.6417081227022059, 0.11204209761186079], 'properties': {'score': 0.5252259969711304, 'page_number': 11}, 'text_representation': 'Table 2 – continued from previous page\\n'}\n",
      "{'type': 'table', 'bbox': [0.0491087116914637, 0.11063697814941406, 0.9548430319393383, 0.887091064453125], 'properties': {'score': 0.8294187188148499, 'title': None, 'columns': None, 'rows': None, 'page_number': 11}, 'text_representation': 'Approach\\n Fields of\\nApplication\\n Similarities\\n Differences\\n Y. Xu et\\nal. (Xu\\net al., 2022)\\n Computer\\nVision\\n • The survey covers the fields\\n of computer vision and multi-\\nmodal in a similar fashion to\\nour survey\\n J Li et al.\\n(Li et al.,\\n2023)\\n Computer\\nVision\\n • Comparative analysis of trans-\\nformer models is presented\\nin this paper for several tasks\\ninvolved in medical vision.\\nSeveral criteria are considered\\nwhen comparing papers, in-\\ncluding the type of dataset, the\\ntype of input data, and the ar-\\nchitecture of the model\\n • This survey focuses primarily on recent advancements in com-\\nputer vision by comparing the performance of different trans-\\nformer models. Specifically, this study discusses four areas of\\nresearch: advances in the design of the ViT models for image\\nclassification, high-level vision tasks (such as object detection\\nand semantic segmentation), low-level vision tasks (such as\\nsuper-resolution, and image generation), and multimodal learn-\\ning (such as visual question answering (VQA), image caption-\\ning)\\n • This paper describes in detail several transformer models that\\nhave been developed for medical images; however, it does not\\nprovide information regarding medical signals\\n F Shamshad\\net al.\\n(Shamshad\\net al., 2023)\\n Computer\\nVision-\\nmedical\\n • A review of a number of trans-\\nformer models with a focus\\non some tasks related to medi-\\ncal images and different image\\nmodalities, and a description of\\nthe datasets used for these tasks\\n • This paper compares deep learning models starting with CNNs\\nand moving up to vision transformers. In this paper, medical\\nimage modalities and several medical computer vision tasks\\nare discussed to compare papers through the specification of\\ndatasets used and also provide an overview of models’ perfor-\\nmance. In this paper, the comparison is based solely on medical\\nimages; medical signals are not considered\\n Salman\\nKhan et\\nal. (Khan\\net al., 2022)\\n Computer\\nVision\\n • A overview of existing trans-\\n former computer vision models\\nand classified the models based\\non popular tasks\\n • While this existing survey paper compared the popular tech-\\n niques in terms of architectural design and experimental value,\\nwhile our survey worked based on popular tasks and applica-\\ntions.\\n • In the computer vision section, we put a special focus on Medi-\\n cal image tasks besides natural image processing.\\n • This survey focused on computer vision tasks only, while we\\ncovered other four fields of applications, namely NLP, Multi-\\nmodal, Audio/speech, and signal processing besides computer\\nvision\\n L. Ruan et\\nal. (Ruan &\\nJin, 2022)\\n Multi-\\nmodal(NLP-\\nCV)\\n • Categorize transformer vision-\\n language models based on tasks\\nand summarize downstream\\ntasks and commonly used video\\ndataset\\n • This existing survey focused on multi-modal(NLP-CV) tasks\\nonly, while we covered other four fields of applications-NLP,\\nComputer vision, Audio/speech, and signal processing besides\\nmulti-modal\\n Continued on next page\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e0496d40>, 'tokens': [{'text': 'Approach\\n', 'bbox': {'x1': 0.07006699346405229, 'y1': 0.12046538308080808, 'x2': 0.1390075343137255, 'y2': 0.13304442348484843}}, {'text': 'Fields of\\nApplication\\n', 'bbox': {'x1': 0.16759313725490196, 'y1': 0.11354619116161614, 'x2': 0.24859623790849675, 'y2': 0.13996235277777763}}, {'text': 'Similarities\\n', 'bbox': {'x1': 0.3413366013071895, 'y1': 0.12046538308080808, 'x2': 0.4209234499999999, 'y2': 0.13304442348484843}}, {'text': 'Differences\\n', 'bbox': {'x1': 0.6886797385620915, 'y1': 0.12046538308080808, 'x2': 0.7670294016339868, 'y2': 0.13304442348484843}}, {'text': 'Y. Xu et\\nal. (Xu\\net al., 2022)\\n', 'bbox': {'x1': 0.06565522875816993, 'y1': 0.19967591111111102, 'x2': 0.14341885653594771, 'y2': 0.2399291939393938}}, {'text': 'Computer\\nVision\\n', 'bbox': {'x1': 0.1755375816993464, 'y1': 0.20659384040404036, 'x2': 0.24065261437908497, 'y2': 0.233010002020202}}, {'text': '• The survey covers the fields\\n', 'bbox': {'x1': 0.27002450980392156, 'y1': 0.19527313333333327, 'x2': 0.4655323954248366, 'y2': 0.20785217373737377}}, {'text': 'of computer vision and multi-\\nmodal in a similar fashion to\\nour survey\\n', 'bbox': {'x1': 0.28386111111111106, 'y1': 0.20911025454545443, 'x2': 0.47829459869281055, 'y2': 0.2493635373737372}}, {'text': 'J Li et al.\\n(Li et al.,\\n2023)\\n', 'bbox': {'x1': 0.07424183006535948, 'y1': 0.3574170727272727, 'x2': 0.1348313679738562, 'y2': 0.3976703555555555}}, {'text': 'Computer\\nVision\\n', 'bbox': {'x1': 0.1755375816993464, 'y1': 0.3643362646464646, 'x2': 0.24065261437908497, 'y2': 0.39075242626262624}}, {'text': '• Comparative analysis of trans-\\nformer models is presented\\nin this paper for several tasks\\ninvolved in medical vision.\\nSeveral criteria are considered\\nwhen comparing papers, in-\\ncluding the type of dataset, the\\ntype of input data, and the ar-\\nchitecture of the model\\n', 'bbox': {'x1': 0.27002450980392156, 'y1': 0.31842212323232316, 'x2': 0.4836991464052288, 'y2': 0.44169687070707064}}, {'text': '• This survey focuses primarily on recent advancements in com-\\nputer vision by comparing the performance of different trans-\\nformer models. Specifically, this study discusses four areas of\\nresearch: advances in the design of the ViT models for image\\nclassification, high-level vision tasks (such as object detection\\nand semantic segmentation), low-level vision tasks (such as\\nsuper-resolution, and image generation), and multimodal learn-\\ning (such as visual question answering (VQA), image caption-\\ning)\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.16068096161616163, 'x2': 0.9383716395424838, 'y2': 0.28395570909090867}}, {'text': '• This paper describes in detail several transformer models that\\nhave been developed for medical images; however, it does not\\nprovide information regarding medical signals\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.35993348686868687, 'x2': 0.9294183225490198, 'y2': 0.40018676969696965}}, {'text': 'F Shamshad\\net al.\\n(Shamshad\\net al., 2023)\\n', 'bbox': {'x1': 0.06451633986928104, 'y1': 0.49440318383838383, 'x2': 0.1445589937908497, 'y2': 0.5484935878787879}}, {'text': 'Computer\\nVision-\\nmedical\\n', 'bbox': {'x1': 0.1755375816993464, 'y1': 0.5013223757575758, 'x2': 0.24065261437908497, 'y2': 0.5415756585858585}}, {'text': '• A review of a number of trans-\\nformer models with a focus\\non some tasks related to medi-\\ncal images and different image\\nmodalities, and a description of\\nthe datasets used for these tasks\\n', 'bbox': {'x1': 0.27002450980392156, 'y1': 0.4830824767676768, 'x2': 0.4904711098039216, 'y2': 0.5648471232323232}}, {'text': '• This paper compares deep learning models starting with CNNs\\nand moving up to vision transformers. In this paper, medical\\nimage modalities and several medical computer vision tasks\\nare discussed to compare papers through the specification of\\ndatasets used and also provide an overview of models’ perfor-\\nmance. In this paper, the comparison is based solely on medical\\nimages; medical signals are not considered\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.4761645474747474, 'x2': 0.9417413424836605, 'y2': 0.5717650525252526}}, {'text': 'Salman\\nKhan et\\nal. (Khan\\net al., 2022)\\n', 'bbox': {'x1': 0.06565522875816993, 'y1': 0.6502592444444445, 'x2': 0.14341885653594771, 'y2': 0.704348385858586}}, {'text': 'Computer\\nVision\\n', 'bbox': {'x1': 0.1755375816993464, 'y1': 0.664095103030303, 'x2': 0.24065261437908497, 'y2': 0.6905112646464646}}, {'text': '• A overview of existing trans-\\n', 'bbox': {'x1': 0.27002450980392156, 'y1': 0.652774395959596, 'x2': 0.4735089869281047, 'y2': 0.6653534363636363}}, {'text': 'former computer vision models\\nand classified the models based\\non popular tasks\\n', 'bbox': {'x1': 0.28386111111111106, 'y1': 0.6666115171717172, 'x2': 0.48868044640522873, 'y2': 0.7068648}}, {'text': '• While this existing survey paper compared the popular tech-\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.6062314666666667, 'x2': 0.9189677029411766, 'y2': 0.6188105070707071}}, {'text': 'niques in terms of architectural design and experimental value,\\nwhile our survey worked based on popular tasks and applica-\\ntions.\\n', 'bbox': {'x1': 0.526372549019608, 'y1': 0.6200685878787879, 'x2': 0.9348717065359479, 'y2': 0.6603218707070707}}, {'text': '• In the computer vision section, we put a special focus on Medi-\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.6666115171717172, 'x2': 0.9396091683006536, 'y2': 0.6791905575757576}}, {'text': 'cal image tasks besides natural image processing.\\n', 'bbox': {'x1': 0.526372549019608, 'y1': 0.6804486383838385, 'x2': 0.8487245183006539, 'y2': 0.6930276787878789}}, {'text': '• This survey focused on computer vision tasks only, while we\\ncovered other four fields of applications, namely NLP, Multi-\\nmodal, Audio/speech, and signal processing besides computer\\nvision\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.6993173252525253, 'x2': 0.9305741143790851, 'y2': 0.7534077292929293}}, {'text': 'L. Ruan et\\nal. (Ruan &\\nJin, 2022)\\n', 'bbox': {'x1': 0.06584313725490196, 'y1': 0.7991948505050505, 'x2': 0.14323235359477127, 'y2': 0.8394481333333332}}, {'text': 'Multi-\\nmodal(NLP-\\nCV)\\n', 'bbox': {'x1': 0.16695098039215686, 'y1': 0.7991948505050505, 'x2': 0.24924010294117646, 'y2': 0.8394481333333332}}, {'text': '• Categorize transformer vision-\\n', 'bbox': {'x1': 0.27002450980392156, 'y1': 0.7878741434343434, 'x2': 0.4834390294117648, 'y2': 0.8004531838383837}}, {'text': 'language models based on tasks\\nand summarize downstream\\ntasks and commonly used video\\ndataset\\n', 'bbox': {'x1': 0.28386111111111106, 'y1': 0.8017112646464646, 'x2': 0.49185480424836603, 'y2': 0.855800406060606}}, {'text': '• This existing survey focused on multi-modal(NLP-CV) tasks\\nonly, while we covered other four fields of applications-NLP,\\nComputer vision, Audio/speech, and signal processing besides\\nmulti-modal\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.7947920727272728, 'x2': 0.9333089457516343, 'y2': 0.8488824767676768}}, {'text': 'Continued on next page\\n', 'bbox': {'x1': 0.7883888888888889, 'y1': 0.8719019212121213, 'x2': 0.9432324366013072, 'y2': 0.8844809616161616}}]}\n",
      "{'type': 'table', 'bbox': [0.04955187629250919, 0.10977001536976208, 0.9560895852481618, 0.231632343639027], 'properties': {'score': 0.7030977606773376, 'title': None, 'columns': None, 'rows': None, 'page_number': 12}, 'text_representation': 'Approach\\n A Shin et\\nal. (Shin\\net al., 2022)\\n Fields of\\nApplication\\n Multi-modal\\n(Perfor-\\nmance\\n/Archi-\\ntecture)\\n Similarities\\n Differences\\n • They survey transformers for\\nmulti-modal tasks, which we\\ndo also include in our different\\napplication tasks\\n • Cover only one application area in detail, which is multimodal\\n visual-linguistic tasks\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e0497b80>, 'tokens': [{'text': 'Approach\\n', 'bbox': {'x1': 0.07006699346405229, 'y1': 0.12046538308080808, 'x2': 0.1390075343137255, 'y2': 0.13304442348484843}}, {'text': 'A Shin et\\nal. (Shin\\net al., 2022)\\n', 'bbox': {'x1': 0.06565522875816993, 'y1': 0.1650837393939394, 'x2': 0.14341885653594771, 'y2': 0.20533575959595957}}, {'text': 'Fields of\\nApplication\\n', 'bbox': {'x1': 0.16759313725490196, 'y1': 0.11354619116161614, 'x2': 0.24859623790849675, 'y2': 0.13996235277777763}}, {'text': 'Multi-modal\\n(Perfor-\\nmance\\n/Archi-\\ntecture)\\n', 'bbox': {'x1': 0.16694281045751633, 'y1': 0.1512466181818181, 'x2': 0.24924821176470588, 'y2': 0.21917288080808056}}, {'text': 'Similarities\\n', 'bbox': {'x1': 0.3413366013071895, 'y1': 0.12046538308080808, 'x2': 0.4209234499999999, 'y2': 0.13304442348484843}}, {'text': 'Differences\\n', 'bbox': {'x1': 0.6886797385620915, 'y1': 0.12046538308080808, 'x2': 0.7670294016339868, 'y2': 0.13304442348484843}}, {'text': '• They survey transformers for\\nmulti-modal tasks, which we\\ndo also include in our different\\napplication tasks\\n', 'bbox': {'x1': 0.27002450980392156, 'y1': 0.16068096161616163, 'x2': 0.484643314379085, 'y2': 0.21477010303030297}}, {'text': '• Cover only one application area in detail, which is multimodal\\n', 'bbox': {'x1': 0.5125359477124184, 'y1': 0.1745180828282828, 'x2': 0.9328209261437911, 'y2': 0.18709712323232328}}, {'text': 'visual-linguistic tasks\\n', 'bbox': {'x1': 0.526372549019608, 'y1': 0.18835394141414133, 'x2': 0.6679163513071896, 'y2': 0.2009329818181818}}]}\n",
      "{'type': 'Caption', 'bbox': [0.2513771326401654, 0.24169403076171875, 0.7515293255974265, 0.2558660333806818], 'properties': {'score': 0.7129295468330383, 'page_number': 12}, 'text_representation': 'Table 2: Comparative summary between our survey and the existing surveys\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09406712251551011, 0.2742301802201704, 0.9156979549632352, 0.496019287109375], 'properties': {'score': 0.9227207899093628, 'page_number': 12}, 'text_representation': 'After having a thorough search and analysis of these survey papers, we realized that still, a survey on transformers is missing\\nwhich focused on a most common field of application together and discussed the contribution of transformer-based mod-\\nels in the execution of different deep learning tasks in regarding fields of application. In this paper, first, we surveyed all\\ntransformer-based models out there based on our best possible search, identified top-5 fields of application and the proportion\\nof transformers models’ contribution in the progression of top fields of application: Natural Language Processing (NLP),\\nComputer Vision (CV), Multi-modal, Audio and Speech, and Signal processing. Moreover, we proposed a taxonomy of\\ntransformer models based on these top five fields of application whereas top performed, and significant models are being\\nclassified and analyzed based on their task’s execution under the regarding fields. Through this survey, different aspects of\\nTransformer-based models’ tasks and applications become more explicit in different fields, and it also depicted the fields of\\ntransformer applications that got higher and less attention by the researchers so far. Based on this analysis, we discussed\\nfuture prospects and possibilities of transformers application in different fields of application. One of the objectives of this\\nsurvey is to make a combined source of reference for a better understanding of the contribution of transformer models in\\ndifferent fields and the characteristics and execution methods of the models which kept significant contributions to improving\\nthe performance of the different tasks in their fields. Besides, this paper would be a resource to perceive future possibilities\\nand scope of transformer-based models’ application for enthusiastic researchers who wants to extend and work for the new\\napplication of transformers.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09492451387293198, 0.5084450461647727, 0.43513072294347427, 0.5258063299005682], 'properties': {'score': 0.8423537015914917, 'page_number': 12}, 'text_representation': '5 TRANSFORMER APPLICATIONS\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09436565623563879, 0.5336076216264205, 0.9151254451976103, 0.6444382546164773], 'properties': {'score': 0.7357239127159119, 'page_number': 12}}\n",
      "{'type': 'Text', 'bbox': [0.09431019502527574, 0.6453096701882103, 0.9125213982077206, 0.6869352028586647], 'properties': {'score': 0.5593754649162292, 'page_number': 12}}\n",
      "{'type': 'Text', 'bbox': [0.09429525936351103, 0.6873921342329545, 0.913807373046875, 0.812010664506392], 'properties': {'score': 0.6683606505393982, 'page_number': 12}}\n",
      "{'type': 'Text', 'bbox': [0.0941426176183364, 0.8128770862926137, 0.9098537310431986, 0.840870361328125], 'properties': {'score': 0.5117563009262085, 'page_number': 12}}\n",
      "{'type': 'Section-header', 'bbox': [0.09496146706973806, 0.8541802423650569, 0.8412961713005515, 0.8704847301136364], 'properties': {'score': 0.8782804012298584, 'page_number': 12}, 'text_representation': '6 APPLICATION-BASED CLASSIFICATION TAXONOMY OF TRANSFORMERS\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09445917466107537, 0.8792340087890625, 0.9131451775045956, 0.9480970348011364], 'properties': {'score': 0.9224737286567688, 'page_number': 12}, 'text_representation': 'As a result of conducting a thorough comprehensive analysis of all selected articles following the methodology explained\\nin Section 3, we noticed that the existing categorizations did not fully capture the wide range of transformer-based models\\nand their diverse applications across different fields. Hence, in this study, we aimed to propose a more comprehensive\\ntaxonomy of transformers that would reflect their practical applications. To achieve this, we carefully reviewed a large\\nnumber of transformer models and classified them based on their tasks within their respective fields of application. Our\\n'}\n",
      "{'type': 'Image', 'bbox': [0.200178420122932, 0.10135084672407671, 0.7604072122012868, 0.3709715409712358], 'properties': {'score': 0.9156967997550964, 'image_size': None, 'image_mode': None, 'image_format': None, 'page_number': 13}}\n",
      "{'type': 'Caption', 'bbox': [0.29638442095588236, 0.3988566728071733, 0.7036597397748162, 0.41348860307173296], 'properties': {'score': 0.8922095894813538, 'page_number': 13}, 'text_representation': 'Figure 4: Proportion of transformer application in Top-5 fields\\n'}\n",
      "{'type': 'Text', 'bbox': [0.0941096586339614, 0.4409844970703125, 0.9114749684053309, 0.5244243275035512], 'properties': {'score': 0.9404101371765137, 'page_number': 13}, 'text_representation': 'analysis identified several highly impactful and significant transformer-based models that have been successfully applied in a\\nvariety of fields. We then organized these models into five different application areas: Natural Language Processing (NLP),\\nComputer Vision, Multi-modality, Audio and Speech, and Signal Processing. The proposed taxonomy in Figure 5 provides a\\nmore nuanced and comprehensive framework for understanding the diverse applications of transformers. We believe that this\\ntaxonomy would be beneficial for researchers and practitioners working on transformer-based models, as it would help them\\nto identify the most relevant models and techniques for their specific applications.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09432874791762408, 0.5349386319247159, 0.4283458754595588, 0.5488609730113636], 'properties': {'score': 0.5769475102424622, 'page_number': 13}, 'text_representation': '6.1 NATURAL LANGUAGE PROCESSING (NLP)\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09420275519875919, 0.554216641512784, 0.9114491900275735, 0.6789424272017045], 'properties': {'score': 0.9373217821121216, 'page_number': 13}, 'text_representation': 'Transformers have become a vital tool in NLP, and various NLP tasks have largely benefited from these models. Our pro-\\nposed taxonomy focuses on NLP and organizes transformer models into seven popular NLP tasks, including Translation,\\nSummarization, Classification and Segmentation, Question Answering, Text Generation, Natural Language Reasoning, and\\nAutomated Symbolic Reasoning. To ensure a comprehensive analysis, we only considered transformer models that have\\nsignificantly impacted the NLP field and improved its performance. Our analysis included an in-depth discussion of each\\nNLP task, along with essential information about each model presented in Table 3. We also highlighted the significance and\\nworking methods of each model. This taxonomy provides a valuable framework for understanding the different transformer\\nmodels used in NLP and their practical applications. It can help researchers and practitioners select the most appropriate\\ntransformer model for their specific NLP task.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09403093225815717, 0.6882870760830966, 0.3313993925206801, 0.7015783136541193], 'properties': {'score': 0.6869376301765442, 'page_number': 13}}\n",
      "{'type': 'Text', 'bbox': [0.09421804989085478, 0.7053911798650568, 0.9125046673943015, 0.8583580433238637], 'properties': {'score': 0.9374216794967651, 'page_number': 13}, 'text_representation': '6.1.1 LANGUAGE TRANSLATION\\nLanguage translation is a fundamental task in NLP, aimed at converting input text from one language to another. Its primary\\nobjective is to produce an output text that accurately reflects the meaning of the source text in the desired language. For\\nexample, given an English sentence as input text, the task aims to produce its equivalent in French or any other desired\\nlanguage. The original transformer model was developed explicitly for the purpose of language translation, highlighting the\\nsignificance of this task in the NLP field. Table 3 identifies the transformer-based models that have demonstrated significant\\nperformance in the Language Translation task. These models play a vital role in facilitating effective communication and\\ncollaboration across different languages, enabling more efficient information exchange and knowledge sharing. Overall, the\\nlanguage translation task represents a crucial area of research in NLP, with significant implications for diverse applications,\\nincluding business, science, education, and social interactions. The transformer-based models presented in the table offer\\npromising solutions for advancing the state-of-the-art in this field, paving the way for new and innovative approaches to\\nlanguage translation (Chowdhary & Chowdhary, 2020, Monroe, 2017, Hirschberg & Manning, 2015).\\n'}\n",
      "{'type': 'Image', 'bbox': [0.039995758954216455, 0.09361287897283381, 0.9886391314338235, 0.9601047585227273], 'properties': {'score': 0.8312069773674011, 'image_size': None, 'image_mode': None, 'image_format': None, 'page_number': 14}}\n",
      "{'type': 'Caption', 'bbox': [0.29908037971047796, 0.9732076748934659, 0.7033281393612132, 0.9876032049005682], 'properties': {'score': 0.7558093070983887, 'page_number': 14}, 'text_representation': 'Figure 5: Application-based taxonomy of transformer models\\n'}\n",
      "{'type': 'table', 'bbox': [0.051768987319048713, 0.09733564897017045, 0.9386154354319853, 0.5183667547052557], 'properties': {'score': 0.7494960427284241, 'title': None, 'columns': None, 'rows': None, 'page_number': 15}, 'text_representation': 'Transformer\\nModels\\n Transformer-\\n2017\\n(Vaswani\\net al., 2017)\\n Task Ac-\\ncomplished\\n Year\\n Architecture\\n(Encoder/\\nDecoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training\\nDataset\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n Language\\nTranslation\\n 2017\\n Encoder &\\nDecoder\\n No\\n NA\\n XLM\\n(Conneau\\n& Lample,\\n2019)\\n Translation and\\nClassification\\nfor multiple\\nlanguage\\n 2019\\n Encoder &\\nDecoder\\n Yes\\n 2019\\n Encoder &\\nDecoder\\n Yes\\n BART (Lewis\\net al., 2020)\\n Switch\\nTransformer\\n(Fedus\\net al., 2021)\\n Language\\nTranslation,\\nSentence\\nReconstruction,\\nComprehension,\\ntext Generation\\n Language\\nunderstanding\\ntask- Translation,\\nquestion\\nanswering,\\nClassification,\\nand so on.\\n 2021\\n Encoder &\\nDecoder\\n Yes\\n C4(Colossal Clean\\nCrawled Corpus)\\n WMT’16, WMT’14\\nEnglish-French,\\nWMT’16\\n(English-German,\\nEnglish-Romanian,\\nRomanian-English)\\nCorrupting docu-\\nments, 1M steps\\non a combina-\\ntion of books and\\nWikipedia data,\\nnews, stories, and\\nweb text (Training)\\n WMT 2014 English-\\nGerman,WMT 2014\\nEnglish-French\\n Wikipedia of 16 XNLI lan-\\nguages(English, French, Span-\\nish, Russian, Arabic, Chinese,\\nHindi, German, Greek, Bul-\\ngarian, Turkish, Vietnamese,\\nThai, Urdu, Swahili, Japanese)\\n SQuAD, MNLI, ELI5,\\nXSum, ConvAI2, CNN/DM,\\nCNN/DailyMail, WMT16\\nRomanian-English, augmented\\nwith back-translation data\\nfrom Sennrich et al. (2016).\\n GLUE and SuperGLUE\\nbenchmarks, CNNDM, BBC\\nXSum, and SQuAD data sets,\\nARC Reasoning Challenge,3\\nclosed-book question an-\\nswering data sets (Natural\\nQuestions, Web Questions,\\nand Trivia QA), Wino-\\ngrande Schema Challenge,\\nAdversarial NLI Benchmark\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e04dc7c0>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06615849673202615, 'y1': 0.10662826186868693, 'x2': 0.15495912254901958, 'y2': 0.13304442348484843}}, {'text': 'Transformer-\\n2017\\n(Vaswani\\net al., 2017)\\n', 'bbox': {'x1': 0.06761437908496733, 'y1': 0.14181227474747468, 'x2': 0.1535011071895425, 'y2': 0.195901416161616}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.19561274509803922, 'y1': 0.10662826186868693, 'x2': 0.27430426209150327, 'y2': 0.13304442348484843}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.11354619116161614, 'x2': 0.3455820872549019, 'y2': 0.12612523156565647}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\n', 'bbox': {'x1': 0.3656486928104575, 'y1': 0.099709069949495, 'x2': 0.4536516594771242, 'y2': 0.13996235277777763}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.4746160130718954, 'y1': 0.099709069949495, 'x2': 0.5379078248366013, 'y2': 0.13996235277777763}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.590173202614379, 'y1': 0.10662826186868693, 'x2': 0.6757831918300653, 'y2': 0.13304442348484843}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7508382352941176, 'y1': 0.10662826186868693, 'x2': 0.8982423905228758, 'y2': 0.13304442348484843}}, {'text': 'Language\\nTranslation\\n', 'bbox': {'x1': 0.19816830065359478, 'y1': 0.15564939595959598, 'x2': 0.2717482875816994, 'y2': 0.182064294949495}}, {'text': '2017\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.16256732525252518, 'x2': 0.3455983660130719, 'y2': 0.17514636565656566}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.37184803921568627, 'y1': 0.15564939595959598, 'x2': 0.44282342483660136, 'y2': 0.182064294949495}}, {'text': 'No\\n', 'bbox': {'x1': 0.4963153594771242, 'y1': 0.16256732525252518, 'x2': 0.5162080019607842, 'y2': 0.17514636565656566}}, {'text': 'NA\\n', 'bbox': {'x1': 0.6215098039215686, 'y1': 0.16256732525252518, 'x2': 0.6444465741830064, 'y2': 0.17514636565656566}}, {'text': 'XLM\\n(Conneau\\n& Lample,\\n2019)\\n', 'bbox': {'x1': 0.07529084967320263, 'y1': 0.21150040606060597, 'x2': 0.1458267088235294, 'y2': 0.2655895474747473}}, {'text': 'Translation and\\nClassification\\nfor multiple\\nlanguage\\n', 'bbox': {'x1': 0.18438071895424835, 'y1': 0.21150040606060597, 'x2': 0.2855369222222222, 'y2': 0.2655895474747473}}, {'text': '2019\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.23225545656565647, 'x2': 0.3455983660130719, 'y2': 0.24483449696969695}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.37184803921568627, 'y1': 0.22533752727272727, 'x2': 0.44282342483660136, 'y2': 0.2517536888888889}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.23225545656565647, 'x2': 0.5181038937908496, 'y2': 0.24483449696969695}}, {'text': '2019\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.3226986383838384, 'x2': 0.3455983660130719, 'y2': 0.3352776787878789}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.37184803921568627, 'y1': 0.31578070909090905, 'x2': 0.44282342483660136, 'y2': 0.34219687070707067}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.3226986383838384, 'x2': 0.5181038937908496, 'y2': 0.3352776787878789}}, {'text': 'BART (Lewis\\net al., 2020)\\n', 'bbox': {'x1': 0.06518954248366013, 'y1': 0.31578070909090905, 'x2': 0.15592734052287582, 'y2': 0.34219687070707067}}, {'text': 'Switch\\nTransformer\\n(Fedus\\net al., 2021)\\n', 'bbox': {'x1': 0.07016176470588235, 'y1': 0.4200610121212121, 'x2': 0.15095324150326797, 'y2': 0.47415141616161616}}, {'text': 'Language\\nTranslation,\\nSentence\\nReconstruction,\\nComprehension,\\ntext Generation\\n', 'bbox': {'x1': 0.18137745098039215, 'y1': 0.2881064666666666, 'x2': 0.28854051601307185, 'y2': 0.3698711131313128}}, {'text': 'Language\\nunderstanding\\ntask- Translation,\\nquestion\\nanswering,\\nClassification,\\nand so on.\\n', 'bbox': {'x1': 0.1782761437908497, 'y1': 0.3993059616161616, 'x2': 0.2916414156862745, 'y2': 0.49490646666666666}}, {'text': '2021\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.4408173252525252, 'x2': 0.3455983660130719, 'y2': 0.4533963656565656}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.37184803921568627, 'y1': 0.4338981333333334, 'x2': 0.44282342483660136, 'y2': 0.46031429494949494}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.4408173252525252, 'x2': 0.5181038937908496, 'y2': 0.4533963656565656}}, {'text': 'C4(Colossal Clean\\nCrawled Corpus)\\n', 'bbox': {'x1': 0.5717058823529412, 'y1': 0.4338981333333334, 'x2': 0.6942523738562092, 'y2': 0.46031429494949494}}, {'text': 'WMT’16, WMT’14\\nEnglish-French,\\nWMT’16\\n(English-German,\\nEnglish-Romanian,\\nRomanian-English)\\nCorrupting docu-\\nments, 1M steps\\non a combina-\\ntion of books and\\nWikipedia data,\\nnews, stories, and\\nweb text (Training)\\n', 'bbox': {'x1': 0.5674232026143791, 'y1': 0.19766328484848483, 'x2': 0.6985323209150326, 'y2': 0.3767890424242421}}, {'text': 'WMT 2014 English-\\nGerman,WMT 2014\\nEnglish-French\\n', 'bbox': {'x1': 0.7567140522875817, 'y1': 0.14873020404040402, 'x2': 0.892364944117647, 'y2': 0.1889834868686868}}, {'text': 'Wikipedia of 16 XNLI lan-\\nguages(English, French, Span-\\nish, Russian, Arabic, Chinese,\\nHindi, German, Greek, Bul-\\ngarian, Turkish, Vietnamese,\\nThai, Urdu, Swahili, Japanese)\\n', 'bbox': {'x1': 0.7242549019607843, 'y1': 0.19766328484848483, 'x2': 0.924825481372549, 'y2': 0.27942666868686844}}, {'text': 'SQuAD, MNLI, ELI5,\\nXSum, ConvAI2, CNN/DM,\\nCNN/DailyMail, WMT16\\nRomanian-English, augmented\\nwith back-translation data\\nfrom Sennrich et al. (2016).\\n', 'bbox': {'x1': 0.7237091503267973, 'y1': 0.2881064666666666, 'x2': 0.9253704065359478, 'y2': 0.3698711131313128}}, {'text': 'GLUE and SuperGLUE\\nbenchmarks, CNNDM, BBC\\nXSum, and SQuAD data sets,\\nARC Reasoning Challenge,3\\nclosed-book question an-\\nswering data sets (Natural\\nQuestions, Web Questions,\\nand Trivia QA), Wino-\\ngrande Schema Challenge,\\nAdversarial NLI Benchmark\\n', 'bbox': {'x1': 0.7277794117647058, 'y1': 0.3785509111111111, 'x2': 0.9213012888888887, 'y2': 0.5156627797979798}}]}\n",
      "{'type': 'Caption', 'bbox': [0.28828668930951284, 0.5286365300958806, 0.7137763528262868, 0.5433388449928978], 'properties': {'score': 0.6635621786117554, 'page_number': 15}, 'text_representation': 'Table 3: Transformer models for NLP - language translation task\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09051394294289981, 0.5586153897372159, 0.9130666216681985, 0.6839424826882102], 'properties': {'score': 0.8037488460540771, 'page_number': 15}, 'text_representation': '• Transformer:\\n In 2017, Vaswani et al. (Vaswani et al., 2017) introduced the first transformer model, which has since\\nrevolutionized the field of NLP. The transformer model was designed specifically for language translation and is known as\\nthe Vanilla transformer model. Unlike its predecessors, the transformer model incorporates both an encoder and a decoder\\nmodule, employing multi-head attention and masked-multi-head attention mechanisms. The encoder module is responsible\\nfor analyzing the contextual information of the input language, while the decoder module generates the output in the target\\nlanguage, using the output of the encoder and masked multi-head attention. The transformer model’s success is largely\\nattributed to its ability to perform parallel computations, which allows it to process words simultaneously with positional\\ninformation. This feature makes it highly efficient in processing large volumes of text and enables it to handle long-range\\ndependencies, which are crucial in language translation.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09031358606675093, 0.6871681906960228, 0.9120410874310662, 0.7992952658913353], 'properties': {'score': 0.8034326434135437, 'page_number': 15}, 'text_representation': '• XLM: It is a cross-lingual language pretraining model developed to support multiple languages. The model is built using\\ntwo methods: a supervised method and an unsupervised method. The unsupervised method utilizes Masked Language\\nModeling (MLM) and Casual Language Modeling (CLM) techniques and has shown remarkable effectiveness in translation\\ntasks. On the other hand, the supervised method has further improved the translation tasks (Conneau & Lample, 2019).\\nThis combination of supervised and unsupervised learning has made the XLM model a powerful tool for cross-lingual\\napplications, making it possible to perform natural language processing tasks in multiple languages. The effectiveness\\nof the XLM model in translation tasks has made it a popular choice among researchers in the field of natural language\\nprocessing.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09015998391544118, 0.8022212912819602, 0.9115328440946692, 0.900560136274858], 'properties': {'score': 0.8689642548561096, 'page_number': 15}, 'text_representation': '• BART: BART (Bidirectional and Auto-Regressive Transformers) is an advanced pre-trained model primarily aimed at\\ncleaning up the corrupt text. It features two pre-training stages: the first stage corrupts the text with noise, while the second\\nstage focuses on recovering the original text from the noisy version. BART employs a transformer translation model that\\nintegrates both the encoder and decoder modules, allowing it to perform various tasks such as text generation, translation,\\nand comprehension with impressive accuracy (Lewis et al., 2020). Its bi-directional approach enables it to learn from the\\npast and future tokens, while its auto-regressive properties make it suitable for generating output tokens sequentially. These\\nfeatures make BART an incredibly versatile model for various natural language processing tasks.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0912521182789522, 0.903501143022017, 0.91056884765625, 0.9458951083096591], 'properties': {'score': 0.8309729695320129, 'page_number': 15}, 'text_representation': '• Switch Transformer: The Switch transformer model is a recent development in the field of NLP that has gained attention\\nfor its ability to perform various tasks with high accuracy. It incorporates two key components: a permutation-based routing\\nmechanism and a gating mechanism. The permutation-based routing mechanism allows the model to learn a routing strategy\\n'}\n",
      "{'type': 'Text', 'bbox': [0.1054250290814568, 0.10063815030184659, 0.9126812385110294, 0.21188178322531961], 'properties': {'score': 0.9387575387954712, 'page_number': 16}, 'text_representation': 'that selects which parts of the input sequence to attend to. This enables the model to handle variable-length inputs, as it can\\ndynamically determine which parts of the sequence to attend to for each input. The gating mechanism allows the model to\\nperform both Classification and Segmentation tasks. The gating mechanism is designed to learn how to combine information\\nfrom different parts of the input sequence in order to make predictions. This allows the model to perform Classification\\ntasks by predicting a label for the entire input sequence, or Segmentation tasks by predicting labels for each part of the\\ninput sequence. The Switch transformer is a highly versatile model that can effectively perform both Classification and\\nSegmentation tasks (Bogatinovski et al., 2022). Its detailed description can be found in the dedicated Classification &\\nSegmentation section below (Fedus et al., 2021).\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.0940431931439568, 0.22115855823863637, 0.3977497055951287, 0.23481095747514205], 'properties': {'score': 0.7400012612342834, 'page_number': 16}}\n",
      "{'type': 'Text', 'bbox': [0.09381825166590073, 0.23816164883700283, 0.9118329934512868, 0.34878750887784093], 'properties': {'score': 0.9366648197174072, 'page_number': 16}, 'text_representation': '6.1.2 CLASSIFICATION & SEGMENTATION\\nText classification and segmentation are fundamental tasks in natural language processing (NLP) that enable the automatic\\norganization and analysis of large volumes of textual data. Text classification involves assigning tags or labels to text based\\non its contents, such as sentiment, topic, or intent, among others. This process helps to categorize textual documents from\\ndifferent sources and can be useful in a variety of applications, such as recommendation systems, information retrieval, and\\ncontent filtering. On the other hand, text segmentation involves dividing the text into meaningful units, such as sentences,\\nwords, or topics, to facilitate further analysis or processing. This task is crucial for various NLP applications, including\\nlanguage understanding, summarization, and question answering, among others (Chowdhary & Chowdhary, 2020, Kuhn,\\n2014, Hu et al., 2016).\\nTransformer-based models have been shown to achieve state-of-the-art performance in text classification and segmentation\\ntasks. These models are characterized by their ability to capture long-range dependencies and contextual information in text,\\nmaking them well-suited for complex NLP tasks. Table 4 highlights some of the most prominent transformer-based models\\nthat have demonstrated significant performance in text classification and segmentation tasks.\\n'}\n",
      "{'type': 'Text', 'bbox': [0.0936134428136489, 0.34969521262428976, 0.9118917308134191, 0.40500926624644884], 'properties': {'score': 0.8031930923461914, 'page_number': 16}}\n",
      "{'type': 'table', 'bbox': [0.05206006667193244, 0.4247487848455256, 0.9389186724494485, 0.8331067449396307], 'properties': {'score': 0.8071258068084717, 'title': None, 'columns': None, 'rows': None, 'page_number': 16}, 'text_representation': 'Transformer\\nModels\\n Task Ac-\\ncomplished\\n Year\\n Architecture\\n(Encoder/\\nDecoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training\\nDataset\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n GPT &\\nvariants\\n(Radford\\net al., 2018;\\n2019, Brown\\net al., 2020)\\n XLM\\n(Conneau\\n& Lample,\\n2019)\\n Text classifica-\\ntion, Question\\nanswering,\\ntextual entail-\\nment, semantic\\nsimilarity\\n Translation and\\nClassification\\nfor multiple\\nlanguage\\n T5 (Raffel\\net al., 2020)\\n Text summariza-\\ntion, Question\\nanswering, text\\nclassification\\n 2018\\n Decoder\\n Yes\\n Book corpus\\n 2019\\n Encoder &\\nDecoder\\n Yes\\n WMT’16, WMT’14\\nEnglish-French,\\nWMT’16\\n(English-German,\\nEnglish-Romanian,\\nRomanian-English)\\n 2020\\n Encoder &\\nDecoder\\n Yes\\n C4 (Colossal Clean\\nCrawled Corpus)\\n Charformer\\n(Tay et al.,\\n2022)\\n Classification\\ntask, toxicity\\ndetection,\\nand so on.\\n 2022\\n Encoder &\\nDecoder\\n Yes\\n The same datasets\\nused in T5 model-\\nC4(Colossal Clean\\nCrawled Corpus)\\n SNLI, MNLI, QNLI, Sc-\\niTail, RTE, RACE, CNN,\\nSQuaD, MRPC, QQP,\\nSTS-B, SST2 & CoLA\\n Wikipedia of 16 XNLI lan-\\nguages(English, French, Span-\\nish, Russian, Arabic, Chinese,\\nHindi, German, Greek, Bul-\\ngarian, Turkish, Vietnamese,\\nThai, Urdu, Swahili, Japanese)\\nGLUE and SuperGLUE\\nbenchmarks, CNN/Daily\\nMail abstractive summa-\\nrization, SQuAD question\\nanswering, WMT En-\\nglish to German, French,\\nand Romanian translation\\nGLUE IMDb, AGNews,(Maas\\net al., 2011), (Zhang et al.,\\n2015), Civil Comments,\\nWikipedia Comments,\\nTyDiQA-GoldP, XQuAD,\\nMLQA, XNLI, PAWS-X..\\n Continued on next page\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e04ddf90>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06615849673202615, 'y1': 0.43478103964646464, 'x2': 0.15495912254901958, 'y2': 0.46119720126262626}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.19561274509803922, 'y1': 0.43478103964646464, 'x2': 0.27430426209150327, 'y2': 0.46119720126262626}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.4417002315656566, 'x2': 0.3455820872549019, 'y2': 0.454279271969697}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\n', 'bbox': {'x1': 0.3656486928104575, 'y1': 0.42786311035353536, 'x2': 0.4536516594771242, 'y2': 0.4681163931818182}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.4746160130718954, 'y1': 0.42786311035353536, 'x2': 0.5379078248366013, 'y2': 0.4681163931818182}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.590173202614379, 'y1': 0.43478103964646464, 'x2': 0.6757831918300653, 'y2': 0.46119720126262626}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7508382352941176, 'y1': 0.43478103964646464, 'x2': 0.8982423905228758, 'y2': 0.46119720126262626}}, {'text': 'GPT &\\nvariants\\n(Radford\\net al., 2018;\\n2019, Brown\\net al., 2020)\\n', 'bbox': {'x1': 0.06825816993464053, 'y1': 0.4699650525252525, 'x2': 0.15285887614379084, 'y2': 0.551729698989899}}, {'text': 'XLM\\n(Conneau\\n& Lample,\\n2019)\\n', 'bbox': {'x1': 0.07529084967320263, 'y1': 0.5673274262626262, 'x2': 0.1458267088235294, 'y2': 0.6214178303030303}}, {'text': 'Text classifica-\\ntion, Question\\nanswering,\\ntextual entail-\\nment, semantic\\nsimilarity\\n', 'bbox': {'x1': 0.1856748366013072, 'y1': 0.4699650525252525, 'x2': 0.28424271732026146, 'y2': 0.551729698989899}}, {'text': 'Translation and\\nClassification\\nfor multiple\\nlanguage\\n', 'bbox': {'x1': 0.18438071895424835, 'y1': 0.5673274262626262, 'x2': 0.2855369222222222, 'y2': 0.6214178303030303}}, {'text': 'T5 (Raffel\\net al., 2020)\\n', 'bbox': {'x1': 0.0716764705882353, 'y1': 0.6716077292929293, 'x2': 0.14944009836601307, 'y2': 0.6980238909090909}}, {'text': 'Text summariza-\\ntion, Question\\nanswering, text\\nclassification\\n', 'bbox': {'x1': 0.18027124183006535, 'y1': 0.657770608080808, 'x2': 0.2896482179738562, 'y2': 0.7118610121212121}}, {'text': '2018\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.5045572242424243, 'x2': 0.3455983660130719, 'y2': 0.5171362646464647}}, {'text': 'Decoder\\n', 'bbox': {'x1': 0.37976797385620914, 'y1': 0.5045572242424243, 'x2': 0.4349041277777778, 'y2': 0.5171362646464647}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.5045572242424243, 'x2': 0.5181038937908496, 'y2': 0.5171362646464647}}, {'text': 'Book corpus\\n', 'bbox': {'x1': 0.5916062091503268, 'y1': 0.5045572242424243, 'x2': 0.6743511369281046, 'y2': 0.5171362646464647}}, {'text': '2019\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.5880824767676768, 'x2': 0.3455983660130719, 'y2': 0.6006615171717171}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.37184803921568627, 'y1': 0.5811645474747476, 'x2': 0.44282342483660136, 'y2': 0.6075807090909091}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.5880824767676768, 'x2': 0.5181038937908496, 'y2': 0.6006615171717171}}, {'text': 'WMT’16, WMT’14\\nEnglish-French,\\nWMT’16\\n(English-German,\\nEnglish-Romanian,\\nRomanian-English)\\n', 'bbox': {'x1': 0.5674232026143791, 'y1': 0.5534903050505051, 'x2': 0.6985323209150326, 'y2': 0.6352549515151515}}, {'text': '2020\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.6785269212121212, 'x2': 0.3455983660130719, 'y2': 0.6911059616161616}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.37184803921568627, 'y1': 0.6716077292929293, 'x2': 0.44282342483660136, 'y2': 0.6980238909090909}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.6785269212121212, 'x2': 0.5181038937908496, 'y2': 0.6911059616161616}}, {'text': 'C4 (Colossal Clean\\nCrawled Corpus)\\n', 'bbox': {'x1': 0.5696699346405228, 'y1': 0.6716077292929293, 'x2': 0.6962861156862745, 'y2': 0.6980238909090909}}, {'text': 'Charformer\\n(Tay et al.,\\n2022)\\n', 'bbox': {'x1': 0.07258823529411765, 'y1': 0.7551329818181819, 'x2': 0.14852864215686273, 'y2': 0.7953862646464647}}, {'text': 'Classification\\ntask, toxicity\\ndetection,\\nand so on.\\n', 'bbox': {'x1': 0.19064052287581698, 'y1': 0.7482150525252526, 'x2': 0.2792783611111111, 'y2': 0.802304193939394}}, {'text': '2022\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.768970103030303, 'x2': 0.3455983660130719, 'y2': 0.7815491434343433}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.37184803921568627, 'y1': 0.7620521737373737, 'x2': 0.44282342483660136, 'y2': 0.7884670727272727}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.768970103030303, 'x2': 0.5181038937908496, 'y2': 0.7815491434343433}}, {'text': 'The same datasets\\nused in T5 model-\\nC4(Colossal Clean\\nCrawled Corpus)\\n', 'bbox': {'x1': 0.5717058823529412, 'y1': 0.7482150525252526, 'x2': 0.6942523738562092, 'y2': 0.802304193939394}}, {'text': 'SNLI, MNLI, QNLI, Sc-\\niTail, RTE, RACE, CNN,\\nSQuaD, MRPC, QQP,\\nSTS-B, SST2 & CoLA\\n', 'bbox': {'x1': 0.7421209150326797, 'y1': 0.48380217373737383, 'x2': 0.9069596202614378, 'y2': 0.5378925777777778}}, {'text': 'Wikipedia of 16 XNLI lan-\\nguages(English, French, Span-\\nish, Russian, Arabic, Chinese,\\nHindi, German, Greek, Bul-\\ngarian, Turkish, Vietnamese,\\nThai, Urdu, Swahili, Japanese)\\nGLUE and SuperGLUE\\nbenchmarks, CNN/Daily\\nMail abstractive summa-\\nrization, SQuAD question\\nanswering, WMT En-\\nglish to German, French,\\nand Romanian translation\\nGLUE IMDb, AGNews,(Maas\\net al., 2011), (Zhang et al.,\\n2015), Civil Comments,\\nWikipedia Comments,\\nTyDiQA-GoldP, XQuAD,\\nMLQA, XNLI, PAWS-X..\\n', 'bbox': {'x1': 0.7242549019607843, 'y1': 0.5534903050505051, 'x2': 0.924825481372549, 'y2': 0.8161413151515151}}, {'text': 'Continued on next page\\n', 'bbox': {'x1': 0.7715947712418301, 'y1': 0.8184057090909092, 'x2': 0.9264383189542483, 'y2': 0.8309847494949495}}]}\n",
      "{'type': 'table', 'bbox': [0.0518617338292739, 0.10024462613192471, 0.9387401625689338, 0.2952945223721591], 'properties': {'score': 0.7797933220863342, 'title': None, 'columns': None, 'rows': None, 'page_number': 17}, 'text_representation': 'Transformer\\nModels\\n Task Ac-\\ncomplished\\n Year\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training\\nDataset\\n Switch\\nTransformer\\n(Fedus\\net al., 2021)\\n Language\\nunderstanding\\ntask- Translation,\\nquestion\\nanswering,\\nClassification,\\nand so on.\\n 2021\\n Encoder &\\nDecoder\\n Yes\\n C4(Colossal Clean\\nCrawled Corpus)\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n GLUE and SuperGLUE\\nbenchmarks, CNNDM, BBC\\nXSum and SQuAD data sets,\\nARC Reasoning Challenge,3\\nclosed-book question an-\\nswering data sets (Natural\\nQuestions, Web Questions,\\nand Trivia QA), Wino-\\ngrande Schema Challenge,\\nAdversarial NLI Benchmark\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e04df400>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06615849673202615, 'y1': 0.12046538308080808, 'x2': 0.15495912254901958, 'y2': 0.14688154469696957}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.19561274509803922, 'y1': 0.12046538308080808, 'x2': 0.27430426209150327, 'y2': 0.14688154469696957}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.12738331237373743, 'x2': 0.3455820872549019, 'y2': 0.13996235277777777}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.4746160130718954, 'y1': 0.11354619116161614, 'x2': 0.5379078248366013, 'y2': 0.15379947398989877}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.590173202614379, 'y1': 0.12046538308080808, 'x2': 0.6757831918300653, 'y2': 0.14688154469696957}}, {'text': 'Switch\\nTransformer\\n(Fedus\\net al., 2021)\\n', 'bbox': {'x1': 0.07016176470588235, 'y1': 0.19715949696969695, 'x2': 0.15095324150326797, 'y2': 0.25124990101010086}}, {'text': 'Language\\nunderstanding\\ntask- Translation,\\nquestion\\nanswering,\\nClassification,\\nand so on.\\n', 'bbox': {'x1': 0.1782761437908497, 'y1': 0.17640444646464645, 'x2': 0.2916414156862745, 'y2': 0.2720049515151512}}, {'text': '2021\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.21791581010101005, 'x2': 0.3455983660130719, 'y2': 0.23049485050505053}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.37184803921568627, 'y1': 0.21099661818181809, 'x2': 0.44282342483660136, 'y2': 0.23741277979797973}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.21791581010101005, 'x2': 0.5181038937908496, 'y2': 0.23049485050505053}}, {'text': 'C4(Colossal Clean\\nCrawled Corpus)\\n', 'bbox': {'x1': 0.5717058823529412, 'y1': 0.21099661818181809, 'x2': 0.6942523738562092, 'y2': 0.23741277979797973}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7508382352941176, 'y1': 0.12046538308080808, 'x2': 0.8982423905228758, 'y2': 0.14688154469696957}}, {'text': 'GLUE and SuperGLUE\\nbenchmarks, CNNDM, BBC\\nXSum and SQuAD data sets,\\nARC Reasoning Challenge,3\\nclosed-book question an-\\nswering data sets (Natural\\nQuestions, Web Questions,\\nand Trivia QA), Wino-\\ngrande Schema Challenge,\\nAdversarial NLI Benchmark\\n', 'bbox': {'x1': 0.729813725490196, 'y1': 0.15564939595959598, 'x2': 0.9192659130718953, 'y2': 0.2927612646464642}}]}\n",
      "{'type': 'Caption', 'bbox': [0.22215411017922795, 0.3055773370916193, 0.780107852711397, 0.3202980180220171], 'properties': {'score': 0.4770938754081726, 'page_number': 17}, 'text_representation': 'Table 4: Transformer models for NLP - language classification & segmentation tasks\\n'}\n",
      "{'type': 'Text', 'bbox': [0.22215411017922795, 0.3055773370916193, 0.780107852711397, 0.3202980180220171], 'properties': {'score': 0.4763605296611786, 'page_number': 17}, 'text_representation': 'Table 4: Transformer models for NLP - language classification & segmentation tasks\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09078300027286305, 0.3388046542080966, 0.9131162396599265, 0.39467429421164774], 'properties': {'score': 0.7937527894973755, 'page_number': 17}, 'text_representation': '• Charformer: It is a transformer-based model that introduces Gradient-based subword tokenization (GBST), a lightweight\\napproach to learning latent subwords directly from characters at the byte level. The model has both English and multi-\\nlingual variants and has demonstrated outstanding performance on language understanding tasks, such as the classification\\nof long text documents (Tay et al., 2022).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0909860409007353, 0.3988615001331676, 0.9148068416819853, 0.510994540127841], 'properties': {'score': 0.8241173624992371, 'page_number': 17}, 'text_representation': '• Switch Transformer: The use of pre-trained models such as BERT and GPT, trained on large datasets, has gained\\npopularity in the field of natural language processing. However, there are concerns about the economic and environmental\\ncosts of training such models. To address these concerns, the Switch transformer was introduced, which offers a larger\\nmodel size without a significant increase in computational cost. The Switch transformer replaces the feed-forward neural\\nnetwork (FFN) with a switch layer that contains multiple FFNs, resulting in a model with trillions of parameters. Despite\\nthe increase in model size, the computational cost of the Switch transformer remains comparable to that of other models. In\\nfact, the Switch transformer has been evaluated on 11 different tasks and has shown significant improvement in tasks such\\nas translation, question-answering, classification, and summarization (Fedus et al., 2021).\\n'}\n",
      "{'type': 'Text', 'bbox': [0.10472973991842831, 0.5262553821910512, 0.9134176097196691, 0.5687949440696023], 'properties': {'score': 0.8432040810585022, 'page_number': 17}, 'text_representation': 'GPT & Variants, XLM, T5: These models are versatile and capable of performing a range of NLP tasks, including but\\nnot limited to classification, segmentation, question answering, and language translation. Section 6.1.3 will provide detailed\\ndescription.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09441543579101562, 0.5775212513316761, 0.3145138370289522, 0.5921178644353693], 'properties': {'score': 0.7014917135238647, 'page_number': 17}}\n",
      "{'type': 'Text', 'bbox': [0.09355834063361673, 0.5947514759410512, 0.913345947265625, 0.6647126908735795], 'properties': {'score': 0.8587296605110168, 'page_number': 17}, 'text_representation': '6.1.3 QUESTION ANSWERING\\nQuestion Answering is a classical NLP task. It involves matching a text query to the most relevant answer in the form of text,\\nbased on the relevance of the text to the query. This task is challenging, as finding a concise and accurate answer to a given\\nquery can be difficult (Chowdhary & Chowdhary, 2020, Hirschman & Gaizauskas, 2001). Recent research has focused on\\nthis task, leading to the development of several transformer-based models that leverage deep learning techniques to improve\\nthe accuracy and efficiency of this task. A detailed overview of these models is provided in Table 5.\\n'}\n",
      "{'type': 'table', 'bbox': [0.05260256150189568, 0.6843711159446023, 0.9389619715073529, 0.8980255126953125], 'properties': {'score': 0.7476849555969238, 'title': None, 'columns': None, 'rows': None, 'page_number': 17}, 'text_representation': 'Transformer\\nModels\\n Task Ac-\\ncomplished\\n Year\\n Architecture\\n(Encoder/\\nDecoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training\\nDataset\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n BERT\\n(Devlin\\net al., 2019)\\n ELECTRA\\n(Clark et al.,\\n2020a)\\n Question\\nanswering,\\nSentence\\nPrediction,\\nlanguage\\nunderstanding\\nLanguage\\nunderstanding\\ntasks- Question\\nanswering\\nand so on\\n 2018\\n Encoder\\n Yes\\n Book Corpus,\\nEnglish Wikipedia\\n SQuAD v1.1, SQuAD\\nv2.0, SWAG, QNLI, MNLI\\n 2020\\n Encoder\\n Yes\\n Wikipedia,\\nBooksCorpus,\\nClueWeb, Common-\\nCrawl, Gigaword\\n SQuAD 1.1,\\nSQuAD 2.0, GLUE\\n Continued on next page\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e04dff40>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06615849673202615, 'y1': 0.6934464436868687, 'x2': 0.15495912254901958, 'y2': 0.7198626053030304}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.19561274509803922, 'y1': 0.6934464436868687, 'x2': 0.27430426209150327, 'y2': 0.7198626053030304}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.7003643729797979, 'x2': 0.3455820872549019, 'y2': 0.7129434133838384}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\n', 'bbox': {'x1': 0.3656486928104575, 'y1': 0.6865285143939394, 'x2': 0.4536516594771242, 'y2': 0.7267805345959596}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.4746160130718954, 'y1': 0.6865285143939394, 'x2': 0.5379078248366013, 'y2': 0.7267805345959596}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.590173202614379, 'y1': 0.6934464436868687, 'x2': 0.6757831918300653, 'y2': 0.7198626053030304}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7508382352941176, 'y1': 0.6934464436868687, 'x2': 0.8982423905228758, 'y2': 0.7198626053030304}}, {'text': 'BERT\\n(Devlin\\net al., 2019)\\n', 'bbox': {'x1': 0.0716764705882353, 'y1': 0.7493855070707072, 'x2': 0.14944009836601307, 'y2': 0.78963878989899}}, {'text': 'ELECTRA\\n(Clark et al.,\\n2020a)\\n', 'bbox': {'x1': 0.06986928104575164, 'y1': 0.8259928303030304, 'x2': 0.15124679313725495, 'y2': 0.8662448505050505}}, {'text': 'Question\\nanswering,\\nSentence\\nPrediction,\\nlanguage\\nunderstanding\\nLanguage\\nunderstanding\\ntasks- Question\\nanswering\\nand so on\\n', 'bbox': {'x1': 0.18454411764705883, 'y1': 0.7286304565656566, 'x2': 0.28537474575163396, 'y2': 0.8800819717171717}}, {'text': '2018\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.7632226282828283, 'x2': 0.3455983660130719, 'y2': 0.7758016686868687}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.3802156862745098, 'y1': 0.7632226282828283, 'x2': 0.434456508496732, 'y2': 0.7758016686868687}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.7632226282828283, 'x2': 0.5181038937908496, 'y2': 0.7758016686868687}}, {'text': 'Book Corpus,\\nEnglish Wikipedia\\n', 'bbox': {'x1': 0.5724869281045751, 'y1': 0.7563046989898989, 'x2': 0.6934706588235294, 'y2': 0.782719597979798}}, {'text': 'SQuAD v1.1, SQuAD\\nv2.0, SWAG, QNLI, MNLI\\n', 'bbox': {'x1': 0.7351928104575163, 'y1': 0.7563046989898989, 'x2': 0.9138847388888888, 'y2': 0.782719597979798}}, {'text': '2020\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.8398299515151515, 'x2': 0.3455983660130719, 'y2': 0.852408991919192}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.3802156862745098, 'y1': 0.8398299515151515, 'x2': 0.434456508496732, 'y2': 0.852408991919192}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.8398299515151515, 'x2': 0.5181038937908496, 'y2': 0.852408991919192}}, {'text': 'Wikipedia,\\nBooksCorpus,\\nClueWeb, Common-\\nCrawl, Gigaword\\n', 'bbox': {'x1': 0.5658039215686274, 'y1': 0.8190736383838384, 'x2': 0.7001525127450979, 'y2': 0.8731640424242425}}, {'text': 'SQuAD 1.1,\\nSQuAD 2.0, GLUE\\n', 'bbox': {'x1': 0.7603366013071896, 'y1': 0.8329107595959595, 'x2': 0.8887434457516341, 'y2': 0.8593269212121212}}, {'text': 'Continued on next page\\n', 'bbox': {'x1': 0.7715947712418301, 'y1': 0.8823463656565657, 'x2': 0.9264383189542483, 'y2': 0.8949254060606061}}]}\n",
      "{'type': 'table', 'bbox': [0.05144971510943244, 0.10844379771839488, 0.9390910070082721, 0.6139764959161932], 'properties': {'score': 0.8449016213417053, 'title': None, 'columns': None, 'rows': None, 'page_number': 18}, 'text_representation': 'Transformer\\nModels\\n Task Ac-\\ncomplished\\n Year\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training\\nDataset\\n GPT &\\nvariants\\n(Radford\\net al., 2018;\\n2019, Brown\\net al., 2020)\\n Text classifica-\\ntion, Question\\nanswering,\\ntextual entail-\\nment, semantic\\nsimilarity\\n Switch\\nTransformer\\n(Fedus\\net al., 2021)\\n Language\\nunderstanding\\ntask- Translation,\\nquestion\\nanswering,\\nClassification,\\nand so on.\\n T5 (Raffel\\net al., 2020)\\n Text summariza-\\ntion, Question\\nanswering, text\\nclassification\\n 2018\\n Decoder\\n Yes\\n Book corpus\\n 2021\\n Encoder &\\nDecoder\\n Yes\\n C4(Colossal Clean\\nCrawled Corpus)\\n 2020\\n Encoder &\\nDecoder\\n Yes\\n C4 (Colossal Clean\\nCrawled Corpus)\\n InstructGPT\\n(Ouyang\\net al., 2022)\\n Text Generation,\\nQuestion\\nAnswering,\\nsummarization,\\nand so on.\\n 2022\\n Decoder\\n Yes\\n Based on the\\npre-training\\nmodel GPT-3\\n Dataset(Fine-tuning,\\nTraining, Testing)\\n SNLI, MNLI, QNLI, Sc-\\niTail, RTE, RACE, CNN,\\nSQuaD, MRPC, QQP,\\nSTS-B, SST2 & CoLA\\n GLUE and SuperGLUE\\nbenchmarks, CNNDM, BBC\\nXSum, and SQuAD data sets,\\nARC Reasoning Challenge,3\\nclosed-book question an-\\nswering data sets (Natural\\nQuestions, Web Questions,\\nand Trivia QA), Wino-\\ngrande Schema Challenge,\\nAdversarial NLI Benchmark\\nGLUE and SuperGLUE\\nbenchmarks, CNN/Daily\\nMail abstractive summa-\\nrization, SQuAD question\\nanswering, WMT En-\\nglish to German, French,\\nand Romanian translation\\nSFT dataset, RM dataset, PPO\\ndataset, a dataset of prompts\\nand completions Winogender,\\nCrowS-Pairs, Real Toxicity\\nPrompts, TruthfulQA, DROP,\\nQuAC, SquadV2, Hellaswag,\\nSST, RTE and WSC, WMT\\n15 Fr ! En, CNN/Daily Mail\\nSummarization, Reddit TLDR\\nSummarization datasets.\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e0518c10>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06615849673202615, 'y1': 0.12046538308080808, 'x2': 0.15495912254901958, 'y2': 0.14688154469696957}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.19561274509803922, 'y1': 0.12046538308080808, 'x2': 0.27430426209150327, 'y2': 0.14688154469696957}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.12738331237373743, 'x2': 0.3455820872549019, 'y2': 0.13996235277777777}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.4746160130718954, 'y1': 0.11354619116161614, 'x2': 0.5379078248366013, 'y2': 0.15379947398989877}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.590173202614379, 'y1': 0.12046538308080808, 'x2': 0.6757831918300653, 'y2': 0.14688154469696957}}, {'text': 'GPT &\\nvariants\\n(Radford\\net al., 2018;\\n2019, Brown\\net al., 2020)\\n', 'bbox': {'x1': 0.06825816993464053, 'y1': 0.15564939595959598, 'x2': 0.15285887614379084, 'y2': 0.2374127797979796}}, {'text': 'Text classifica-\\ntion, Question\\nanswering,\\ntextual entail-\\nment, semantic\\nsimilarity\\n', 'bbox': {'x1': 0.1856748366013072, 'y1': 0.15564939595959598, 'x2': 0.28424271732026146, 'y2': 0.2374127797979796}}, {'text': 'Switch\\nTransformer\\n(Fedus\\net al., 2021)\\n', 'bbox': {'x1': 0.07016176470588235, 'y1': 0.28068474949494954, 'x2': 0.15095324150326797, 'y2': 0.3347751535353535}}, {'text': 'Language\\nunderstanding\\ntask- Translation,\\nquestion\\nanswering,\\nClassification,\\nand so on.\\n', 'bbox': {'x1': 0.1782761437908497, 'y1': 0.25992969898989887, 'x2': 0.2916414156862745, 'y2': 0.3555302040404036}}, {'text': 'T5 (Raffel\\net al., 2020)\\n', 'bbox': {'x1': 0.0716764705882353, 'y1': 0.41263929494949486, 'x2': 0.14944009836601307, 'y2': 0.43905545656565653}}, {'text': 'Text summariza-\\ntion, Question\\nanswering, text\\nclassification\\n', 'bbox': {'x1': 0.18027124183006535, 'y1': 0.39880217373737376, 'x2': 0.2896482179738562, 'y2': 0.45289257777777775}}, {'text': '2018\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.19024156767676761, 'x2': 0.3455983660130719, 'y2': 0.2028206080808081}}, {'text': 'Decoder\\n', 'bbox': {'x1': 0.37976797385620914, 'y1': 0.19024156767676761, 'x2': 0.4349041277777778, 'y2': 0.2028206080808081}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.19024156767676761, 'x2': 0.5181038937908496, 'y2': 0.2028206080808081}}, {'text': 'Book corpus\\n', 'bbox': {'x1': 0.5916062091503268, 'y1': 0.19024156767676761, 'x2': 0.6743511369281046, 'y2': 0.2028206080808081}}, {'text': '2021\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.30144106262626263, 'x2': 0.3455983660130719, 'y2': 0.3140201030303031}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.37184803921568627, 'y1': 0.2945218707070707, 'x2': 0.44282342483660136, 'y2': 0.3209380323232323}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.30144106262626263, 'x2': 0.5181038937908496, 'y2': 0.3140201030303031}}, {'text': 'C4(Colossal Clean\\nCrawled Corpus)\\n', 'bbox': {'x1': 0.5717058823529412, 'y1': 0.2945218707070707, 'x2': 0.6942523738562092, 'y2': 0.3209380323232323}}, {'text': '2020\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.41955848686868685, 'x2': 0.3455983660130719, 'y2': 0.43213752727272725}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.37184803921568627, 'y1': 0.41263929494949486, 'x2': 0.44282342483660136, 'y2': 0.43905545656565653}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.41955848686868685, 'x2': 0.5181038937908496, 'y2': 0.43213752727272725}}, {'text': 'C4 (Colossal Clean\\nCrawled Corpus)\\n', 'bbox': {'x1': 0.5696699346405228, 'y1': 0.41263929494949486, 'x2': 0.6962861156862745, 'y2': 0.43905545656565653}}, {'text': 'InstructGPT\\n(Ouyang\\net al., 2022)\\n', 'bbox': {'x1': 0.07031699346405229, 'y1': 0.5238387898989899, 'x2': 0.15079917385620917, 'y2': 0.5640920727272727}}, {'text': 'Text Generation,\\nQuestion\\nAnswering,\\nsummarization,\\nand so on.\\n', 'bbox': {'x1': 0.1804983660130719, 'y1': 0.5100016686868688, 'x2': 0.28941953692810457, 'y2': 0.577929193939394}}, {'text': '2022\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.537675911111111, 'x2': 0.3455983660130719, 'y2': 0.5502549515151515}}, {'text': 'Decoder\\n', 'bbox': {'x1': 0.37976797385620914, 'y1': 0.537675911111111, 'x2': 0.4349041277777778, 'y2': 0.5502549515151515}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.537675911111111, 'x2': 0.5181038937908496, 'y2': 0.5502549515151515}}, {'text': 'Based on the\\npre-training\\nmodel GPT-3\\n', 'bbox': {'x1': 0.589187908496732, 'y1': 0.5238387898989899, 'x2': 0.6767676274509803, 'y2': 0.5640920727272727}}, {'text': 'Dataset(Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7528725490196078, 'y1': 0.12046538308080808, 'x2': 0.8962070147058822, 'y2': 0.14688154469696957}}, {'text': 'SNLI, MNLI, QNLI, Sc-\\niTail, RTE, RACE, CNN,\\nSQuaD, MRPC, QQP,\\nSTS-B, SST2 & CoLA\\n', 'bbox': {'x1': 0.7421209150326797, 'y1': 0.16948525454545452, 'x2': 0.9069596202614378, 'y2': 0.22357565858585843}}, {'text': 'GLUE and SuperGLUE\\nbenchmarks, CNNDM, BBC\\nXSum, and SQuAD data sets,\\nARC Reasoning Challenge,3\\nclosed-book question an-\\nswering data sets (Natural\\nQuestions, Web Questions,\\nand Trivia QA), Wino-\\ngrande Schema Challenge,\\nAdversarial NLI Benchmark\\nGLUE and SuperGLUE\\nbenchmarks, CNN/Daily\\nMail abstractive summa-\\nrization, SQuAD question\\nanswering, WMT En-\\nglish to German, French,\\nand Romanian translation\\nSFT dataset, RM dataset, PPO\\ndataset, a dataset of prompts\\nand completions Winogender,\\nCrowS-Pairs, Real Toxicity\\nPrompts, TruthfulQA, DROP,\\nQuAC, SquadV2, Hellaswag,\\nSST, RTE and WSC, WMT\\n15 Fr ! En, CNN/Daily Mail\\nSummarization, Reddit TLDR\\nSummarization datasets.\\n', 'bbox': {'x1': 0.7255081699346405, 'y1': 0.2391746484848484, 'x2': 0.9235718205882355, 'y2': 0.6125213656565657}}]}\n",
      "{'type': 'Caption', 'bbox': [0.29021015840418196, 0.6254170920632103, 0.7112628532858456, 0.6401517555930397], 'properties': {'score': 0.4561883211135864, 'page_number': 18}, 'text_representation': 'Table 5: Transformer models for NLP - question answering task\\n'}\n",
      "{'type': 'Text', 'bbox': [0.29021015840418196, 0.6254170920632103, 0.7112628532858456, 0.6401517555930397], 'properties': {'score': 0.4443851113319397, 'page_number': 18}, 'text_representation': 'Table 5: Transformer models for NLP - question answering task\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09071939804974724, 0.6637037242542614, 0.9119952033547795, 0.7336545632102273], 'properties': {'score': 0.5204197764396667, 'page_number': 18}}\n",
      "{'type': 'Text', 'bbox': [0.10502323823816637, 0.7326910400390625, 0.9115531652113971, 0.816454911665483], 'properties': {'score': 0.5744144916534424, 'page_number': 18}, 'text_representation': '• BERT & BERT variants: BERT is an acronym that stands for Bidirectional Encoder Representations of transformers. It\\nwas introduced by the Google AI team and is embedded within the encoder module of the transformer. BERT employs a\\nbidirectional approach, allowing it to pre-train a transformer on unannotated text by considering the context of each word.\\nAs a result, BERT has achieved remarkable performance on various natural language processing (NLP) tasks (Devlin et al.,\\n2019).\\nA variety of BERT-based models have been developed with different characteristics. For instance, some are optimized for\\nfast computation, while others produce superior results with a small number of parameters. Some are also tailored to specific\\ntasks, such as RoBERTa, which is designed for masked language modeling and next sentence prediction (Liu et al., 2019).\\nFlueBERT is another model that can be used for tasks such as text classification, paraphrasing, natural language inference,\\nparsing, and word sense disambiguation (Le et al., 2020). Additionally, DistilBERT is suitable for question answering and\\nother specific tasks. These models have significantly improved pre-trained transformer models (Sanh et al., 2019).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0902849713493796, 0.8228171608664773, 0.9112589039522059, 0.9487114923650568], 'properties': {'score': 0.5302185416221619, 'page_number': 18}, 'text_representation': '• GPT & GPT variants: Generative Pre-Trained Transformer (GPT) models are built exclusively on the decoder block\\nof transformers, which significantly improves the progress of transformers in natural language processing. GPT adopts a\\nsemi-supervised approach to language comprehension, which involves unsupervised pre-training and supervised fine-tuning\\nmethods (Radford et al., 2018). In 2019, following the success of the GPT model, a massively pre-trained transformer-\\nbased model called GPT-2 with 1.5 billion parameters was introduced, which significantly improved the pre-trained version\\nof transformers (Radford et al., 2019). Subsequently, in 2020, the largest pre-trained version of GPT with 175 billion\\nparameters, called GPT-3, was released. This model is 10 times larger than the previous non-sparse language model. One\\nof the most notable achievements of GPT-3 is that it exhibits strong performance across a range of tasks without the need\\nfor gradient updates or fine-tuning, which is a requirement for pre-training models like BERT (Brown et al., 2020).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09194070255055146, 0.099914419000799, 0.9107665297564338, 0.17066703102805397], 'properties': {'score': 0.6681929230690002, 'page_number': 19}, 'text_representation': '• Electra: An acronym for “Efficiently Learning an Encoder that Classifies Token Replacements Accurately”, utilizes a dis-\\ntinct pre-training method compared to other pre-trained models. Electra deploys a ”Masked Language Modeling” approach\\nthat masks certain words and trains the model to predict them. Additionally, Electra incorporates a ”Discriminator” network\\nthat aids in comprehending language without the need to memorize the training data. This unique approach enables Electra\\nto generate superior text and surpass the performance of BERT (Clark et al., 2020a).\\n'}\n",
      "{'type': 'Text', 'bbox': [0.10538813871495864, 0.1859269991787997, 0.9115411017922794, 0.22852501609108664], 'properties': {'score': 0.8695154190063477, 'page_number': 19}, 'text_representation': 'InstructGPT, T5 and Switch Transformer: While the InstructGPT model can generate text apart from question-\\nanswering tasks, the T5 is significant in test summarization tasks and Switch transformer models can perform classification\\nand segmentation tasks as well. More descriptions of these models have been provided in Sections 6.1.1 and 6.1.2.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09463665232938878, 0.23735698353160511, 0.3117559455422794, 0.2516383777965199], 'properties': {'score': 0.7121076583862305, 'page_number': 19}}\n",
      "{'type': 'Text', 'bbox': [0.09374597886029412, 0.25425711891867897, 0.9117113539751838, 0.32454911665482955], 'properties': {'score': 0.8624887466430664, 'page_number': 19}, 'text_representation': '6.1.4 TEXT SUMMARIZATION\\nText summarization is a natural language processing task that involves breaking down lengthy texts into shorter versions while\\nretaining essential and valuable information and preserving the meaning of the text. Text summarization is particularly useful\\nin comprehending lengthy textual documents, and it also helps to reduce computational resources and time (Chowdhary &\\nChowdhary, 2020, Tas & Kiyani, 2007). transformer-based models have shown exceptional performance in text summariza-\\ntion tasks. The transformer-based models in text summarization are listed in Table 6.\\n'}\n",
      "{'type': 'table', 'bbox': [0.051787463917451745, 0.3416470891779119, 0.9389751120174632, 0.9304222523082386], 'properties': {'score': 0.803913950920105, 'title': None, 'columns': None, 'rows': None, 'page_number': 19}, 'text_representation': 'Transformer\\nModels\\n Task Ac-\\ncomplished\\n Year\\n Architecture\\n(Encoder/\\nDecoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training\\nDataset\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n GPT &\\nvariants\\n(Radford\\net al., 2018;\\n2019, Brown\\net al., 2020)\\n PEGASUS\\n(Zhang\\net al., 2020a)\\n Switch\\nTransformer\\n(Fedus\\net al., 2021)\\n Text classifica-\\ntion, Question\\nanswering,\\ntextual entail-\\nment, semantic\\nsimilarity\\n 2018\\n Decoder\\n Yes\\n Book corpus\\n Text sum-\\nmarization\\n 2020\\n Encoder &\\nDecoder\\n Yes\\n C4, HugeNews\\n Language\\nunderstanding\\ntask- Translation,\\nquestion\\nanswering,\\nClassification,\\nand so on.\\n 2021\\n Encoder &\\nDecoder\\n Yes\\n C4(Colossal Clean\\nCrawled Corpus)\\n T5 (Raffel\\net al., 2020)\\n Text summariza-\\ntion, Question\\nanswering, text\\nclassification\\n 2020\\n Encoder &\\nDecoder\\n Yes\\n C4 (Colossal Clean\\nCrawled Corpus)\\n InstructGPT\\n(Ouyang\\net al., 2022)\\n Text Generation,\\nQuestion\\nAnswering,\\nsummarization,\\nand so on.\\n 2022\\n Decoder\\n Yes\\n Based on the\\npre-training\\nmodel GPT-3\\n SNLI, MNLI, QNLI, Sc-\\niTail, RTE, RACE, CNN,\\nSQuaD, MRPC, QQP,\\nSTS-B, SST2 & CoLA\\n XSum, CNN/DailyMail,\\nNEWSROOM, Multi-News,\\nGigaword, arXiv, PubMed,\\nBIGPATENT, WikiHow,\\nReddit TIFU, AESLC,BillSum\\nGLUE and SuperGLUE\\nbenchmarks, CNNDM, BBC\\nXSum, and SQuAD data sets,\\nARC Reasoning Challenge,3\\nclosed-book question an-\\nswering data sets (Natural\\nQuestions, Web Questions,\\nand Trivia QA), Wino-\\ngrande Schema Challenge,\\nAdversarial NLI Benchmark\\nGLUE and SuperGLUE\\nbenchmarks, CNN/Daily\\nMail abstractive summa-\\nrization, SQuAD question\\nanswering, WMT En-\\nglish to German, French,\\nand Romanian translation\\nSFT dataset, RM dataset, PPO\\ndataset, a dataset of prompts\\nand completions Winogender,\\nCrowS-Pairs, Real Toxicity\\nPrompts, TruthfulQA, DROP,\\nQuAC, SquadV2, Hellaswag,\\nSST, RTE, and WSC, WMT\\n15 Fr ! En, CNN/Daily Mail\\nSummarization, Reddit TLDR\\nSummarization datasets.\\n Continued on next page\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e051a3e0>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06615849673202615, 'y1': 0.3510121002525252, 'x2': 0.15495912254901958, 'y2': 0.3774282618686869}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.19561274509803922, 'y1': 0.3510121002525252, 'x2': 0.27430426209150327, 'y2': 0.3774282618686869}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.35793002954545455, 'x2': 0.3455820872549019, 'y2': 0.37050906994949495}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\n', 'bbox': {'x1': 0.3656486928104575, 'y1': 0.344094170959596, 'x2': 0.4536516594771242, 'y2': 0.3843461911616162}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.4746160130718954, 'y1': 0.344094170959596, 'x2': 0.5379078248366013, 'y2': 0.3843461911616162}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.590173202614379, 'y1': 0.3510121002525252, 'x2': 0.6757831918300653, 'y2': 0.3774282618686869}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7508382352941176, 'y1': 0.3510121002525252, 'x2': 0.8982423905228758, 'y2': 0.3774282618686869}}, {'text': 'GPT &\\nvariants\\n(Radford\\net al., 2018;\\n2019, Brown\\net al., 2020)\\n', 'bbox': {'x1': 0.06825816993464053, 'y1': 0.3866986383838384, 'x2': 0.15285887614379084, 'y2': 0.4684632848484849}}, {'text': 'PEGASUS\\n(Zhang\\net al., 2020a)\\n', 'bbox': {'x1': 0.06806209150326797, 'y1': 0.4840610121212121, 'x2': 0.1530534879084967, 'y2': 0.524314294949495}}, {'text': 'Switch\\nTransformer\\n(Fedus\\net al., 2021)\\n', 'bbox': {'x1': 0.07016176470588235, 'y1': 0.5814233858585858, 'x2': 0.15095324150326797, 'y2': 0.6355137898989899}}, {'text': 'Text classifica-\\ntion, Question\\nanswering,\\ntextual entail-\\nment, semantic\\nsimilarity\\n', 'bbox': {'x1': 0.1856748366013072, 'y1': 0.3866986383838384, 'x2': 0.28424271732026146, 'y2': 0.4684632848484849}}, {'text': '2018\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.42129207272727276, 'x2': 0.3455983660130719, 'y2': 0.4338711131313131}}, {'text': 'Decoder\\n', 'bbox': {'x1': 0.37976797385620914, 'y1': 0.42129207272727276, 'x2': 0.4349041277777778, 'y2': 0.4338711131313131}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.42129207272727276, 'x2': 0.5181038937908496, 'y2': 0.4338711131313131}}, {'text': 'Book corpus\\n', 'bbox': {'x1': 0.5916062091503268, 'y1': 0.42129207272727276, 'x2': 0.6743511369281046, 'y2': 0.4338711131313131}}, {'text': 'Text sum-\\nmarization\\n', 'bbox': {'x1': 0.20014705882352943, 'y1': 0.49098020404040404, 'x2': 0.26977130751633993, 'y2': 0.517395103030303}}, {'text': '2020\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.4978981333333334, 'x2': 0.3455983660130719, 'y2': 0.5104771737373738}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.37184803921568627, 'y1': 0.49098020404040404, 'x2': 0.44282342483660136, 'y2': 0.517395103030303}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.4978981333333334, 'x2': 0.5181038937908496, 'y2': 0.5104771737373738}}, {'text': 'C4, HugeNews\\n', 'bbox': {'x1': 0.5834509803921568, 'y1': 0.4978981333333334, 'x2': 0.6825072238562091, 'y2': 0.5104771737373738}}, {'text': 'Language\\nunderstanding\\ntask- Translation,\\nquestion\\nanswering,\\nClassification,\\nand so on.\\n', 'bbox': {'x1': 0.1782761437908497, 'y1': 0.5606683353535353, 'x2': 0.2916414156862745, 'y2': 0.6562688404040404}}, {'text': '2021\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.6021784363636363, 'x2': 0.3455983660130719, 'y2': 0.6147574767676768}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.37184803921568627, 'y1': 0.5952605070707071, 'x2': 0.44282342483660136, 'y2': 0.6216766686868687}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.6021784363636363, 'x2': 0.5181038937908496, 'y2': 0.6147574767676768}}, {'text': 'C4(Colossal Clean\\nCrawled Corpus)\\n', 'bbox': {'x1': 0.5717058823529412, 'y1': 0.5952605070707071, 'x2': 0.6942523738562092, 'y2': 0.6216766686868687}}, {'text': 'T5 (Raffel\\net al., 2020)\\n', 'bbox': {'x1': 0.0716764705882353, 'y1': 0.7133779313131313, 'x2': 0.14944009836601307, 'y2': 0.739794092929293}}, {'text': 'Text summariza-\\ntion, Question\\nanswering, text\\nclassification\\n', 'bbox': {'x1': 0.18027124183006535, 'y1': 0.6995408101010102, 'x2': 0.2896482179738562, 'y2': 0.7536312141414142}}, {'text': '2020\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.7202958606060605, 'x2': 0.3455983660130719, 'y2': 0.732874901010101}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.37184803921568627, 'y1': 0.7133779313131313, 'x2': 0.44282342483660136, 'y2': 0.739794092929293}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.7202958606060605, 'x2': 0.5181038937908496, 'y2': 0.732874901010101}}, {'text': 'C4 (Colossal Clean\\nCrawled Corpus)\\n', 'bbox': {'x1': 0.5696699346405228, 'y1': 0.7133779313131313, 'x2': 0.6962861156862745, 'y2': 0.739794092929293}}, {'text': 'InstructGPT\\n(Ouyang\\net al., 2022)\\n', 'bbox': {'x1': 0.07031699346405229, 'y1': 0.8245774262626263, 'x2': 0.15079917385620917, 'y2': 0.8648307090909091}}, {'text': 'Text Generation,\\nQuestion\\nAnswering,\\nsummarization,\\nand so on.\\n', 'bbox': {'x1': 0.1804983660130719, 'y1': 0.8107403050505051, 'x2': 0.28941953692810457, 'y2': 0.8786678303030303}}, {'text': '2022\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.8384145474747475, 'x2': 0.3455983660130719, 'y2': 0.8509935878787879}}, {'text': 'Decoder\\n', 'bbox': {'x1': 0.37976797385620914, 'y1': 0.8384145474747475, 'x2': 0.4349041277777778, 'y2': 0.8509935878787879}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.8384145474747475, 'x2': 0.5181038937908496, 'y2': 0.8509935878787879}}, {'text': 'Based on the\\npre-training\\nmodel GPT-3\\n', 'bbox': {'x1': 0.589187908496732, 'y1': 0.8245774262626263, 'x2': 0.6767676274509803, 'y2': 0.8648307090909091}}, {'text': 'SNLI, MNLI, QNLI, Sc-\\niTail, RTE, RACE, CNN,\\nSQuaD, MRPC, QQP,\\nSTS-B, SST2 & CoLA\\n', 'bbox': {'x1': 0.7421209150326797, 'y1': 0.40053575959595966, 'x2': 0.9069596202614378, 'y2': 0.45462616363636366}}, {'text': 'XSum, CNN/DailyMail,\\nNEWSROOM, Multi-News,\\nGigaword, arXiv, PubMed,\\nBIGPATENT, WikiHow,\\nReddit TIFU, AESLC,BillSum\\nGLUE and SuperGLUE\\nbenchmarks, CNNDM, BBC\\nXSum, and SQuAD data sets,\\nARC Reasoning Challenge,3\\nclosed-book question an-\\nswering data sets (Natural\\nQuestions, Web Questions,\\nand Trivia QA), Wino-\\ngrande Schema Challenge,\\nAdversarial NLI Benchmark\\nGLUE and SuperGLUE\\nbenchmarks, CNN/Daily\\nMail abstractive summa-\\nrization, SQuAD question\\nanswering, WMT En-\\nglish to German, French,\\nand Romanian translation\\nSFT dataset, RM dataset, PPO\\ndataset, a dataset of prompts\\nand completions Winogender,\\nCrowS-Pairs, Real Toxicity\\nPrompts, TruthfulQA, DROP,\\nQuAC, SquadV2, Hellaswag,\\nSST, RTE, and WSC, WMT\\n15 Fr ! En, CNN/Daily Mail\\nSummarization, Reddit TLDR\\nSummarization datasets.\\n', 'bbox': {'x1': 0.7236846405228757, 'y1': 0.47022389090909095, 'x2': 0.9253947330065359, 'y2': 0.9132600020202021}}, {'text': 'Continued on next page\\n', 'bbox': {'x1': 0.7715947712418301, 'y1': 0.915524395959596, 'x2': 0.9264383189542483, 'y2': 0.9281034363636363}}]}\n",
      "{'type': 'table', 'bbox': [0.052141786463120406, 0.10144595753062854, 0.9388275505514706, 0.15712799072265626], 'properties': {'score': 0.6185730695724487, 'title': None, 'columns': None, 'rows': None, 'page_number': 20}, 'text_representation': 'Transformer\\nModels\\n Task Ac-\\ncomplished\\n Year\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training\\nDataset\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e051bc40>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06615849673202615, 'y1': 0.12046538308080808, 'x2': 0.15495912254901958, 'y2': 0.14688154469696957}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.19561274509803922, 'y1': 0.12046538308080808, 'x2': 0.27430426209150327, 'y2': 0.14688154469696957}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.12738331237373743, 'x2': 0.3455820872549019, 'y2': 0.13996235277777777}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.4746160130718954, 'y1': 0.11354619116161614, 'x2': 0.5379078248366013, 'y2': 0.15379947398989877}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.590173202614379, 'y1': 0.12046538308080808, 'x2': 0.6757831918300653, 'y2': 0.14688154469696957}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7508382352941176, 'y1': 0.12046538308080808, 'x2': 0.8982423905228758, 'y2': 0.14688154469696957}}]}\n",
      "{'type': 'Caption', 'bbox': [0.29252154181985296, 0.16637447010387074, 0.7107897231158088, 0.1808320340243253], 'properties': {'score': 0.46226966381073, 'page_number': 20}, 'text_representation': 'Table 6: Transformer models for NLP - text summarization task\\n'}\n",
      "{'type': 'Text', 'bbox': [0.29252154181985296, 0.16637447010387074, 0.7107897231158088, 0.1808320340243253], 'properties': {'score': 0.38815534114837646, 'page_number': 20}, 'text_representation': 'Table 6: Transformer models for NLP - text summarization task\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0909152670467601, 0.19959062056107954, 0.9112679515165442, 0.2553116122159091], 'properties': {'score': 0.6502254605293274, 'page_number': 20}}\n",
      "{'type': 'List-item', 'bbox': [0.0912769182990579, 0.26000255237926134, 0.9110336483226102, 0.31595176003196024], 'properties': {'score': 0.6258912682533264, 'page_number': 20}}\n",
      "{'type': 'Text', 'bbox': [0.1051442045323989, 0.3317903553355824, 0.9113275505514706, 0.3741586026278409], 'properties': {'score': 0.7687577605247498, 'page_number': 20}, 'text_representation': 'GPT & variants, InstructGPT and Switch Transformer: These models have been discussed in earlier sections. More-\\nover, apart from text summarization, certain models such as GPT and its variants can perform question-answering tasks,\\nInstructGPT can generate text, and Charformer models are capable of classification and segmentation tasks as well.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09433750825769761, 0.382699668190696, 0.28286141788258273, 0.3971788163618608], 'properties': {'score': 0.5098320841789246, 'page_number': 20}}\n",
      "{'type': 'Text', 'bbox': [0.0942766346650965, 0.39969415838068184, 0.9142013729319853, 0.4699578857421875], 'properties': {'score': 0.782175600528717, 'page_number': 20}, 'text_representation': '6.1.5 TEXT GENERATION\\nThe task of text generation has gained immense popularity in the field of NLP due to its usefulness in generating long-form\\ndocumentation, among other applications. Text generation models attempt to derive meaning from trained text data and\\ncreate a connection between the text that has been previously outputted. These models typically operate on the basis of\\nthis connection (Chowdhary & Chowdhary, 2020, Reiter & Dale, 1997). The use of transformer-based models has led to\\nsignificant advancements in the task of text generation. Please refer to Table 7.\\n'}\n",
      "{'type': 'table', 'bbox': [0.05222368128159467, 0.49025046608664774, 0.9441810518152574, 0.8154883367365057], 'properties': {'score': 0.7535510659217834, 'title': None, 'columns': None, 'rows': None, 'page_number': 20}, 'text_representation': 'Transformer\\nModels\\n Task Ac-\\ncomplished\\n Year\\n Architecture\\n(Encoder/\\nDecoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training\\nDataset\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n CTRL\\n(Keskar\\net al., 2019)\\n Text Generation\\n 2019\\n Encoder &\\nDecoder\\n Yes\\n Language\\nTranslation,\\nSentence\\nReconstruction,\\nComprehension,\\ntext Generation\\n 2019\\n Encoder &\\nDecoder\\n Text Prediction\\n 2020\\n Encoder &\\nDecoder\\n Yes\\n Yes\\n BART (Lewis\\net al., 2020)\\n ProphetNET\\n(Qi et al.,\\n2020)\\n Project Gutenberg,\\nsubreddits, News\\nData, Amazon\\nReview, open\\nWebText, WMT\\nTranslation date,\\nquestion-answer\\npairs, MRQA\\nCorrupting docu-\\nments, 1M steps\\non a combina-\\ntion of books and\\nWikipedia data,\\nnews, stories, and\\nweb text (Training)\\nBookcorpus, English\\nWikipedia news,\\nbooks, stories,\\nand web text\\n Multilingual Wikipedia\\nand Open WebText.\\n SQuAD, MNLI, ELI5,\\nXSum, ConvAI2, CNN/DM,\\nCNN/DailyMail, WMT16\\nRomanian-English, augmented\\nwith back-translation data\\nfrom Sennrich et al. (2016).\\n CNN/dailymail, Giga-word\\nCorpus, SQuAD dataset.\\n Continued on next page\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e03543d0>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06615849673202615, 'y1': 0.4992254840909091, 'x2': 0.15495912254901958, 'y2': 0.5256416457070707}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.19561274509803922, 'y1': 0.4992254840909091, 'x2': 0.27430426209150327, 'y2': 0.5256416457070707}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.5061434133838384, 'x2': 0.3455820872549019, 'y2': 0.5187224537878787}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\n', 'bbox': {'x1': 0.3656486928104575, 'y1': 0.4923062921717172, 'x2': 0.4536516594771242, 'y2': 0.532559575}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.4746160130718954, 'y1': 0.4923062921717172, 'x2': 0.5379078248366013, 'y2': 0.532559575}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.590173202614379, 'y1': 0.4992254840909091, 'x2': 0.6757831918300653, 'y2': 0.5256416457070707}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7531535947712419, 'y1': 0.4992254840909091, 'x2': 0.90055775, 'y2': 0.5256416457070707}}, {'text': 'CTRL\\n(Keskar\\net al., 2019)\\n', 'bbox': {'x1': 0.0716764705882353, 'y1': 0.569504193939394, 'x2': 0.14944009836601307, 'y2': 0.6097574767676768}}, {'text': 'Text Generation\\n', 'bbox': {'x1': 0.18253267973856208, 'y1': 0.5833413151515151, 'x2': 0.2873841611111111, 'y2': 0.5959203555555556}}, {'text': '2019\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.5833413151515151, 'x2': 0.3455983660130719, 'y2': 0.5959203555555556}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.37184803921568627, 'y1': 0.5764233858585859, 'x2': 0.44282342483660136, 'y2': 0.6028395474747474}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.5833413151515151, 'x2': 0.5181038937908496, 'y2': 0.5959203555555556}}, {'text': 'Language\\nTranslation,\\nSentence\\nReconstruction,\\nComprehension,\\ntext Generation\\n', 'bbox': {'x1': 0.18137745098039215, 'y1': 0.6530294464646464, 'x2': 0.28854051601307185, 'y2': 0.7347940929292929}}, {'text': '2019\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.6876228808080808, 'x2': 0.3455983660130719, 'y2': 0.7002019212121213}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.37184803921568627, 'y1': 0.6807036888888889, 'x2': 0.44282342483660136, 'y2': 0.7071198505050504}}, {'text': 'Text Prediction\\n', 'bbox': {'x1': 0.18523529411764705, 'y1': 0.7642289414141414, 'x2': 0.2846822277777778, 'y2': 0.7768079818181819}}, {'text': '2020\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.7642289414141414, 'x2': 0.3455983660130719, 'y2': 0.7768079818181819}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.37184803921568627, 'y1': 0.7573110121212121, 'x2': 0.44282342483660136, 'y2': 0.7837259111111111}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.6876228808080808, 'x2': 0.5181038937908496, 'y2': 0.7002019212121213}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.7642289414141414, 'x2': 0.5181038937908496, 'y2': 0.7768079818181819}}, {'text': 'BART (Lewis\\net al., 2020)\\n', 'bbox': {'x1': 0.06518954248366013, 'y1': 0.6807036888888889, 'x2': 0.15592734052287582, 'y2': 0.7071198505050504}}, {'text': 'ProphetNET\\n(Qi et al.,\\n2020)\\n', 'bbox': {'x1': 0.06941339869281046, 'y1': 0.7503918202020201, 'x2': 0.1517025212418301, 'y2': 0.7906451030303031}}, {'text': 'Project Gutenberg,\\nsubreddits, News\\nData, Amazon\\nReview, open\\nWebText, WMT\\nTranslation date,\\nquestion-answer\\npairs, MRQA\\nCorrupting docu-\\nments, 1M steps\\non a combina-\\ntion of books and\\nWikipedia data,\\nnews, stories, and\\nweb text (Training)\\nBookcorpus, English\\nWikipedia news,\\nbooks, stories,\\nand web text\\n', 'bbox': {'x1': 0.5646977124183007, 'y1': 0.5349120222222222, 'x2': 0.7012602147058824, 'y2': 0.7975630323232322}}, {'text': 'Multilingual Wikipedia\\nand Open WebText.\\n', 'bbox': {'x1': 0.7505326797385621, 'y1': 0.5764233858585859, 'x2': 0.9031785950980393, 'y2': 0.6028395474747474}}, {'text': 'SQuAD, MNLI, ELI5,\\nXSum, ConvAI2, CNN/DM,\\nCNN/DailyMail, WMT16\\nRomanian-English, augmented\\nwith back-translation data\\nfrom Sennrich et al. (2016).\\n', 'bbox': {'x1': 0.7260261437908497, 'y1': 0.6530294464646464, 'x2': 0.9276874, 'y2': 0.7347940929292929}}, {'text': 'CNN/dailymail, Giga-word\\nCorpus, SQuAD dataset.\\n', 'bbox': {'x1': 0.737452614379085, 'y1': 0.7573110121212121, 'x2': 0.9162584941176471, 'y2': 0.7837259111111111}}, {'text': 'Continued on next page\\n', 'bbox': {'x1': 0.7762271241830065, 'y1': 0.7998274262626263, 'x2': 0.9310706718954249, 'y2': 0.8124064666666667}}]}\n",
      "{'type': 'table', 'bbox': [0.05174875147202436, 0.09966142134232954, 0.9437485638786764, 0.29512698086825284], 'properties': {'score': 0.7574057579040527, 'title': None, 'columns': None, 'rows': None, 'page_number': 21}, 'text_representation': 'Transformer\\nModels\\n Task Ac-\\ncomplished\\n Year\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training\\nDataset\\n InstructGPT\\n(Ouyang\\net al., 2022)\\n Text Generation,\\nQuestion\\nAnswering,\\nsummarization\\nand so on.\\n 2022\\n Decoder\\n Yes\\n Based on the\\npre-training\\nmodel GPT-3\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n SFT dataset, RM dataset, PPO\\ndataset, dataset of prompts\\nand completions Winogender,\\nCrowS-Pairs, Real Toxicity\\nPrompts , TruthfulQA , DROP\\n, QuAC , SquadV2 , Hellaswag\\n, SST , RTE and WSC, WMT\\n15 Fr ! En, CNN/Daily Mail\\nSummarization, Reddit TLDR\\nSummarization datasets.\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e0355450>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06615849673202615, 'y1': 0.12046538308080808, 'x2': 0.15495912254901958, 'y2': 0.14688154469696957}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.19561274509803922, 'y1': 0.12046538308080808, 'x2': 0.27430426209150327, 'y2': 0.14688154469696957}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.12738331237373743, 'x2': 0.3455820872549019, 'y2': 0.13996235277777777}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.4746160130718954, 'y1': 0.11354619116161614, 'x2': 0.5379078248366013, 'y2': 0.15379947398989877}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.590173202614379, 'y1': 0.12046538308080808, 'x2': 0.6757831918300653, 'y2': 0.14688154469696957}}, {'text': 'InstructGPT\\n(Ouyang\\net al., 2022)\\n', 'bbox': {'x1': 0.07031699346405229, 'y1': 0.2040786888888889, 'x2': 0.15079917385620917, 'y2': 0.24433197171717166}}, {'text': 'Text Generation,\\nQuestion\\nAnswering,\\nsummarization\\nand so on.\\n', 'bbox': {'x1': 0.1804983660130719, 'y1': 0.19024156767676761, 'x2': 0.28941953692810457, 'y2': 0.2581678303030301}}, {'text': '2022\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.21791581010101005, 'x2': 0.3455983660130719, 'y2': 0.23049485050505053}}, {'text': 'Decoder\\n', 'bbox': {'x1': 0.37976797385620914, 'y1': 0.21791581010101005, 'x2': 0.4349041277777778, 'y2': 0.23049485050505053}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.21791581010101005, 'x2': 0.5181038937908496, 'y2': 0.23049485050505053}}, {'text': 'Based on the\\npre-training\\nmodel GPT-3\\n', 'bbox': {'x1': 0.589187908496732, 'y1': 0.2040786888888889, 'x2': 0.6767676274509803, 'y2': 0.24433197171717166}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7531535947712419, 'y1': 0.12046538308080808, 'x2': 0.90055775, 'y2': 0.14688154469696957}}, {'text': 'SFT dataset, RM dataset, PPO\\ndataset, dataset of prompts\\nand completions Winogender,\\nCrowS-Pairs, Real Toxicity\\nPrompts , TruthfulQA , DROP\\n, QuAC , SquadV2 , Hellaswag\\n, SST , RTE and WSC, WMT\\n15 Fr ! En, CNN/Daily Mail\\nSummarization, Reddit TLDR\\nSummarization datasets.\\n', 'bbox': {'x1': 0.7250816993464051, 'y1': 0.15564939595959598, 'x2': 0.928631291503268, 'y2': 0.2927612646464642}}]}\n",
      "{'type': 'Caption', 'bbox': [0.30585057875689337, 0.3055464588512074, 0.6970000861672794, 0.32003612171519885], 'properties': {'score': 0.49508315324783325, 'page_number': 21}, 'text_representation': 'Table 7: Transformer models for NLP - text generation task\\n'}\n",
      "{'type': 'Text', 'bbox': [0.30585057875689337, 0.3055464588512074, 0.6970000861672794, 0.32003612171519885], 'properties': {'score': 0.41505974531173706, 'page_number': 21}, 'text_representation': 'Table 7: Transformer models for NLP - text generation task\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09060725492589614, 0.33879877263849434, 0.9117395737591911, 0.39440732088955965], 'properties': {'score': 0.7941456437110901, 'page_number': 21}, 'text_representation': '• CTRL: The acronym CTRL denotes the Conditional transformer Language model, which excels in generating realistic text\\nresembling human language, contingent on a given condition. In addition, CTRL can produce text in multiple languages.\\nThis model is large-scale, boasting 1.63 billion parameters, and can be fine-tuned for various generative tasks, such as\\nquestion answering and text summarization (Keskar et al., 2019).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09100994334501379, 0.3994006902521307, 0.9114578785615809, 0.4550244972922585], 'properties': {'score': 0.8216985464096069, 'page_number': 21}, 'text_representation': '• ProphetNET: ProphetNET is a sequence-to-sequence model that utilizes future n-gram prediction to facilitate text gener-\\nation by predicting n-grams ahead. The model adheres to the transformer architecture, comprising encoder and decoder\\nmodules. It distinguishes itself by employing an n-stream self-attention mechanism. ProphetNET demonstrates remarkable\\nperformance in summarization and is also competent in question generation tasks (Qi et al., 2020).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09134534050436581, 0.4595507257634943, 0.9121436982996324, 0.529548173384233], 'properties': {'score': 0.8276795148849487, 'page_number': 21}, 'text_representation': '• InstructGPT: It was proposed as a solution to the problem of language generative models failing to produce realistic and\\ntruthful results. It achieves this by incorporating human feedback during fine-tuning and reinforcement learning from the\\nfeedback. The GPT-3 model was fine-tuned for this purpose. As a result, InstructGPT can generate more realistic and\\nnatural output that is useful in real-life applications. ChatGPT, which follows a similar methodology as InstructGPT, has\\ngained significant attention in the field of NLP at the end of 2022 (Ouyang et al., 2022).\\n'}\n",
      "{'type': 'Text', 'bbox': [0.10537135404698988, 0.5450503817471591, 0.9115651568244485, 0.5731323797052557], 'properties': {'score': 0.8324379324913025, 'page_number': 21}, 'text_representation': 'BART: The BART model’s description is mentioned above in the language translation section. This model can execute the\\nlanguage translation task as well.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09394206327550551, 0.5821482155539772, 0.3891565659466912, 0.5965866921164773], 'properties': {'score': 0.4311441481113434, 'page_number': 21}}\n",
      "{'type': 'List-item', 'bbox': [0.09394206327550551, 0.5821482155539772, 0.3891565659466912, 0.5965866921164773], 'properties': {'score': 0.3715420365333557, 'page_number': 21}}\n",
      "{'type': 'Text', 'bbox': [0.09370130651137408, 0.599738935990767, 0.9135145479090073, 0.738534102006392], 'properties': {'score': 0.7994639277458191, 'page_number': 21}, 'text_representation': '6.1.6 NATURAL LANGUAGE REASONING\\nThe pursuit of natural language reasoning is a field of study that is distinct from that of question-answering. Question-\\nanswering focuses on finding the answer to a specific query within a given text passage. On the other hand, natural language\\nreasoning involves the application of deductive reasoning to derive a conclusion from the given premises and rules that are\\nrepresented in natural language. Neural network architectures aim to learn how to utilize these premises and rules to infer\\nnew conclusions. Previously, a similar task was traditionally tackled by systems equipped with the knowledge represented\\nin a formal format and rules to be applied for the derivation of new knowledge. However, the use of formal representation\\nhas posed a significant challenge to this line of research (Mark A Musen, 1988). With the advent of transformers and\\ntheir remarkable performance in numerous NLP tasks, it is now possible to circumvent formal representation and allow\\ntransformers to engage in reasoning directly using natural language. Table 8 highlights some of the significant transformer\\nmodels for natural language reasoning tasks.\\n'}\n",
      "{'type': 'table', 'bbox': [0.05133065616383272, 0.7572592440518466, 0.9349931784237132, 0.9448445268110796], 'properties': {'score': 0.8175092339515686, 'title': None, 'columns': None, 'rows': None, 'page_number': 21}, 'text_representation': 'Transformer\\nModels\\n (Clark et al., 2020)\\n(Clark et al., 2020b)\\n(Richardson et al.,\\n2022) (Richardson\\n& Sabharwal, 2022)\\n (Saha et al., 2020)\\n(Saha et al., 2020)\\n (Sinha et al., 2019)\\n(Sinha et al., 2019)\\n Task Ac-\\ncomplished\\n Binary Clas-\\nsification\\n Binary Clas-\\nsification\\n Binary Classifi-\\ncation, Sequence\\nGeneration\\nSequence\\nGeneration\\n Year\\n 2020\\n 2022\\n 2020\\n Architecture\\n(Encoder/\\nDecoder)\\nRoBERTa\\n(Encoder)\\n RoBERTa\\nLarge (Encoder)\\n PRover\\n[RoBERTa-\\nbased](Encoder)\\n 2019\\n BERT (Encoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training\\nDataset\\n Dataset (Fine-\\ntuning, Train-\\ning, Testing)\\n Yes\\n Yes\\n No\\n No\\n RACE\\n RuleTaker\\n RACE\\n Hard-RuleTaker\\n NA\\n NA\\n RuleTaker\\n CLUTRR\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e0356050>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.08977941176470589, 'y1': 0.7676636154040404, 'x2': 0.17858003758169935, 'y2': 0.794079777020202}}, {'text': '(Clark et al., 2020)\\n(Clark et al., 2020b)\\n(Richardson et al.,\\n2022) (Richardson\\n& Sabharwal, 2022)\\n', 'bbox': {'x1': 0.06839705882352941, 'y1': 0.8028463656565658, 'x2': 0.19996198235294121, 'y2': 0.8712764161616162}}, {'text': '(Saha et al., 2020)\\n(Saha et al., 2020)\\n', 'bbox': {'x1': 0.07473039215686274, 'y1': 0.8799562141414141, 'x2': 0.1936304418300654, 'y2': 0.9063723757575757}}, {'text': '(Sinha et al., 2019)\\n(Sinha et al., 2019)\\n', 'bbox': {'x1': 0.07201143790849673, 'y1': 0.9150521737373738, 'x2': 0.19634859281045755, 'y2': 0.9414683353535354}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.24934150326797383, 'y1': 0.7676636154040404, 'x2': 0.3280330202614379, 'y2': 0.794079777020202}}, {'text': 'Binary Clas-\\nsification\\n', 'bbox': {'x1': 0.24731535947712419, 'y1': 0.8028463656565658, 'x2': 0.33006028725490194, 'y2': 0.8292625272727272}}, {'text': 'Binary Clas-\\nsification\\n', 'bbox': {'x1': 0.24731535947712419, 'y1': 0.8379423252525252, 'x2': 0.33006028725490194, 'y2': 0.8643584868686869}}, {'text': 'Binary Classifi-\\ncation, Sequence\\nGeneration\\nSequence\\nGeneration\\n', 'bbox': {'x1': 0.23353431372549016, 'y1': 0.8730382848484849, 'x2': 0.34383917908496725, 'y2': 0.9414683353535354}}, {'text': 'Year\\n', 'bbox': {'x1': 0.38060620915032684, 'y1': 0.7745815446969696, 'x2': 0.41314744673202614, 'y2': 0.7871605851010101}}, {'text': '2020\\n', 'bbox': {'x1': 0.38059803921568625, 'y1': 0.8097655575757576, 'x2': 0.4131555555555555, 'y2': 0.822344597979798}}, {'text': '2022\\n', 'bbox': {'x1': 0.38059803921568625, 'y1': 0.8448615171717172, 'x2': 0.4131555555555555, 'y2': 0.8574405575757575}}, {'text': '2020\\n', 'bbox': {'x1': 0.38059803921568625, 'y1': 0.886875406060606, 'x2': 0.4131555555555555, 'y2': 0.8994544464646466}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\nRoBERTa\\n(Encoder)\\n', 'bbox': {'x1': 0.45411764705882357, 'y1': 0.7607444234848485, 'x2': 0.5421206137254903, 'y2': 0.8292625272727272}}, {'text': 'RoBERTa\\nLarge (Encoder)\\n', 'bbox': {'x1': 0.44470751633986927, 'y1': 0.8379423252525252, 'x2': 0.5515287274509804, 'y2': 0.8643584868686869}}, {'text': 'PRover\\n[RoBERTa-\\nbased](Encoder)\\n', 'bbox': {'x1': 0.44433333333333325, 'y1': 0.8730382848484849, 'x2': 0.5519033673202614, 'y2': 0.9132915676767677}}, {'text': '2019\\n', 'bbox': {'x1': 0.38059803921568625, 'y1': 0.9219713656565657, 'x2': 0.4131555555555555, 'y2': 0.934550406060606}}, {'text': 'BERT (Encoder)\\n', 'bbox': {'x1': 0.4432271241830065, 'y1': 0.9219713656565657, 'x2': 0.5530110692810458, 'y2': 0.934550406060606}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.5816094771241831, 'y1': 0.7607444234848485, 'x2': 0.644901288888889, 'y2': 0.8009977063131313}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.6740081699346405, 'y1': 0.7676636154040404, 'x2': 0.7596181591503268, 'y2': 0.794079777020202}}, {'text': 'Dataset (Fine-\\ntuning, Train-\\ning, Testing)\\n', 'bbox': {'x1': 0.8037450980392157, 'y1': 0.7607444234848485, 'x2': 0.9018408947712417, 'y2': 0.8009977063131313}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.6014133986928104, 'y1': 0.8097655575757576, 'x2': 0.6250989918300652, 'y2': 0.822344597979798}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.6014133986928104, 'y1': 0.8448615171717172, 'x2': 0.6250989918300652, 'y2': 0.8574405575757575}}, {'text': 'No\\n', 'bbox': {'x1': 0.6033088235294118, 'y1': 0.886875406060606, 'x2': 0.6232014660130719, 'y2': 0.8994544464646466}}, {'text': 'No\\n', 'bbox': {'x1': 0.6033088235294118, 'y1': 0.9219713656565657, 'x2': 0.6232014660130719, 'y2': 0.934550406060606}}, {'text': 'RACE\\n', 'bbox': {'x1': 0.6954313725490195, 'y1': 0.8097655575757576, 'x2': 0.7381956702614378, 'y2': 0.822344597979798}}, {'text': 'RuleTaker\\n', 'bbox': {'x1': 0.8191699346405229, 'y1': 0.8097655575757576, 'x2': 0.8864174846405228, 'y2': 0.822344597979798}}, {'text': 'RACE\\n', 'bbox': {'x1': 0.6954313725490195, 'y1': 0.8448615171717172, 'x2': 0.7381956702614378, 'y2': 0.8574405575757575}}, {'text': 'Hard-RuleTaker\\n', 'bbox': {'x1': 0.8001895424836601, 'y1': 0.8448615171717172, 'x2': 0.9053991565359476, 'y2': 0.8574405575757575}}, {'text': 'NA\\n', 'bbox': {'x1': 0.70534477124183, 'y1': 0.886875406060606, 'x2': 0.728281541503268, 'y2': 0.8994544464646466}}, {'text': 'NA\\n', 'bbox': {'x1': 0.70534477124183, 'y1': 0.9219713656565657, 'x2': 0.728281541503268, 'y2': 0.934550406060606}}, {'text': 'RuleTaker\\n', 'bbox': {'x1': 0.8191699346405229, 'y1': 0.886875406060606, 'x2': 0.8864174846405228, 'y2': 0.8994544464646466}}, {'text': 'CLUTRR\\n', 'bbox': {'x1': 0.8206830065359477, 'y1': 0.9219713656565657, 'x2': 0.8849027075163399, 'y2': 0.934550406060606}}]}\n",
      "{'type': 'table', 'bbox': [0.05164313372443704, 0.10119748202237216, 0.9351720473345588, 0.1851500216397372], 'properties': {'score': 0.7211607694625854, 'title': None, 'columns': None, 'rows': None, 'page_number': 22}, 'text_representation': 'Transformer\\nModels\\n (Picco et al., 2021)\\n(Picco et al., 2021)\\n Task Ac-\\ncomplished\\n Binary Clas-\\nsification\\n Year\\n 2021\\n Yes\\n Pre-training\\nDataset\\n Dataset (Fine-\\ntuning, Train-\\ning, Testing)\\n RACE\\n RuleTaker\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e03574c0>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.08977941176470589, 'y1': 0.12046538308080808, 'x2': 0.17858003758169935, 'y2': 0.14688154469696957}}, {'text': '(Picco et al., 2021)\\n(Picco et al., 2021)\\n', 'bbox': {'x1': 0.07246732026143791, 'y1': 0.15564939595959598, 'x2': 0.1958928647058824, 'y2': 0.182064294949495}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.24934150326797383, 'y1': 0.12046538308080808, 'x2': 0.3280330202614379, 'y2': 0.14688154469696957}}, {'text': 'Binary Clas-\\nsification\\n', 'bbox': {'x1': 0.24731535947712419, 'y1': 0.15564939595959598, 'x2': 0.33006028725490194, 'y2': 0.182064294949495}}, {'text': 'Year\\n', 'bbox': {'x1': 0.38060620915032684, 'y1': 0.12738331237373743, 'x2': 0.41314744673202614, 'y2': 0.13996235277777777}}, {'text': '2021\\n', 'bbox': {'x1': 0.38059803921568625, 'y1': 0.16256732525252518, 'x2': 0.4131555555555555, 'y2': 0.17514636565656566}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.6014133986928104, 'y1': 0.16256732525252518, 'x2': 0.6250989918300652, 'y2': 0.17514636565656566}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.6740081699346405, 'y1': 0.12046538308080808, 'x2': 0.7596181591503268, 'y2': 0.14688154469696957}}, {'text': 'Dataset (Fine-\\ntuning, Train-\\ning, Testing)\\n', 'bbox': {'x1': 0.8037450980392157, 'y1': 0.11354619116161614, 'x2': 0.9018408947712417, 'y2': 0.15379947398989877}}, {'text': 'RACE\\n', 'bbox': {'x1': 0.6954313725490195, 'y1': 0.16256732525252518, 'x2': 0.7381956702614378, 'y2': 0.17514636565656566}}, {'text': 'RuleTaker\\n', 'bbox': {'x1': 0.8191699346405229, 'y1': 0.16256732525252518, 'x2': 0.8864174846405228, 'y2': 0.17514636565656566}}]}\n",
      "{'type': 'Caption', 'bbox': [0.30367083381204046, 0.19516122991388493, 0.6982430491727941, 0.20897546941583806], 'properties': {'score': 0.6231634020805359, 'page_number': 22}, 'text_representation': 'Table 8: Transformer models for natural language reasoning\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09086224724264706, 0.22848237471147018, 0.9120408002068014, 0.32535350452769884], 'properties': {'score': 0.7128949165344238, 'page_number': 22}, 'text_representation': '• RoBERTa:\\n In a 2020 study by Clark et al. (2020b), a binary classification task was assigned to the transformer, which\\naimed to determine whether a given statement can be inferred from a provided set of premises and rules represented in\\nnatural language. The architecture utilized for the transformer was RoBERTa-large, which was pre-trained on a dataset\\nof high school exam questions that required reasoning skills. This pre-training enabled the transformer to achieve a high\\naccuracy of 98% on the test dataset. The dataset contained theories that were randomly sampled and constructed using sets\\nof names and attributes. The task required the transformer to classify whether the given statement (Statement) followed\\nfrom the provided premises and rules (Context) (Clark et al., 2020b)\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09120035508099725, 0.3299339433149858, 0.9122807760799633, 0.4833681973544034], 'properties': {'score': 0.7477578520774841, 'page_number': 22}, 'text_representation': '• RoBERTa-Large:\\n In the work by Richardson & Sabharwal (2022), the authors aimed to address a limitation of the\\ndataset construction approach presented in the work by Clark et al. (2020b). They highlighted that the uniform random\\nsampling of theories, as done in (Clark et al., 2020b), does not always result in challenging instances. To overcome this\\nlimitation, they proposed a novel methodology for creating more challenging algorithmic reasoning datasets. The key\\nidea of their methodology is to sample hard instances from ordinary SAT propositional formulas and translate them into\\nnatural language using a predefined set of English rule languages. By following this approach, they were able to construct\\na more challenging dataset that is consequential for training robust models and for reliable evaluation. To demonstrate the\\neffectiveness of their approach, the authors conducted experiments where they tested the models trained using the dataset\\nfrom (Clark et al., 2020b) on their newly constructed dataset. The results showed that the models achieved an accuracy of\\n57.7% and 59.6% for T5 and RoBERTa, respectively. These findings highlight that models trained on easy datasets may not\\nbe capable of solving challenging instances of the problem.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09099024155560662, 0.48735706676136364, 0.9116912482766544, 0.58472900390625], 'properties': {'score': 0.7870268821716309, 'page_number': 22}, 'text_representation': '• PRover: In a related study, Saha et al. (2020) proposed a model called PRover, which is an interpretable joint transformer\\ncapable of generating a corresponding proof with an accuracy of 87%. The task addressed by PRover is the same as that\\nin the study by Clark et al. (2020b) and Richardson & Sabharwal (2022), where the aim is to determine whether a given\\nconclusion follows from the provided premises and rules. The proof generated by PRover is represented as a directed graph,\\nwhere the nodes represent statements and rules, and the edges indicate which new statements follow from applying rules\\non the previous statements. Overall, the proposed approach by Saha et al. (2020) provides a promising direction towards\\nachieving interpretable and accurate reasoning models.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0914190673828125, 0.588935380415483, 0.9120537252987132, 0.7004045521129262], 'properties': {'score': 0.8246738314628601, 'page_number': 22}, 'text_representation': '• BERT-based: In (Picco et al., 2021), a BERT-based architecture called “neural unifier” was proposed to improve the\\ngeneralization performance of the model on the RuleTaker dataset. The authors aimed to mimic some elements of the\\nbackward-chaining reasoning procedure to enhance the model’s ability to handle queries that require multiple steps to\\nanswer, even when trained on shallow queries only. The neural unifier consists of two standard BERT transformers, namely\\nthe fact-checking unit and the unification unit. The fact-checking unit is trained to classify whether a query of depth 0,\\nrepresented by the embedding vector q-0, follows from a given knowledge base represented by the embedding vector C.\\nThe unification unit takes as input the embedding vector q-n of a depth-n query and the embedding vector of the knowledge\\nbase, vector C, and tries to predict an embedding vector q0, thereby performing backward-chaining.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.091514793844784, 0.7049095015092329, 0.9119254078584559, 0.7749498401988636], 'properties': {'score': 0.786280632019043, 'page_number': 22}, 'text_representation': '• BERT: Sinha et al. (2019) introduced a dataset named CLUTRR, which differs from the previously discussed studies in\\nthat the rules are not given in the input to be used to infer conclusions. Instead, the BERT transformer model is tasked with\\nboth extracting relationships between entities and inferring the rules governing these relationships. For instance, given a\\nknowledge base consisting of statements such as “Alice is Bob’s mother” and “Jim is Alice’s father”, the network can infer\\nthat “Jim is Bob’s grandfather”.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09458754595588235, 0.7840352006392045, 0.40172528435202204, 0.797331709428267], 'properties': {'score': 0.6577511429786682, 'page_number': 22}}\n",
      "{'type': 'Text', 'bbox': [0.094217628030216, 0.8013515403053977, 0.9124656048943015, 0.9123585094105113], 'properties': {'score': 0.8887209892272949, 'page_number': 22}, 'text_representation': '6.1.7 AUTOMATED SYMBOLIC REASONING\\nAutomated symbolic reasoning is a subfield of computer science that deals with solving logical problems such as SAT solving\\nand automated theorem proving. These problems are traditionally addressed using search techniques with heuristics. How-\\never, recent studies have explored the use of learning-based techniques to improve the efficiency and effectiveness of these\\nmethods. One approach is to learn the selection of efficient heuristics used by traditional algorithms. Alternatively, an end-to-\\nend learning-based solution can be employed for the problem. Both approaches have shown promising results and offer the\\npotential for further advancements in automated symbolic reasoning (Kurin et al., 2020, Selsam et al., 2019). In this regard, a\\nnumber of transformer based models have shown significant performance in automated symbolic reasoning tasks. For those\\nmodels, please look at Table 9.\\n'}\n",
      "{'type': 'table', 'bbox': [0.051236599192899815, 0.0970612890070135, 0.934583309397978, 0.3109681008078835], 'properties': {'score': 0.6417233943939209, 'title': None, 'columns': None, 'rows': None, 'page_number': 23}, 'text_representation': 'Transformer\\nModels\\n (Shi et al., 2022)\\n(Shi et al., 2022b)\\n (Shi et al., 2021)\\n(Shi et al., 2021)\\n (Hahn et al., 2021)\\n(Hahn et al., 2021)\\n (Polu et al,\\n2020) (Polu &\\nSutskever, 2020)\\n Task Ac-\\ncomplished\\n Binary Clas-\\nsification\\n Binary Clas-\\nsification\\n Sequence\\nGeneration\\n Sequence\\nGeneration\\n Year\\n 2022\\n 2021\\n 2021\\n Architecture\\n(Encoder/\\nDecoder)\\nSATFormer\\n(Encoder /\\nDecoder)\\nTRSAT (Encoder\\n/ Decoder)\\nTransformer\\n(Encoder /\\nDecoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training\\nDataset\\n No\\n No\\n No\\n NA\\n NA\\n NA\\n 2020\\n GPT-f (Decoder)\\n Yes\\n CommonCrawl,\\nGithub,\\narXiv,\\nWebMath\\n Dataset (Fine-\\ntuning, Train-\\ning, Testing)\\n Synthetic\\n Synthetic, SATLIB\\n Synthetic\\n set.mm\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e038c0a0>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.08977941176470589, 'y1': 0.10662826186868693, 'x2': 0.17858003758169935, 'y2': 0.13304442348484843}}, {'text': '(Shi et al., 2022)\\n(Shi et al., 2022b)\\n', 'bbox': {'x1': 0.07562581699346406, 'y1': 0.14873020404040402, 'x2': 0.1927352032679739, 'y2': 0.17514636565656566}}, {'text': '(Shi et al., 2021)\\n(Shi et al., 2021)\\n', 'bbox': {'x1': 0.07969444444444446, 'y1': 0.18382616363636353, 'x2': 0.18866445163398696, 'y2': 0.21024232525252518}}, {'text': '(Hahn et al., 2021)\\n(Hahn et al., 2021)\\n', 'bbox': {'x1': 0.07292320261437908, 'y1': 0.2189221232323232, 'x2': 0.19543713660130724, 'y2': 0.24533828484848483}}, {'text': '(Polu et al,\\n2020) (Polu &\\nSutskever, 2020)\\n', 'bbox': {'x1': 0.0794264705882353, 'y1': 0.26093601212121204, 'x2': 0.18893367679738565, 'y2': 0.3011892949494948}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.24934150326797383, 'y1': 0.10662826186868693, 'x2': 0.3280330202614379, 'y2': 0.13304442348484843}}, {'text': 'Binary Clas-\\nsification\\n', 'bbox': {'x1': 0.24731535947712419, 'y1': 0.14873020404040402, 'x2': 0.33006028725490194, 'y2': 0.17514636565656566}}, {'text': 'Binary Clas-\\nsification\\n', 'bbox': {'x1': 0.24731535947712419, 'y1': 0.18382616363636353, 'x2': 0.33006028725490194, 'y2': 0.21024232525252518}}, {'text': 'Sequence\\nGeneration\\n', 'bbox': {'x1': 0.25252450980392155, 'y1': 0.2189221232323232, 'x2': 0.32485103235294116, 'y2': 0.24533828484848483}}, {'text': 'Sequence\\nGeneration\\n', 'bbox': {'x1': 0.25252450980392155, 'y1': 0.2678539414141414, 'x2': 0.32485103235294116, 'y2': 0.29427010303030304}}, {'text': 'Year\\n', 'bbox': {'x1': 0.38060620915032684, 'y1': 0.11354619116161614, 'x2': 0.41314744673202614, 'y2': 0.12612523156565647}}, {'text': '2022\\n', 'bbox': {'x1': 0.38059803921568625, 'y1': 0.15564939595959598, 'x2': 0.4131555555555555, 'y2': 0.16822843636363646}}, {'text': '2021\\n', 'bbox': {'x1': 0.38059803921568625, 'y1': 0.1907440929292929, 'x2': 0.4131555555555555, 'y2': 0.20332313333333338}}, {'text': '2021\\n', 'bbox': {'x1': 0.38059803921568625, 'y1': 0.22584005252525255, 'x2': 0.4131555555555555, 'y2': 0.23841909292929303}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\nSATFormer\\n(Encoder /\\nDecoder)\\nTRSAT (Encoder\\n/ Decoder)\\nTransformer\\n(Encoder /\\nDecoder)\\n', 'bbox': {'x1': 0.4413790849673203, 'y1': 0.099709069949495, 'x2': 0.5548583081699346, 'y2': 0.252256214141414}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.5816094771241831, 'y1': 0.099709069949495, 'x2': 0.644901288888889, 'y2': 0.13996235277777763}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.6740081699346405, 'y1': 0.10662826186868693, 'x2': 0.7596181591503268, 'y2': 0.13304442348484843}}, {'text': 'No\\n', 'bbox': {'x1': 0.6033088235294118, 'y1': 0.15564939595959598, 'x2': 0.6232014660130719, 'y2': 0.16822843636363646}}, {'text': 'No\\n', 'bbox': {'x1': 0.6033088235294118, 'y1': 0.1907440929292929, 'x2': 0.6232014660130719, 'y2': 0.20332313333333338}}, {'text': 'No\\n', 'bbox': {'x1': 0.6033088235294118, 'y1': 0.22584005252525255, 'x2': 0.6232014660130719, 'y2': 0.23841909292929303}}, {'text': 'NA\\n', 'bbox': {'x1': 0.70534477124183, 'y1': 0.15564939595959598, 'x2': 0.728281541503268, 'y2': 0.16822843636363646}}, {'text': 'NA\\n', 'bbox': {'x1': 0.70534477124183, 'y1': 0.1907440929292929, 'x2': 0.728281541503268, 'y2': 0.20332313333333338}}, {'text': 'NA\\n', 'bbox': {'x1': 0.70534477124183, 'y1': 0.22584005252525255, 'x2': 0.728281541503268, 'y2': 0.23841909292929303}}, {'text': '2020\\n', 'bbox': {'x1': 0.38059803921568625, 'y1': 0.27477313333333336, 'x2': 0.4131555555555555, 'y2': 0.2873521737373738}}, {'text': 'GPT-f (Decoder)\\n', 'bbox': {'x1': 0.443047385620915, 'y1': 0.27477313333333336, 'x2': 0.5531894633986928, 'y2': 0.2873521737373738}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.6014133986928104, 'y1': 0.27477313333333336, 'x2': 0.6250989918300652, 'y2': 0.2873521737373738}}, {'text': 'CommonCrawl,\\nGithub,\\narXiv,\\nWebMath\\n', 'bbox': {'x1': 0.6704950980392157, 'y1': 0.2540168202020201, 'x2': 0.7747117078431373, 'y2': 0.30810722424242404}}, {'text': 'Dataset (Fine-\\ntuning, Train-\\ning, Testing)\\n', 'bbox': {'x1': 0.8037450980392157, 'y1': 0.099709069949495, 'x2': 0.9018408947712417, 'y2': 0.13996235277777763}}, {'text': 'Synthetic\\n', 'bbox': {'x1': 0.8220424836601307, 'y1': 0.15564939595959598, 'x2': 0.8835436320261437, 'y2': 0.16822843636363646}}, {'text': 'Synthetic, SATLIB\\n', 'bbox': {'x1': 0.7903888888888889, 'y1': 0.1907440929292929, 'x2': 0.9151981277777779, 'y2': 0.20332313333333338}}, {'text': 'Synthetic\\n', 'bbox': {'x1': 0.8220424836601307, 'y1': 0.22584005252525255, 'x2': 0.8835436320261437, 'y2': 0.23841909292929303}}, {'text': 'set.mm\\n', 'bbox': {'x1': 0.8290506535947713, 'y1': 0.27477313333333336, 'x2': 0.8765357911764706, 'y2': 0.2873521737373738}}]}\n",
      "{'type': 'Caption', 'bbox': [0.29130739099839154, 0.32090701016512785, 0.7101387293198529, 0.33553136652166193], 'properties': {'score': 0.7721155881881714, 'page_number': 23}, 'text_representation': 'Table 9: Transformer models for automated symbolic reasoning\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09111300300149357, 0.36304177024147727, 0.9117890481387868, 0.4600040227716619], 'properties': {'score': 0.700083315372467, 'page_number': 23}, 'text_representation': '• SATformer: The SAT-solving problem for boolean formulas was addressed by Shi et al. in 2022 (Shi et al., 2021) through\\nthe introduction of SATformer, a hierarchical transformer architecture that offers an end-to-end learning-based solution\\nfor solving the problem. Traditionally, in the context of SAT-solving, a boolean formula is transformed into its conjunctive\\nnormal form (CNF), which serves as an input for the SAT solver. The CNF formula is a conjunction of boolean variables and\\ntheir negations, known as literals, organized into clauses where each clause is a disjunction of these literals. For example,\\na CNF formula utilizing boolean variables would be represented as (A OR B) AND (NOT A OR C), where each clause (A\\nOR B) and (NOT A OR C) is made up of literals.\\nThe authors employ a graph neural network (GNN) to obtain the embeddings of the clauses in the CNF formula. SATformer\\nthen operates on these clause embeddings to capture the interdependencies among clauses, with the self-attention weight\\nbeing trained to be high when groups of clauses that could potentially lead to an unsatisfiable formula are attended together,\\nand low otherwise. Through this approach, SATformer efficiently learns the correlations between clauses, resulting in\\nimproved SAT prediction capabilities (Shi et al., 2022b).\\n'}\n",
      "{'type': 'Text', 'bbox': [0.10592303107766544, 0.46477971857244316, 0.912154541015625, 0.5343193470348011], 'properties': {'score': 0.8867453932762146, 'page_number': 23}}\n",
      "{'type': 'List-item', 'bbox': [0.09135866052964155, 0.5433347389914772, 0.9116763126148897, 0.5988398881392045], 'properties': {'score': 0.7632731795310974, 'page_number': 23}, 'text_representation': '• TRSAT: Another research endeavor conducted by Shi et al. in 2021 investigated a variant of the boolean SAT problem\\nknown as MaxSAT and introduced a transformer model named TRSAT, which serves as an end-to-end learning-based SAT\\nsolver (Shi et al., 2021). A comparable problem to the boolean SAT is the satisfiability of a linear temporal formula (Pnueli,\\n1977), where a satisfying symbolic trace to the formula is sought after given a linear temporal formula.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0908059512867647, 0.6081106289950284, 0.91158447265625, 0.733192138671875], 'properties': {'score': 0.7802622318267822, 'page_number': 23}, 'text_representation': '• Transformer: In a study conducted by Hahn et al. (2021), the authors addressed the boolean SAT problem and the temporal\\nsatisfiability problem, both of which are more complex than binary classification tasks that were tackled in previous studies.\\nIn these problems, the task is to generate a satisfying sequence assignment for a given formula, rather than simply classify-\\ning whether the formula is satisfied or not. The authors constructed their datasets by using classical solvers to generate linear\\ntemporal formulas with their corresponding satisfying symbolic traces, and boolean formulas with their corresponding sat-\\nisfying partial assignments. The authors employed a standard transformer architecture to solve the sequence-to-sequence\\ntask. The Transformer was able to generate satisfying traces, some of which were not observed during training, demon-\\nstrating its capability to solve the problem and not merely mimic the behavior of the classical solvers used in the dataset\\ngeneration process.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09103819903205423, 0.7418929221413352, 0.9122967888327206, 0.811786055131392], 'properties': {'score': 0.8098453283309937, 'page_number': 23}, 'text_representation': '• GPT-f: In their work, Polu & Sutskever (2020) presented GPT-F, an automated prover and proof assistant that utilizes a\\ndecoder-only transformers architecture similar to GPT-2 and GPT-3. GPT-F was trained on a dataset called set.mm, which\\ncontains approximately 38,000 proofs. The largest model investigated by the authors consists of 36 layers and 774 million\\ntrainable parameters. This deep learning network has generated novel proofs that have been accepted and incorporated into\\nmathematical proof libraries and communities.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09405372170840992, 0.8270968905362216, 0.2683856739717371, 0.8406819846413353], 'properties': {'score': 0.6061367988586426, 'page_number': 23}, 'text_representation': '6.2 COMPUTER VISION\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09398531745461856, 0.8484777277166193, 0.9119566434972426, 0.9462856223366477], 'properties': {'score': 0.9142974019050598, 'page_number': 23}, 'text_representation': 'Motivated by the success of transformers in natural language processing, researchers have explored the application of the\\ntransformer concept in computer vision tasks. Traditionally, convolutional neural networks (CNNs) have been considered\\nthe fundamental component for processing visual data. However, different types of images require different processing\\ntechniques, with natural images and medical images being two prime examples. Furthermore, research in computer vision for\\nnatural images and medical images is vast and distinct. As a result, transformer models for computer vision can be broadly\\nclassified into two categories: (i) those designed for natural image processing, and (ii) those designed for medical image\\nprocessing.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09419223561006433, 0.10117932406338778, 0.3606894459443934, 0.11470735723322088], 'properties': {'score': 0.5625494718551636, 'page_number': 24}}\n",
      "{'type': 'Text', 'bbox': [0.09375142714556525, 0.11843754161487927, 0.9125136431525736, 0.21544401689009232], 'properties': {'score': 0.9193177223205566, 'page_number': 24}, 'text_representation': '6.2.1 NATURAL IMAGE PROCESSING\\nIn the domain of computer vision, natural image processing is a primary focus as compared to medical image processing,\\nowing to the greater availability of natural image data. Furthermore, computer vision with natural images has wide-ranging\\napplications in various domains. Among the numerous tasks associated with computer vision and natural images, we have\\nidentified four of the most common and popular tasks: (i) classification and segmentation, (ii) recognition and feature extrac-\\ntion, (iii) mask modeling prediction, and (iv) image generation. In this context, we have provided a comprehensive discussion\\nof each of these computer vision tasks with natural images. Additionally, we have presented a table that provides crucial\\ninformation about each transformer-based model and have highlighted their working methods and significance.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09462078318876378, 0.22432435469193893, 0.31664378446691177, 0.23750851717862215], 'properties': {'score': 0.7612527012825012, 'page_number': 24}, 'text_representation': 'IMAGE CLASSIFICATION\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09408948112936581, 0.24175059925426137, 0.9128125718060662, 0.3386062899502841], 'properties': {'score': 0.9233543872833252, 'page_number': 24}, 'text_representation': '6.2.2\\nImage classification is a crucial and popular task in the field of computer vision, which aims to analyze and categorize images\\nbased on their features, type, genre, or objects. This task is considered as a primary stage for many other image processing\\ntasks. For example, if we have a set of images of different animals, we can classify them into different animal categories such\\nas cat, dog, horse, etc., based on their characteristics and features (Szummer & Picard, 1998, Lu & Weng, 2007). Due to its\\nsignificance, many transformer-based models have been developed to address image classification tasks. Table 10 highlights\\nsome of the significant transformer models for image classification tasks and discusses their important features and working\\nmethodologies.\\n'}\n",
      "{'type': 'table', 'bbox': [0.05179807326372932, 0.3609033203125, 0.9391725068933824, 0.7828064519708807], 'properties': {'score': 0.7644245028495789, 'title': None, 'columns': None, 'rows': None, 'page_number': 24}, 'text_representation': 'ViT Variants\\n(d’Ascoli\\net al., 2021,\\nAhmed\\net al., 2021,\\nTouvron\\net al., 2021,\\nArnab\\net al., 2021)\\n BEIT (Bao\\net al., 2022)\\n IBOT (Zhou\\net al., 2021b)\\n Conformer\\n(Peng et al.,\\n2021)\\n Transformer\\nModels\\n Task Ac-\\ncomplished\\n Year\\n Architecture\\n(Encoder/\\nDecoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n VIT (Doso-\\nvitskiy\\net al., 2021)\\n Image classifi-\\ncation, image\\nrecognition\\n 2021\\n Encoder\\n Yes\\n Image clas-\\nsification\\n 2020-\\n2021\\n Encoder\\n Yes\\n Pre-training\\nDataset\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n JFT-300M,\\nILSVRC-2012\\nImageNet,\\nImageNet-21k\\nConViT: ImageNet\\n(Based on DeiT)\\nSiT: STL10,\\nCUB200, CIFAR10,\\nCIFAR100,\\nImageNet-1K,\\nPascal VOC,\\nMS-COCO, Visual-\\nGenome DEIT:\\nImageNet. ViViT:\\nImageNet, JFT\\n ImageNet-RL, CIFAR-\\n10/100, Oxford Flowers-102,\\nOxford-IIIT Pets, VTAB\\n ConViT: ImageNet,CIFAR100\\nSiT: CIFAR-10,CIFAR-100 ,\\nSTL-10, CUB200, ImageNet-\\n1K, Pascal VOC, MS-COCO,\\nVisual-Genome. DEIT:\\nImageNet, iNaturalist 2018,\\niNaturalist 2019, Flowers-102,\\nStanford Cars, CIFAR-100,\\nCIFAR-10. ViViT: Larger JFT,\\nKinetics, Epic Kitchens-100,\\nMoments in Time, SSv2.\\n Image clas-\\nsification &\\nsegmentation\\nImage clas-\\nsification,\\nsegmentation,\\nobject detection\\n& recognition\\nImage recog-\\nnition & object\\ndetection,\\nClassification\\n 2021\\n Encoder\\n Yes\\n ImageNet-1K,\\nImageNet-22k\\n ILSVRC-2012 ImageNet,\\nADE20K, CIFAR-100\\n 2022\\n Encoder\\n Yes\\n ImageNet-1K, ViT-\\nL/16, ImageNet-22K\\n COCO, ADE20K\\n 2021\\n Encoder\\n Not\\nmentioned\\nin paper\\n N/A\\n LibriSpeech\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e038d990>, 'tokens': [{'text': 'ViT Variants\\n(d’Ascoli\\net al., 2021,\\nAhmed\\net al., 2021,\\nTouvron\\net al., 2021,\\nArnab\\net al., 2021)\\n', 'bbox': {'x1': 0.06922712418300653, 'y1': 0.4749347494949495, 'x2': 0.15189065816993463, 'y2': 0.5982107595959596}}, {'text': 'BEIT (Bao\\net al., 2022)\\n', 'bbox': {'x1': 0.0716764705882353, 'y1': 0.6207264161616162, 'x2': 0.14944009836601307, 'y2': 0.6471425777777776}}, {'text': 'IBOT (Zhou\\net al., 2021b)\\n', 'bbox': {'x1': 0.0676062091503268, 'y1': 0.676578688888889, 'x2': 0.15350921601307188, 'y2': 0.702993587878788}}, {'text': 'Conformer\\n(Peng et al.,\\n2021)\\n', 'bbox': {'x1': 0.0716764705882353, 'y1': 0.7324296989898991, 'x2': 0.14944009836601307, 'y2': 0.7726817191919193}}, {'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06615849673202615, 'y1': 0.37006386792929297, 'x2': 0.15495912254901958, 'y2': 0.3964800295454546}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.19561274509803922, 'y1': 0.37006386792929297, 'x2': 0.27430426209150327, 'y2': 0.3964800295454546}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.37698179722222225, 'x2': 0.3455820872549019, 'y2': 0.38956083762626265}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\n', 'bbox': {'x1': 0.3656486928104575, 'y1': 0.36314467601010103, 'x2': 0.4536516594771242, 'y2': 0.40339795883838386}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.4746160130718954, 'y1': 0.36314467601010103, 'x2': 0.5379078248366013, 'y2': 0.40339795883838386}}, {'text': 'VIT (Doso-\\nvitskiy\\net al., 2021)\\n', 'bbox': {'x1': 0.0716764705882353, 'y1': 0.41216581010101005, 'x2': 0.14944009836601307, 'y2': 0.4524190929292929}}, {'text': 'Image classifi-\\ncation, image\\nrecognition\\n', 'bbox': {'x1': 0.18726307189542485, 'y1': 0.41216581010101005, 'x2': 0.28265659477124183, 'y2': 0.4524190929292929}}, {'text': '2021\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.42600293131313133, 'x2': 0.3455983660130719, 'y2': 0.43858197171717167}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.3802156862745098, 'y1': 0.42600293131313133, 'x2': 0.434456508496732, 'y2': 0.43858197171717167}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.42600293131313133, 'x2': 0.5181038937908496, 'y2': 0.43858197171717167}}, {'text': 'Image clas-\\nsification\\n', 'bbox': {'x1': 0.1972173202614379, 'y1': 0.523365305050505, 'x2': 0.27270192189542486, 'y2': 0.5497814666666666}}, {'text': '2020-\\n2021\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.523365305050505, 'x2': 0.3510191924836601, 'y2': 0.5497814666666666}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.3802156862745098, 'y1': 0.5302832343434344, 'x2': 0.434456508496732, 'y2': 0.5428622747474747}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.5302832343434344, 'x2': 0.5181038937908496, 'y2': 0.5428622747474747}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.590173202614379, 'y1': 0.37006386792929297, 'x2': 0.6757831918300653, 'y2': 0.3964800295454546}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7508382352941176, 'y1': 0.37006386792929297, 'x2': 0.8982423905228758, 'y2': 0.3964800295454546}}, {'text': 'JFT-300M,\\nILSVRC-2012\\nImageNet,\\nImageNet-21k\\nConViT: ImageNet\\n(Based on DeiT)\\nSiT: STL10,\\nCUB200, CIFAR10,\\nCIFAR100,\\nImageNet-1K,\\nPascal VOC,\\nMS-COCO, Visual-\\nGenome DEIT:\\nImageNet. ViViT:\\nImageNet, JFT\\n', 'bbox': {'x1': 0.5664232026143791, 'y1': 0.40524661818181823, 'x2': 0.6995346081699347, 'y2': 0.6120478808080808}}, {'text': 'ImageNet-RL, CIFAR-\\n10/100, Oxford Flowers-102,\\nOxford-IIIT Pets, VTAB\\n', 'bbox': {'x1': 0.7293415032679739, 'y1': 0.41216581010101005, 'x2': 0.9197378588235293, 'y2': 0.4524190929292929}}, {'text': 'ConViT: ImageNet,CIFAR100\\nSiT: CIFAR-10,CIFAR-100 ,\\nSTL-10, CUB200, ImageNet-\\n1K, Pascal VOC, MS-COCO,\\nVisual-Genome. DEIT:\\nImageNet, iNaturalist 2018,\\niNaturalist 2019, Flowers-102,\\nStanford Cars, CIFAR-100,\\nCIFAR-10. ViViT: Larger JFT,\\nKinetics, Epic Kitchens-100,\\nMoments in Time, SSv2.\\n', 'bbox': {'x1': 0.7228382352941178, 'y1': 0.4610108376262626, 'x2': 0.9262416264705884, 'y2': 0.6120478808080808}}, {'text': 'Image clas-\\nsification &\\nsegmentation\\nImage clas-\\nsification,\\nsegmentation,\\nobject detection\\n& recognition\\nImage recog-\\nnition & object\\ndetection,\\nClassification\\n', 'bbox': {'x1': 0.18319281045751634, 'y1': 0.6138084868686868, 'x2': 0.2867257124183007, 'y2': 0.7796009111111112}}, {'text': '2021\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.6276456080808082, 'x2': 0.3455983660130719, 'y2': 0.6402246484848485}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.3802156862745098, 'y1': 0.6276456080808082, 'x2': 0.434456508496732, 'y2': 0.6402246484848485}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.6276456080808082, 'x2': 0.5181038937908496, 'y2': 0.6402246484848485}}, {'text': 'ImageNet-1K,\\nImageNet-22k\\n', 'bbox': {'x1': 0.5859656862745097, 'y1': 0.6207264161616162, 'x2': 0.6799917934640521, 'y2': 0.6471425777777776}}, {'text': 'ILSVRC-2012 ImageNet,\\nADE20K, CIFAR-100\\n', 'bbox': {'x1': 0.7404444444444445, 'y1': 0.6207264161616162, 'x2': 0.9086365738562091, 'y2': 0.6471425777777776}}, {'text': '2022\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.6834966181818182, 'x2': 0.3455983660130719, 'y2': 0.6960756585858586}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.3802156862745098, 'y1': 0.6834966181818182, 'x2': 0.434456508496732, 'y2': 0.6960756585858586}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.6834966181818182, 'x2': 0.5181038937908496, 'y2': 0.6960756585858586}}, {'text': 'ImageNet-1K, ViT-\\nL/16, ImageNet-22K\\n', 'bbox': {'x1': 0.5647140522875818, 'y1': 0.676578688888889, 'x2': 0.7012439970588236, 'y2': 0.702993587878788}}, {'text': 'COCO, ADE20K\\n', 'bbox': {'x1': 0.7671160130718955, 'y1': 0.6834966181818182, 'x2': 0.8819626519607844, 'y2': 0.6960756585858586}}, {'text': '2021\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.7462668202020202, 'x2': 0.3455983660130719, 'y2': 0.7588458606060605}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.3802156862745098, 'y1': 0.7462668202020202, 'x2': 0.434456508496732, 'y2': 0.7588458606060605}}, {'text': 'Not\\nmentioned\\nin paper\\n', 'bbox': {'x1': 0.4718970588235294, 'y1': 0.7324296989898991, 'x2': 0.5406259758169935, 'y2': 0.7726817191919193}}, {'text': 'N/A\\n', 'bbox': {'x1': 0.6189624183006536, 'y1': 0.7462668202020202, 'x2': 0.6469944398692811, 'y2': 0.7588458606060605}}, {'text': 'LibriSpeech\\n', 'bbox': {'x1': 0.7847549019607842, 'y1': 0.7462668202020202, 'x2': 0.8643254718954249, 'y2': 0.7588458606060605}}]}\n",
      "{'type': 'Text', 'bbox': [0.2357157808191636, 0.7932235995205966, 0.7680542710248162, 0.8076766690340909], 'properties': {'score': 0.5193321704864502, 'page_number': 24}, 'text_representation': 'Table 10: Transformer models for natural image processing - image classification\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09000846414005055, 0.8238377796519887, 0.9119840016084558, 0.9481528542258523], 'properties': {'score': 0.590587854385376, 'page_number': 24}, 'text_representation': '• ViT Variants: There are several ViT-based models that have been developed for specific tasks. For instance, ConViT is\\nan improved version of ViT that combines CNN and transformer by adding an inductive bias to ViT, resulting in better\\naccuracy for image classification tasks (d’Ascoli et al., 2021). Self-supervised Vision transformer (SiT) allows for the use\\nof the architecture as an autoencoder and seamlessly works with multiple self-supervised tasks (Ahmed et al., 2021). Data\\nEfficient Image Transformer (DeiT) is a type of vision transformer designed for image classification tasks that require less\\ndata to be trained (Touvron et al., 2021). There are numerous ViT variants available with certain improvements or designed\\nfor specific tasks. For example, Video Vision Transformer (ViViT) is a ViT-based model that classifies videos using both\\nencoder and decoder modules of the transformer, whereas most ViT and ViT-variant models use only the encoder module\\n(Arnab et al., 2021).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09157568538890165, 0.10020565379749645, 0.911729736328125, 0.1704855624112216], 'properties': {'score': 0.8265361785888672, 'page_number': 25}, 'text_representation': '• BEIT: Bidirectional Encoder Representation from Image Transformers (BEIT) (Bao et al., 2022) is a transformer-based\\nmodel that draws inspiration from BERT and introduces a new pre-training task called Masked Image Modeling (MIM) for\\nvision Transformers. In MIM, a portion of the image is randomly masked, and the corrupted image is passed through the ar-\\nchitecture, which then recovers the original image tokens. BEIT has shown competitive performance on image classification\\nand segmentation tasks, demonstrating its effectiveness for a variety of computer vision applications.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0915469539866728, 0.17442835027521306, 0.9115240837545956, 0.24485587380149149], 'properties': {'score': 0.8642559051513672, 'page_number': 25}, 'text_representation': '• Conformer: In the field of computer vision, Conformer (Peng et al., 2021) is a model that works similarly to the CMT\\nmodel. While CNN is responsible for capturing the local features of the image, Transformer works for the global context\\nand long-range dependencies of images. However, the Conformer model proposes a new method called cross-attention,\\nwhich combines both local and global features to focus on various parts of the image based on the task. The model has\\nshown promising results for classification and object detection/recognition tasks.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09180238611557905, 0.24921134255149147, 0.9110883645450367, 0.3050696078213778], 'properties': {'score': 0.8644627332687378, 'page_number': 25}, 'text_representation': '• IBOT: IBOT represents Image BERT Pre-training with Online Tokennizer which is a self-supervised model. This model\\nstudied masked image modeling using an online tokenizer and it learns to distill features using a tokenizer. This online\\ntokenizer helps this model to improve the feature representation capability. Besides, the image classification task, this\\nmodel shows significant performance in object detection and segmentation tasks.\\n'}\n",
      "{'type': 'Text', 'bbox': [0.10581225226907169, 0.3207598876953125, 0.9121093031939338, 0.34925717440518467], 'properties': {'score': 0.811903178691864, 'page_number': 25}, 'text_representation': 'ViT The ViT model, which has been discussed in detail in the Recognition and Object Detection section, is also capable of\\nperforming recognition and object detection tasks.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09435741648954504, 0.3576901245117188, 0.4631725715188419, 0.3717727938565341], 'properties': {'score': 0.6108388304710388, 'page_number': 25}, 'text_representation': 'IMAGE RECOGNITION & OBJECT DETECTION\\n'}\n",
      "{'type': 'Text', 'bbox': [0.093863426657284, 0.3755304509943182, 0.9126805922564338, 0.4591421231356534], 'properties': {'score': 0.8415523171424866, 'page_number': 25}, 'text_representation': '6.2.3\\nImage recognition & Object detection is often considered as nearly similar and related task in computer vision. It is the\\ncapability of detecting or recognizing any object, person, or feature in an image or video. An image or video contains a\\nnumber of objects & features; by extracting the features from the image, a model tries to capture the features of an object\\nthrough training. By understanding these useful features, a model can recognize the specific object from the other available\\nobject in the image or video (Zhao et al., 2019, Jiao et al., 2019, H´enaff, 2020, Chen et al., 2019a). Here we highlight and\\ndiscuss the significant transformer models for image/object recognition tasks (see Table 11).\\n'}\n",
      "{'type': 'table', 'bbox': [0.05197879566865809, 0.478115234375, 0.9392239918428309, 0.9159197443181818], 'properties': {'score': 0.8343179225921631, 'title': None, 'columns': None, 'rows': None, 'page_number': 25}, 'text_representation': 'Transformer\\nModels\\n Task Ac-\\ncomplished\\n Year\\n Architecture\\n(Encoder/\\nDecoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n VIT (Doso-\\nvitskiy\\net al., 2021)\\n Conformer\\n(Peng et al.,\\n2021)\\n LoFTR (Sun\\net al., 2021a)\\n CMT (Guo\\net al., 2022a)\\n Transformer\\nin\\nTransformer-\\nTNT (Han\\net al., 2021)\\nSWIN (Liu\\net al., 2021)\\nDETR\\n(Carion\\net al., 2020)\\n Image classifi-\\ncation, image\\nrecognition\\n Image recog-\\nnition & object\\ndetection,\\nclassification\\n Image feature\\nmatching\\n& visual\\nlocalization\\n Image recogni-\\ntion, detection\\n& segmentation\\n 2021\\n Encoder\\n Yes\\n 2021\\n Encoder\\n Not\\nmentioned\\nin paper\\n 2021\\n Encoder &\\nDecoder\\n No\\n 2022\\n Encoder\\n No\\n Image\\nrecognition\\n 2021\\n Encoder &\\nDecoder\\n Object detection\\nand segmentation\\n 2021\\n Encoder\\n Object detection\\n& prediction\\n 2020\\n Encoder &\\nDecoder\\n Yes\\n Yes\\n Yes\\n Pre-training\\nDataset\\n JFT-300M,\\nILSVRC-2012\\nImageNet,\\nImageNet-21k\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n ImageNet-RL, CIFAR-\\n10/100, Oxford Flowers-102,\\nOxford-IIIT Pets, VTAB\\n N/A\\n LibriSpeech\\n NA\\n NA\\n ImageNet\\nILSVRC 2012\\n ImageNet-22k\\n MegaDepth, ScanNet\\nHPatches, ScanNet,\\nMegaDepth, VisLoc\\nbenchmark (the Aachen-\\nDay-Night, InLoc)\\nImageNet, CIFAR10,\\nCIFAR100, Flowers, Stand-\\nford cars, Oxford-IIIT\\npets, COCO val2017\\n COCO2017, ADE20K,\\nOxford 102 Flowers, Oxford-\\nIIIT Pets, iNaturalist 2019,\\nCIFAR-10, CIFAR-100\\n ImageNet-1k, COCO\\n2017, ADE20K\\n ImageNet pretrained\\nbackbone ResNet-50\\n COCO 2017, panoptic\\nsegmentation datasets\\n Continued on next page\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e038f4f0>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06615849673202615, 'y1': 0.48815603964646465, 'x2': 0.15495912254901958, 'y2': 0.5145722012626263}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.19561274509803922, 'y1': 0.48815603964646465, 'x2': 0.27430426209150327, 'y2': 0.5145722012626263}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.4950739689393939, 'x2': 0.3455820872549019, 'y2': 0.5076530093434344}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\n', 'bbox': {'x1': 0.3656486928104575, 'y1': 0.4812368477272727, 'x2': 0.4536516594771242, 'y2': 0.5214901305555556}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.4746160130718954, 'y1': 0.4812368477272727, 'x2': 0.5379078248366013, 'y2': 0.5214901305555556}}, {'text': 'VIT (Doso-\\nvitskiy\\net al., 2021)\\n', 'bbox': {'x1': 0.0716764705882353, 'y1': 0.530760507070707, 'x2': 0.14944009836601307, 'y2': 0.5710137898989899}}, {'text': 'Conformer\\n(Peng et al.,\\n2021)\\n', 'bbox': {'x1': 0.0716764705882353, 'y1': 0.5866127797979798, 'x2': 0.14944009836601307, 'y2': 0.6268648}}, {'text': 'LoFTR (Sun\\net al., 2021a)\\n', 'bbox': {'x1': 0.06806209150326797, 'y1': 0.6563009111111111, 'x2': 0.1530534879084967, 'y2': 0.6827170727272727}}, {'text': 'CMT (Guo\\net al., 2022a)\\n', 'bbox': {'x1': 0.06806209150326797, 'y1': 0.7190698505050505, 'x2': 0.1530534879084967, 'y2': 0.7454860121212122}}, {'text': 'Transformer\\nin\\nTransformer-\\nTNT (Han\\net al., 2021)\\nSWIN (Liu\\net al., 2021)\\nDETR\\n(Carion\\net al., 2020)\\n', 'bbox': {'x1': 0.06761437908496733, 'y1': 0.7610837393939395, 'x2': 0.1535011071895425, 'y2': 0.8992031838383838}}, {'text': 'Image classifi-\\ncation, image\\nrecognition\\n', 'bbox': {'x1': 0.18726307189542485, 'y1': 0.530760507070707, 'x2': 0.28265659477124183, 'y2': 0.5710137898989899}}, {'text': 'Image recog-\\nnition & object\\ndetection,\\nclassification\\n', 'bbox': {'x1': 0.18566666666666667, 'y1': 0.5796935878787879, 'x2': 0.2842508261437909, 'y2': 0.6337839919191919}}, {'text': 'Image feature\\nmatching\\n& visual\\nlocalization\\n', 'bbox': {'x1': 0.18998856209150325, 'y1': 0.6424637898989899, 'x2': 0.27992870098039213, 'y2': 0.6965529313131312}}, {'text': 'Image recogni-\\ntion, detection\\n& segmentation\\n', 'bbox': {'x1': 0.18330718954248365, 'y1': 0.7121519212121212, 'x2': 0.28661218888888884, 'y2': 0.7524052040404041}}, {'text': '2021\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.5445976282828283, 'x2': 0.3455983660130719, 'y2': 0.5571766686868687}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.3802156862745098, 'y1': 0.5445976282828283, 'x2': 0.434456508496732, 'y2': 0.5571766686868687}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.5445976282828283, 'x2': 0.5181038937908496, 'y2': 0.5571766686868687}}, {'text': '2021\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.6004486383838383, 'x2': 0.3455983660130719, 'y2': 0.6130276787878788}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.3802156862745098, 'y1': 0.6004486383838383, 'x2': 0.434456508496732, 'y2': 0.6130276787878788}}, {'text': 'Not\\nmentioned\\nin paper\\n', 'bbox': {'x1': 0.4718970588235294, 'y1': 0.5866127797979798, 'x2': 0.5406259758169935, 'y2': 0.6268648}}, {'text': '2021\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.6632188404040404, 'x2': 0.3455983660130719, 'y2': 0.6757978808080808}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.37184803921568627, 'y1': 0.6563009111111111, 'x2': 0.44282342483660136, 'y2': 0.6827170727272727}}, {'text': 'No\\n', 'bbox': {'x1': 0.4963153594771242, 'y1': 0.6632188404040404, 'x2': 0.5162080019607842, 'y2': 0.6757978808080808}}, {'text': '2022\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.7259890424242423, 'x2': 0.3455983660130719, 'y2': 0.7385680828282829}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.3802156862745098, 'y1': 0.7259890424242423, 'x2': 0.434456508496732, 'y2': 0.7385680828282829}}, {'text': 'No\\n', 'bbox': {'x1': 0.4963153594771242, 'y1': 0.7259890424242423, 'x2': 0.5162080019607842, 'y2': 0.7385680828282829}}, {'text': 'Image\\nrecognition\\n', 'bbox': {'x1': 0.19788398692810458, 'y1': 0.7818400525252525, 'x2': 0.2720337303921569, 'y2': 0.8082562141414141}}, {'text': '2021\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.7887579818181819, 'x2': 0.3455983660130719, 'y2': 0.8013370222222223}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.37184803921568627, 'y1': 0.7818400525252525, 'x2': 0.44282342483660136, 'y2': 0.8082562141414141}}, {'text': 'Object detection\\nand segmentation\\n', 'bbox': {'x1': 0.17788562091503268, 'y1': 0.8307718707070707, 'x2': 0.2920322732026144, 'y2': 0.8571880323232324}}, {'text': '2021\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.8376910626262627, 'x2': 0.3455983660130719, 'y2': 0.850270103030303}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.3802156862745098, 'y1': 0.8376910626262627, 'x2': 0.434456508496732, 'y2': 0.850270103030303}}, {'text': 'Object detection\\n& prediction\\n', 'bbox': {'x1': 0.18138562091503269, 'y1': 0.8658678303030304, 'x2': 0.2885324071895425, 'y2': 0.8922839919191918}}, {'text': '2020\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.8727870222222222, 'x2': 0.3455983660130719, 'y2': 0.8853660626262627}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.37184803921568627, 'y1': 0.8658678303030304, 'x2': 0.44282342483660136, 'y2': 0.8922839919191918}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.7887579818181819, 'x2': 0.5181038937908496, 'y2': 0.8013370222222223}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.8376910626262627, 'x2': 0.5181038937908496, 'y2': 0.850270103030303}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.8727870222222222, 'x2': 0.5181038937908496, 'y2': 0.8853660626262627}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.590173202614379, 'y1': 0.48815603964646465, 'x2': 0.6757831918300653, 'y2': 0.5145722012626263}}, {'text': 'JFT-300M,\\nILSVRC-2012\\nImageNet,\\nImageNet-21k\\n', 'bbox': {'x1': 0.585045751633987, 'y1': 0.5238425777777779, 'x2': 0.6809113584967319, 'y2': 0.5779329818181819}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7508382352941176, 'y1': 0.48815603964646465, 'x2': 0.8982423905228758, 'y2': 0.5145722012626263}}, {'text': 'ImageNet-RL, CIFAR-\\n10/100, Oxford Flowers-102,\\nOxford-IIIT Pets, VTAB\\n', 'bbox': {'x1': 0.7293415032679739, 'y1': 0.530760507070707, 'x2': 0.9197378588235293, 'y2': 0.5710137898989899}}, {'text': 'N/A\\n', 'bbox': {'x1': 0.6189624183006536, 'y1': 0.6004486383838383, 'x2': 0.6469944398692811, 'y2': 0.6130276787878788}}, {'text': 'LibriSpeech\\n', 'bbox': {'x1': 0.7847549019607842, 'y1': 0.6004486383838383, 'x2': 0.8643254718954249, 'y2': 0.6130276787878788}}, {'text': 'NA\\n', 'bbox': {'x1': 0.6215098039215686, 'y1': 0.6632188404040404, 'x2': 0.6444465741830064, 'y2': 0.6757978808080808}}, {'text': 'NA\\n', 'bbox': {'x1': 0.6215098039215686, 'y1': 0.7259890424242423, 'x2': 0.6444465741830064, 'y2': 0.7385680828282829}}, {'text': 'ImageNet\\nILSVRC 2012\\n', 'bbox': {'x1': 0.5857205882352942, 'y1': 0.7818400525252525, 'x2': 0.6802350581699346, 'y2': 0.8082562141414141}}, {'text': 'ImageNet-22k\\n', 'bbox': {'x1': 0.5859656862745097, 'y1': 0.8376910626262627, 'x2': 0.6799917934640521, 'y2': 0.850270103030303}}, {'text': 'MegaDepth, ScanNet\\nHPatches, ScanNet,\\nMegaDepth, VisLoc\\nbenchmark (the Aachen-\\nDay-Night, InLoc)\\nImageNet, CIFAR10,\\nCIFAR100, Flowers, Stand-\\nford cars, Oxford-IIIT\\npets, COCO val2017\\n', 'bbox': {'x1': 0.7335490196078431, 'y1': 0.635544597979798, 'x2': 0.9155292571895424, 'y2': 0.7593231333333333}}, {'text': 'COCO2017, ADE20K,\\nOxford 102 Flowers, Oxford-\\nIIIT Pets, iNaturalist 2019,\\nCIFAR-10, CIFAR-100\\n', 'bbox': {'x1': 0.7284460784313725, 'y1': 0.7680029313131314, 'x2': 0.9206330973856208, 'y2': 0.8220933353535355}}, {'text': 'ImageNet-1k, COCO\\n2017, ADE20K\\n', 'bbox': {'x1': 0.7549150326797386, 'y1': 0.8307718707070707, 'x2': 0.8941635300653594, 'y2': 0.8571880323232324}}, {'text': 'ImageNet pretrained\\nbackbone ResNet-50\\n', 'bbox': {'x1': 0.5649411764705883, 'y1': 0.8658678303030304, 'x2': 0.7010153160130719, 'y2': 0.8922839919191918}}, {'text': 'COCO 2017, panoptic\\nsegmentation datasets\\n', 'bbox': {'x1': 0.7515130718954248, 'y1': 0.8658678303030304, 'x2': 0.8975660901960785, 'y2': 0.8922839919191918}}, {'text': 'Continued on next page\\n', 'bbox': {'x1': 0.7715947712418301, 'y1': 0.9014663151515152, 'x2': 0.9264383189542483, 'y2': 0.9140453555555555}}]}\n",
      "{'type': 'Text', 'bbox': [0.7711023667279412, 0.9025381747159091, 0.9309763470818014, 0.9143927001953125], 'properties': {'score': 0.39629554748535156, 'page_number': 25}, 'text_representation': 'Continued on next page\\n'}\n",
      "{'type': 'table', 'bbox': [0.052101821899414064, 0.10008298006924717, 0.939652099609375, 0.1986035849831321], 'properties': {'score': 0.7728704810142517, 'title': None, 'columns': None, 'rows': None, 'page_number': 26}, 'text_representation': 'Transformer\\nModels\\n Task Ac-\\ncomplished\\n HOTR (Kim\\net al., 2021a)\\n Human-object\\ninteraction\\ndetection\\n Year\\n 2021\\n Architecture\\n(Encoder/\\nDecoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training\\nDataset\\n Dataset(Fine-tuning,\\nTraining, Testing)\\n Encoder &\\nDecoder\\n Yes\\n MS-COCO\\n V-COCO HICO-DET\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e03cd510>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06615849673202615, 'y1': 0.12046538308080808, 'x2': 0.15495912254901958, 'y2': 0.14688154469696957}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.19561274509803922, 'y1': 0.12046538308080808, 'x2': 0.27430426209150327, 'y2': 0.14688154469696957}}, {'text': 'HOTR (Kim\\net al., 2021a)\\n', 'bbox': {'x1': 0.06806209150326797, 'y1': 0.16256732525252518, 'x2': 0.1530534879084967, 'y2': 0.1889834868686868}}, {'text': 'Human-object\\ninteraction\\ndetection\\n', 'bbox': {'x1': 0.1883937908496732, 'y1': 0.15564939595959598, 'x2': 0.2815245663398693, 'y2': 0.19590141616161616}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.12738331237373743, 'x2': 0.3455820872549019, 'y2': 0.13996235277777777}}, {'text': '2021\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.16948525454545452, 'x2': 0.3455983660130719, 'y2': 0.182064294949495}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\n', 'bbox': {'x1': 0.3656486928104575, 'y1': 0.11354619116161614, 'x2': 0.4536516594771242, 'y2': 0.15379947398989877}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.4746160130718954, 'y1': 0.11354619116161614, 'x2': 0.5379078248366013, 'y2': 0.15379947398989877}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.590173202614379, 'y1': 0.12046538308080808, 'x2': 0.6757831918300653, 'y2': 0.14688154469696957}}, {'text': 'Dataset(Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7528725490196078, 'y1': 0.12046538308080808, 'x2': 0.8962070147058822, 'y2': 0.14688154469696957}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.37184803921568627, 'y1': 0.16256732525252518, 'x2': 0.44282342483660136, 'y2': 0.1889834868686868}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.16948525454545452, 'x2': 0.5181038937908496, 'y2': 0.182064294949495}}, {'text': 'MS-COCO\\n', 'bbox': {'x1': 0.5958954248366013, 'y1': 0.16948525454545452, 'x2': 0.6700614470588235, 'y2': 0.182064294949495}}, {'text': 'V-COCO HICO-DET\\n', 'bbox': {'x1': 0.7536944444444444, 'y1': 0.16948525454545452, 'x2': 0.8953847555555556, 'y2': 0.182064294949495}}]}\n",
      "{'type': 'Text', 'bbox': [0.17817151237936582, 0.20859448519620027, 0.8253494801240809, 0.22350846724076703], 'properties': {'score': 0.5159993171691895, 'page_number': 26}, 'text_representation': 'Table 11: Transformer models for natural image processing - image recognition & object detection\\n'}\n",
      "{'type': 'Caption', 'bbox': [0.17817151237936582, 0.20859448519620027, 0.8253494801240809, 0.22350846724076703], 'properties': {'score': 0.4262627065181732, 'page_number': 26}, 'text_representation': 'Table 11: Transformer models for natural image processing - image recognition & object detection\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0908651912913603, 0.2420512528852983, 0.9113144100413603, 0.2978963123668324], 'properties': {'score': 0.7532585263252258, 'page_number': 26}, 'text_representation': '• ViT: The ViT (Vision Transformer) is one of the earliest transformer-based models that has been applied to computer\\nvision. ViT views an image as a sequence of patches and processes it using only the encoder module of the Transformer.\\nViT performs very well for classification tasks and can also be applied to image recognition tasks. It demonstrates that a\\ntransformer-based model can serve as an alternative to convolutional neural networks (Dosovitskiy et al., 2021).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09142586203182444, 0.3022359397194602, 0.91122314453125, 0.37177828702059657], 'properties': {'score': 0.7819698452949524, 'page_number': 26}, 'text_representation': '• TNT: Transformer in Transformers (TNT) is a transformer-based computer vision model that uses a transformer model\\ninside another transformer model to capture features inside local patches of an image (Han et al., 2021). The image is\\ndivided into local patches, which are further divided into smaller patches to capture more detailed information through\\nattention mechanisms. TNT shows promising results in visual recognition tasks and offers an alternative to convolutional\\nneural networks for computer vision tasks.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09143537633559283, 0.3765630548650568, 0.9122032255284926, 0.4461489590731534], 'properties': {'score': 0.8161656260490417, 'page_number': 26}, 'text_representation': '• LoFTR: LoFTR, which stands for Local Feature Matching with Transformer, is a computer vision model that is capable of\\nlearning feature representations directly from raw images, as opposed to relying on hand-crafted feature detectors for feature\\nmatching. This model employs both the encoder and decoder modules of the transformer. The encoder takes features from\\nthe image, while the decoder works to create a feature map. By leveraging the transformer’s ability to capture global context\\nand long-range dependencies, LoFTR can achieve high performance in visual recognition tasks.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09164083144244026, 0.4508553799715909, 0.9124724982766544, 0.5208666437322443], 'properties': {'score': 0.816334068775177, 'page_number': 26}, 'text_representation': '• DETR: The Detection Transformer (DETR) represents a new approach to object detection or recognition, which performs\\nthe object detection task as a direct set of prediction problems (Carion et al., 2020). In contrast, other models accomplish this\\ntask in two stages. DETR uses an encoder to generate object queries, a self-attention mechanism to capture the relationship\\nbetween the queries and objects in the image, and creates an object detection scheme. This model has been shown to be\\neffective for object detection and recognition tasks and represents a significant advancement in the field.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09157645730411305, 0.5249853515625, 0.9116812672334559, 0.5948968505859376], 'properties': {'score': 0.8082870841026306, 'page_number': 26}, 'text_representation': '• HOTR: The HOTR model, which stands for Human-Object Interaction Transformer, is a Transformer-based model\\nIt is the first Transformer-based Human-Object Interaction (HOI)\\ndesigned for predicting Human-Object Interaction.\\ndetection prediction model that employs both the encoder and decoder modules of the Transformer. Unlike conventional\\nhand-crafted post-processing schemes, HOTR uses a prediction set to extract the semantic relationship of the image,\\nmaking it one of the fastest human-object interaction detection models available (Kim et al., 2021a).\\n'}\n",
      "{'type': 'Text', 'bbox': [0.1054069429285386, 0.6107134454900568, 0.9114461741727942, 0.6525943825461648], 'properties': {'score': 0.8687636256217957, 'page_number': 26}, 'text_representation': 'CMT, Conformer & SWIN Transformer The CMT and SWIN Transformer model have already been described in the\\nImage Segmentation and Image Classification sections, respectively. Both of these models are also capable of performing\\nthe task of Image Segmentation. Additionally, the Conformer model was described in the Image Classification section.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09450290455537684, 0.6615926846590909, 0.31317971622242646, 0.6757566139914772], 'properties': {'score': 0.5356823801994324, 'page_number': 26}, 'text_representation': 'IMAGE SEGMENTATION\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09403734992532169, 0.678851318359375, 0.9120823759191177, 0.7623536265980113], 'properties': {'score': 0.8897439241409302, 'page_number': 26}, 'text_representation': '6.2.4\\nSegmentation is the process of partitioning an image based on objects and creating boundaries between them, requiring pixel-\\nlevel information extraction. There are two popular types of image segmentation tasks in computer vision: (i) Semantic\\nSegmentation, which aims to identify and color similar objects belonging to the same class among all other objects in an\\nimage, and (ii) Instance Segmentation, which aims to detect instances of objects and their boundaries (Minaee et al., 2022,\\nHaralick & Shapiro, 1985). In this section, we will discuss some Transformer-based models that have shown exceptional\\nperformance in image segmentation tasks (refer to Table 12 for more details).\\n'}\n",
      "{'type': 'table', 'bbox': [0.05113995271570542, 0.7817226895419034, 0.9391717888327206, 0.9266501686789773], 'properties': {'score': 0.7891844511032104, 'title': None, 'columns': None, 'rows': None, 'page_number': 26}, 'text_representation': 'Transformer\\nModels\\n Task Ac-\\ncomplished\\n Year\\n Architecture\\n(Encoder/\\nDecoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n SWIN (Liu\\net al., 2021)\\n Object detection\\nand segmentation\\n 2021\\n Encoder\\n CMT (Guo\\net al., 2022a)\\n Image recogni-\\ntion, detection\\n& segmentation\\n 2022\\n Encoder\\n Yes\\n No\\n Pre-training\\nDataset\\n ImageNet-22k\\n NA\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n ImageNet-1k, COCO\\n2017, ADE20K\\nImageNet, CIFAR10,\\nCIFAR100, Flowers, Stand-\\nford cars, Oxford-IIIT\\npets, COCO val2017\\n Continued on next page\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e03ce170>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06615849673202615, 'y1': 0.7915638679292929, 'x2': 0.15495912254901958, 'y2': 0.8179800295454546}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.19561274509803922, 'y1': 0.7915638679292929, 'x2': 0.27430426209150327, 'y2': 0.8179800295454546}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.7984817972222222, 'x2': 0.3455820872549019, 'y2': 0.8110608376262626}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\n', 'bbox': {'x1': 0.3656486928104575, 'y1': 0.784644676010101, 'x2': 0.4536516594771242, 'y2': 0.8248979588383838}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.4746160130718954, 'y1': 0.784644676010101, 'x2': 0.5379078248366013, 'y2': 0.8248979588383838}}, {'text': 'SWIN (Liu\\net al., 2021)\\n', 'bbox': {'x1': 0.0716764705882353, 'y1': 0.8267466181818182, 'x2': 0.14944009836601307, 'y2': 0.8531627797979798}}, {'text': 'Object detection\\nand segmentation\\n', 'bbox': {'x1': 0.17788562091503268, 'y1': 0.8267466181818182, 'x2': 0.2920322732026144, 'y2': 0.8531627797979798}}, {'text': '2021\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.83366581010101, 'x2': 0.3455983660130719, 'y2': 0.8462448505050505}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.3802156862745098, 'y1': 0.83366581010101, 'x2': 0.434456508496732, 'y2': 0.8462448505050505}}, {'text': 'CMT (Guo\\net al., 2022a)\\n', 'bbox': {'x1': 0.06806209150326797, 'y1': 0.8687617696969697, 'x2': 0.1530534879084967, 'y2': 0.8951766686868687}}, {'text': 'Image recogni-\\ntion, detection\\n& segmentation\\n', 'bbox': {'x1': 0.18330718954248365, 'y1': 0.8618425777777777, 'x2': 0.28661218888888884, 'y2': 0.9020958606060606}}, {'text': '2022\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.8756796989898991, 'x2': 0.3455983660130719, 'y2': 0.8882587393939394}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.3802156862745098, 'y1': 0.8756796989898991, 'x2': 0.434456508496732, 'y2': 0.8882587393939394}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.83366581010101, 'x2': 0.5181038937908496, 'y2': 0.8462448505050505}}, {'text': 'No\\n', 'bbox': {'x1': 0.4963153594771242, 'y1': 0.8756796989898991, 'x2': 0.5162080019607842, 'y2': 0.8882587393939394}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.590173202614379, 'y1': 0.7915638679292929, 'x2': 0.6757831918300653, 'y2': 0.8179800295454546}}, {'text': 'ImageNet-22k\\n', 'bbox': {'x1': 0.5859656862745097, 'y1': 0.83366581010101, 'x2': 0.6799917934640521, 'y2': 0.8462448505050505}}, {'text': 'NA\\n', 'bbox': {'x1': 0.6215098039215686, 'y1': 0.8756796989898991, 'x2': 0.6444465741830064, 'y2': 0.8882587393939394}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7508382352941176, 'y1': 0.7915638679292929, 'x2': 0.8982423905228758, 'y2': 0.8179800295454546}}, {'text': 'ImageNet-1k, COCO\\n2017, ADE20K\\nImageNet, CIFAR10,\\nCIFAR100, Flowers, Stand-\\nford cars, Oxford-IIIT\\npets, COCO val2017\\n', 'bbox': {'x1': 0.7335490196078431, 'y1': 0.8267466181818182, 'x2': 0.9155292571895424, 'y2': 0.9090137898989898}}, {'text': 'Continued on next page\\n', 'bbox': {'x1': 0.7715947712418301, 'y1': 0.9112781838383838, 'x2': 0.9264383189542483, 'y2': 0.9238572242424243}}]}\n",
      "{'type': 'table', 'bbox': [0.05239358340992647, 0.10098058527166193, 0.9393688246783088, 0.2823214444247159], 'properties': {'score': 0.7322016954421997, 'title': None, 'columns': None, 'rows': None, 'page_number': 27}, 'text_representation': 'Transformer\\nModels\\n Task Ac-\\ncomplished\\n Year\\n Architecture\\n(Encoder/\\nDecoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training\\nDataset\\n Dataset(Fine-tuning,\\nTraining, Testing)\\n SETR (Zheng\\net al., 2021)\\n Image seg-\\nmentation\\n 2020\\n Encoder &\\nDecoder\\n Yes\\n IBOT (Zhou\\net al., 2021b)\\n Image clas-\\nsification,\\nsegmentation,\\nobject detection\\n& recognition\\n 2022\\n Encoder\\n Yes\\n ImageNet-1k,\\npre-trained weights\\nprovided by\\nViT or DeiT\\n ImageNet-1K, ViT-\\nL/16 ImageNet-22K\\n ADE20K, Pascal\\nContext, CityScapes\\n COCO, ADE20K\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e03cee00>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06615849673202615, 'y1': 0.12046538308080808, 'x2': 0.15495912254901958, 'y2': 0.14688154469696957}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.19561274509803922, 'y1': 0.12046538308080808, 'x2': 0.27430426209150327, 'y2': 0.14688154469696957}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.12738331237373743, 'x2': 0.3455820872549019, 'y2': 0.13996235277777777}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\n', 'bbox': {'x1': 0.3656486928104575, 'y1': 0.11354619116161614, 'x2': 0.4536516594771242, 'y2': 0.15379947398989877}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.4746160130718954, 'y1': 0.11354619116161614, 'x2': 0.5379078248366013, 'y2': 0.15379947398989877}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.590173202614379, 'y1': 0.12046538308080808, 'x2': 0.6757831918300653, 'y2': 0.14688154469696957}}, {'text': 'Dataset(Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7528725490196078, 'y1': 0.12046538308080808, 'x2': 0.8962070147058822, 'y2': 0.14688154469696957}}, {'text': 'SETR (Zheng\\net al., 2021)\\n', 'bbox': {'x1': 0.06511601307189542, 'y1': 0.16948525454545452, 'x2': 0.15600031993464053, 'y2': 0.19590141616161616}}, {'text': 'Image seg-\\nmentation\\n', 'bbox': {'x1': 0.1991454248366013, 'y1': 0.16948525454545452, 'x2': 0.2707719607843137, 'y2': 0.19590141616161616}}, {'text': '2020\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.17640444646464645, 'x2': 0.3455983660130719, 'y2': 0.18898348686868693}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.37184803921568627, 'y1': 0.16948525454545452, 'x2': 0.44282342483660136, 'y2': 0.19590141616161616}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.17640444646464645, 'x2': 0.5181038937908496, 'y2': 0.18898348686868693}}, {'text': 'IBOT (Zhou\\net al., 2021b)\\n', 'bbox': {'x1': 0.0676062091503268, 'y1': 0.23225545656565647, 'x2': 0.15350921601307188, 'y2': 0.2586716181818181}}, {'text': 'Image clas-\\nsification,\\nsegmentation,\\nobject detection\\n& recognition\\n', 'bbox': {'x1': 0.18319281045751634, 'y1': 0.21150040606060597, 'x2': 0.2867257124183007, 'y2': 0.27942666868686844}}, {'text': '2022\\n', 'bbox': {'x1': 0.3130408496732026, 'y1': 0.2391746484848484, 'x2': 0.3455983660130719, 'y2': 0.2517536888888889}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.3802156862745098, 'y1': 0.2391746484848484, 'x2': 0.434456508496732, 'y2': 0.2517536888888889}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.49441830065359477, 'y1': 0.2391746484848484, 'x2': 0.5181038937908496, 'y2': 0.2517536888888889}}, {'text': 'ImageNet-1k,\\npre-trained weights\\nprovided by\\nViT or DeiT\\n', 'bbox': {'x1': 0.5699150326797385, 'y1': 0.15564939595959598, 'x2': 0.6960428509803922, 'y2': 0.2097385373737373}}, {'text': 'ImageNet-1K, ViT-\\nL/16 ImageNet-22K\\n', 'bbox': {'x1': 0.5667483660130719, 'y1': 0.23225545656565647, 'x2': 0.69920862124183, 'y2': 0.2586716181818181}}, {'text': 'ADE20K, Pascal\\nContext, CityScapes\\n', 'bbox': {'x1': 0.758187908496732, 'y1': 0.16948525454545452, 'x2': 0.8908923450980392, 'y2': 0.19590141616161616}}, {'text': 'COCO, ADE20K\\n', 'bbox': {'x1': 0.7671160130718955, 'y1': 0.2391746484848484, 'x2': 0.8819626519607844, 'y2': 0.2517536888888889}}]}\n",
      "{'type': 'Caption', 'bbox': [0.23406732895795038, 0.29160003662109374, 0.7693806008731617, 0.30676652388139203], 'properties': {'score': 0.5170466303825378, 'page_number': 27}, 'text_representation': 'Table 12: Transformer models for natural image processing - image segmentation\\n'}\n",
      "{'type': 'Text', 'bbox': [0.23406732895795038, 0.29160003662109374, 0.7693806008731617, 0.30676652388139203], 'properties': {'score': 0.4218122065067291, 'page_number': 27}, 'text_representation': 'Table 12: Transformer models for natural image processing - image segmentation\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09102540857651654, 0.32884355024857953, 0.9117457490808824, 0.3992455777254972], 'properties': {'score': 0.7688806653022766, 'page_number': 27}, 'text_representation': '• SWIN Transformer: The SWIN Transformer (Liu et al., 2021), short for Scaled WINdowed Transformer, is a transformer-\\nbased model that is capable of handling large images by dividing them into small patches, or windows, and processing\\nthem through its architecture. By using shifted windows, the model requires a smaller number of parameters and less\\ncomputational power, making it useful for real-life image applications. SWIN Transformer can perform image classification,\\nsegmentation, and object detection tasks with exceptional accuracy and efficiency (Zidan et al., 2023, Yang & Yang, 2023).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09105390660903033, 0.40494157270951703, 0.9116249712775736, 0.4753179376775568], 'properties': {'score': 0.8106429576873779, 'page_number': 27}, 'text_representation': '• CMT: CNNs Meet Transformer is a model that combines both Convolutional Neural Networks (CNN) and Vision Trans-\\nformer (ViT). CNNs are better suited to capturing local features, while Transformers excel at capturing global context.\\nCMT takes advantage of the strengths of both these models and performs well in image classification tasks as well as object\\ndetection and recognition tasks. The integration of CNN and Transformer allows CMT to handle both spatial and sequential\\ndata effectively, making it a powerful tool for computer vision tasks (Guo et al., 2022a).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09158787446863512, 0.4809925426136364, 0.9135700539981617, 0.5517158647017045], 'properties': {'score': 0.7796562314033508, 'page_number': 27}, 'text_representation': '• SETR: SETR stands for SEgmentation TRansformer, which is a transformer-based model used for image segmentation\\ntasks. It uses sequence-to-sequence prediction methods and removes the dependency of fully convolutional network with\\nvanilla Transformer architecture. Before feeding the image into the Transformer architecture, it divides the image into a\\nsequence of patches and the flattened pixel of each patch. There are three variants of SETR models available with different\\nmodel sizes and performance levels (Zheng et al., 2021).\\nIBOT The IBOT model, described above in the Image Classification section, is also capable of performing the Image\\nClassification task.\\n'}\n",
      "{'type': 'Text', 'bbox': [0.10509735107421875, 0.5537645374644886, 0.9118221507352942, 0.582442793412642], 'properties': {'score': 0.7334215044975281, 'page_number': 27}}\n",
      "{'type': 'Section-header', 'bbox': [0.09449159509995404, 0.5927173406427557, 0.2969339168772978, 0.6072373823686079], 'properties': {'score': 0.7427492141723633, 'page_number': 27}, 'text_representation': 'IMAGE GENERATION\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09348106833065258, 0.6107339754971591, 0.9132405359604779, 0.6954468328302557], 'properties': {'score': 0.8769315481185913, 'page_number': 27}, 'text_representation': '6.2.5\\nImage generation is a challenging task in computer vision, and transformer-based models have shown promising results in\\nthis area due to their parallel computational capability. This task involves generating new images using existing image pixels\\nas input. It can be used for object reconstruction and data augmentation (van den Oord et al., 2016, Liu et al., 2017). While\\nseveral text-to-image generation models exist, we focus on image generation models that use image pixels without any other\\ntype of data. In Table 13, we discuss some transformer-based models that have demonstrated exceptional performance in\\nimage generation tasks.\\n'}\n",
      "{'type': 'table', 'bbox': [0.05142360911649816, 0.719520097212358, 0.9395647116268382, 0.9204966042258522], 'properties': {'score': 0.765099823474884, 'title': None, 'columns': None, 'rows': None, 'page_number': 27}, 'text_representation': 'Transformer\\nModels\\n Image Transformer\\n(Parmar et al., 2018)\\n Task Ac-\\ncomplished\\n Image Gen-\\neration\\n Year\\n 2018\\n Architecture\\n(Encoder/\\nDecoder)\\nEncoder &\\nDecoder\\n Pre-\\ntrained\\n(Yes/NO)\\nNot\\nmentioned\\n Pre-training\\nDataset\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n N/A\\n ImageNet, CIFAR-10, CelebA\\n I-GPT (Chen\\net al., 2020b)\\n Image Gen-\\neration\\n 2020\\n Decoder\\n Yes\\n BooksCorpus\\ndataset,\\n1B Word\\nBenchmark\\n VideoGPT (Yan\\net al., 2021)\\n Video Generation\\n 2021\\n Decoder\\n No\\n NA\\n SNLI, MultiNLI, Question\\nNLI, RTE, SciTail, RACE,\\nStory Cloze, MSR Paraphrase\\nCorpus, Quora Question Pairs,\\nSTS Benchmark, Stanford\\nSentiment Treebank-2, CoLA\\nBAIR RobotNet, Mov-\\ning MNIST, ViZDoom,\\nUCF-101, Tumblr GIF\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e03cfd90>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.08977941176470589, 'y1': 0.7294325547979799, 'x2': 0.17858003758169935, 'y2': 0.7558487164141414}}, {'text': 'Image Transformer\\n(Parmar et al., 2018)\\n', 'bbox': {'x1': 0.06716830065359478, 'y1': 0.7646165676767677, 'x2': 0.2011913166666667, 'y2': 0.7910327292929293}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.2423937908496732, 'y1': 0.7294325547979799, 'x2': 0.32108530784313727, 'y2': 0.7558487164141414}}, {'text': 'Image Gen-\\neration\\n', 'bbox': {'x1': 0.24309313725490195, 'y1': 0.7646165676767677, 'x2': 0.32038468104575163, 'y2': 0.7910327292929293}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3598218954248366, 'y1': 0.7363517467171717, 'x2': 0.39236313300653597, 'y2': 0.7489307871212121}}, {'text': '2018\\n', 'bbox': {'x1': 0.3598218954248366, 'y1': 0.7715357595959597, 'x2': 0.39237941176470587, 'y2': 0.7841148}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\nEncoder &\\nDecoder\\n', 'bbox': {'x1': 0.41242973856209153, 'y1': 0.7225146255050505, 'x2': 0.5004327052287582, 'y2': 0.7910327292929293}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\nNot\\nmentioned\\n', 'bbox': {'x1': 0.5186781045751634, 'y1': 0.7225146255050505, 'x2': 0.5874070215686273, 'y2': 0.7910327292929293}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.6137957516339869, 'y1': 0.7294325547979799, 'x2': 0.6994057408496732, 'y2': 0.7558487164141414}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7513006535947713, 'y1': 0.7294325547979799, 'x2': 0.8987048088235294, 'y2': 0.7558487164141414}}, {'text': 'N/A\\n', 'bbox': {'x1': 0.6425849673202615, 'y1': 0.7715357595959597, 'x2': 0.6706169888888889, 'y2': 0.7841148}}, {'text': 'ImageNet, CIFAR-10, CelebA\\n', 'bbox': {'x1': 0.7256862745098039, 'y1': 0.7715357595959597, 'x2': 0.9243196816993466, 'y2': 0.7841148}}, {'text': 'I-GPT (Chen\\net al., 2020b)\\n', 'bbox': {'x1': 0.09122875816993463, 'y1': 0.8204675777777777, 'x2': 0.17713176503267972, 'y2': 0.8468837393939393}}, {'text': 'Image Gen-\\neration\\n', 'bbox': {'x1': 0.24309313725490195, 'y1': 0.8204675777777777, 'x2': 0.32038468104575163, 'y2': 0.8468837393939393}}, {'text': '2020\\n', 'bbox': {'x1': 0.3598218954248366, 'y1': 0.8273867696969697, 'x2': 0.39237941176470587, 'y2': 0.8399658101010101}}, {'text': 'Decoder\\n', 'bbox': {'x1': 0.42654901960784314, 'y1': 0.8273867696969697, 'x2': 0.4816851735294118, 'y2': 0.8399658101010101}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5411993464052287, 'y1': 0.8273867696969697, 'x2': 0.5648849395424835, 'y2': 0.8399658101010101}}, {'text': 'BooksCorpus\\ndataset,\\n1B Word\\nBenchmark\\n', 'bbox': {'x1': 0.612281045751634, 'y1': 0.8066304565656566, 'x2': 0.7009188839869281, 'y2': 0.8607208606060607}}, {'text': 'VideoGPT (Yan\\net al., 2021)\\n', 'bbox': {'x1': 0.08190849673202615, 'y1': 0.8832377797979798, 'x2': 0.1864506816993464, 'y2': 0.9096539414141415}}, {'text': 'Video Generation\\n', 'bbox': {'x1': 0.2241372549019608, 'y1': 0.890155709090909, 'x2': 0.33934202647058825, 'y2': 0.9027347494949495}}, {'text': '2021\\n', 'bbox': {'x1': 0.3598218954248366, 'y1': 0.890155709090909, 'x2': 0.39237941176470587, 'y2': 0.9027347494949495}}, {'text': 'Decoder\\n', 'bbox': {'x1': 0.42654901960784314, 'y1': 0.890155709090909, 'x2': 0.4816851735294118, 'y2': 0.9027347494949495}}, {'text': 'No\\n', 'bbox': {'x1': 0.5430964052287581, 'y1': 0.890155709090909, 'x2': 0.5629890477124182, 'y2': 0.9027347494949495}}, {'text': 'NA\\n', 'bbox': {'x1': 0.6451323529411765, 'y1': 0.890155709090909, 'x2': 0.6680691232026144, 'y2': 0.9027347494949495}}, {'text': 'SNLI, MultiNLI, Question\\nNLI, RTE, SciTail, RACE,\\nStory Cloze, MSR Paraphrase\\nCorpus, Quora Question Pairs,\\nSTS Benchmark, Stanford\\nSentiment Treebank-2, CoLA\\nBAIR RobotNet, Mov-\\ning MNIST, ViZDoom,\\nUCF-101, Tumblr GIF\\n', 'bbox': {'x1': 0.7254264705882353, 'y1': 0.792794597979798, 'x2': 0.9245807980392158, 'y2': 0.9165718707070708}}]}\n",
      "{'type': 'Caption', 'bbox': [0.24269980037913602, 0.9297686767578125, 0.7606194709329044, 0.9446583140980114], 'properties': {'score': 0.4133884310722351, 'page_number': 27}, 'text_representation': 'Table 13: Transformer models for natural image processing - image generation\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09146291396197151, 0.10047106656161221, 0.9112311868106617, 0.1703389393199574], 'properties': {'score': 0.7733550667762756, 'page_number': 28}, 'text_representation': '• Image Transformer: Image Transformer is an autoregressive sequence generative model that uses the self-attention mech-\\nanism for image generation. This model generates new pixels and increases the size of the image by utilizing the attention\\nmechanism on local pixels. It uses both the encoder and decoder module of the transformer, but does not use masking in the\\nencoder. The encoder layer is used less than the decoder for better performance on image generation. Image Transformer is\\na remarkable model in the field of image generation (Parmar et al., 2018).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09158158246208639, 0.17469250765713779, 0.9109435317095588, 0.24450533780184658], 'properties': {'score': 0.864764392375946, 'page_number': 28}, 'text_representation': '• I-GPT: I-GPT or Image GPT is an image generative model that utilizes the GPT-2 model for training to auto-regressively\\npredict pixels by learning image representation, without using the 2D image. BERT motifs can also be used during pre-\\ntraining.\\nI-GPT has four variants based on the number of parameters: IGPT-S (76M parameters), IGPT-M (455M pa-\\nrameters), IGPT-L (1.4B parameters), and IGPT-XL (6.8M parameters), where models with higher parameters have more\\nvalidation losses (Chen et al., 2020b).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09191578584558824, 0.2494180020419034, 0.911518123851103, 0.31858179265802555], 'properties': {'score': 0.8638796210289001, 'page_number': 28}, 'text_representation': '• VideoGPT: VideoGPT is a generative model that combines two classes of architecture: likelihood-based models and VAE\\n(Vector Quantized Variational Autoencoder). The aim of this combination is to create a model that is easy to maintain and\\nuse, as well as resource-efficient, while also being able to encode spatio-temporal correlations in video frames. VideoGPT\\nhas shown remarkable results compared to other models, particularly in tests conducted on the “BAIR Robot Pushing”\\ndataset (Yan et al., 2021).\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09452984080595128, 0.3297524469549006, 0.3445914234834559, 0.34361017400568183], 'properties': {'score': 0.7034273743629456, 'page_number': 28}, 'text_representation': '6.3 MEDICAL IMAGE PROCESSING\\n'}\n",
      "{'type': 'Text', 'bbox': [0.0939104237275965, 0.348751220703125, 0.9127750172334559, 0.4456453635475852], 'properties': {'score': 0.9231537580490112, 'page_number': 28}, 'text_representation': 'The diagnosis of pathologies based on medical images is often criticized as complicated, time-consuming, error-prone, and\\nsubjective (L´opez-Linares et al., 2020). To overcome these challenges, alternative solutions such as deep learning approaches\\nhave been explored. Deep learning has made great progress in many other applications, such as Natural Language Processing\\nand Computer Vision. Although transformers have been successfully applied in various domains, their application to medical\\nimages is still relatively new. Other deep learning approaches such as Convolutional Neural Networks (CNN), Recurrent\\nNeural Networks (RNN), and Generative Adversarial Networks (GAN) are commonly used. This survey aims to provide a\\ncomprehensive overview of the various transformer models developed for processing medical images.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09472448909983916, 0.45522347190163354, 0.38186692181755516, 0.46872419877485794], 'properties': {'score': 0.7489354610443115, 'page_number': 28}}\n",
      "{'type': 'Text', 'bbox': [0.09406146778779871, 0.47216136585582386, 0.9121421903722426, 0.569381269975142], 'properties': {'score': 0.8915998935699463, 'page_number': 28}, 'text_representation': '6.3.1 MEDICAL IMAGE SEGMENTATION\\nImage segmentation refers to the task of grouping parts of the image that belong to the same category. In general, encoder-\\ndecoder architectures are commonly used for image segmentation (L´opez-Linares et al., 2020). In some cases, image seg-\\nmentation is performed upstream of the classification task to improve the accuracy of the classification results (Wang et al.,\\n2022c). The most frequently used loss functions in image segmentation are Pixel-wise cross-entropy loss and Dice Loss\\n(L´opez-Linares et al., 2020). Common applications of medical image segmentation include detecting lesions, identifying\\ncancer as benign or malignant, and predicting disease risk. This paper presents a comprehensive overview of relevant models\\nused in medical image segmentation. Table 14 provides a summary of these models.\\n'}\n",
      "{'type': 'table', 'bbox': [0.0524700299431296, 0.5891801313920455, 0.93021240234375, 0.9291396262428977], 'properties': {'score': 0.8216782808303833, 'title': None, 'columns': None, 'rows': None, 'page_number': 28}, 'text_representation': 'Transformer\\nName\\n Field of\\napplication\\n Year\\n Fully\\nTransformer\\nArchitecture\\n Image\\ntype\\n FTN (He\\net al., 2022)\\n Skin lesion\\n 2022\\n YES\\n 2D\\n Transformer\\nTask\\n Image seg-\\nmentation /\\nclassification\\n RAT-Net (Zhu\\net al., 2022)\\n Oncology\\n(breast cancer)\\n 2022\\n NO\\n 3D\\nultrasound\\n Image seg-\\nmentation\\n nnFormer\\n(Zhou et al.,\\n2021a)\\n Brain tumor\\nmulti-organ\\ncardiac diagnosis\\n 2022\\n YES\\n 3D\\n TransConver\\n(Liang\\net al., 2022)\\nSwinBTS\\n(Jiang et al.,\\n2022b)\\nMTPA Unet\\n(Jiang et al.,\\n2022a)\\n Brain tumor\\n 2022\\n NO\\n 2D/3D\\n Brain tumor\\n 2022\\n NO\\n Retinal vessel\\n 2022\\n NO\\n 3D\\n 2D\\n Image seg-\\nmentation\\n Image Seg-\\nmentation\\n Image Seg-\\nmentation\\n Image seg-\\nmentation\\n Dataset\\n ISIC 2018 dataset\\n a dataset of 256 subjects(330\\nAutomatic Breast Ultrasound\\nimages for each patient)\\nMedical Segmentation De-\\ncathlon (MSD), Synapse\\nmultiorgan segmentation,\\nAutomatic Cardiac Diag-\\nnosis Challenge (ACDC)\\n MICCAI BraTS2019,\\nMICCAI BraTS2018\\n BraTS 2019, BraTS\\n2020, BraTS 2021\\n DRIVE, CHASE DB1,\\nand STARE Datasets\\n Continued on next page\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e040d150>, 'tokens': [{'text': 'Transformer\\nName\\n', 'bbox': {'x1': 0.07125326797385621, 'y1': 0.5986484638888889, 'x2': 0.16005389379084967, 'y2': 0.6250646255050505}}, {'text': 'Field of\\napplication\\n', 'bbox': {'x1': 0.20578758169934638, 'y1': 0.5986484638888889, 'x2': 0.28358376699346405, 'y2': 0.6250646255050505}}, {'text': 'Year\\n', 'bbox': {'x1': 0.32276797385620914, 'y1': 0.6055663931818182, 'x2': 0.3553092114379085, 'y2': 0.6181454335858586}}, {'text': 'Fully\\nTransformer\\nArchitecture\\n', 'bbox': {'x1': 0.37729411764705884, 'y1': 0.5917292719696969, 'x2': 0.4660947434640523, 'y2': 0.6319825547979798}}, {'text': 'Image\\ntype\\n', 'bbox': {'x1': 0.501235294117647, 'y1': 0.5986484638888889, 'x2': 0.5446344633986928, 'y2': 0.6250646255050505}}, {'text': 'FTN (He\\net al., 2022)\\n', 'bbox': {'x1': 0.07677124183006535, 'y1': 0.6407504060606062, 'x2': 0.15453486960784313, 'y2': 0.6671665676767677}}, {'text': 'Skin lesion\\n', 'bbox': {'x1': 0.20827777777777776, 'y1': 0.6476683353535353, 'x2': 0.28109266307189545, 'y2': 0.6602473757575759}}, {'text': '2022\\n', 'bbox': {'x1': 0.32276797385620914, 'y1': 0.6476683353535353, 'x2': 0.3553254901960784, 'y2': 0.6602473757575759}}, {'text': 'YES\\n', 'bbox': {'x1': 0.4063186274509804, 'y1': 0.6476683353535353, 'x2': 0.43706920163398694, 'y2': 0.6602473757575759}}, {'text': '2D\\n', 'bbox': {'x1': 0.5129885620915033, 'y1': 0.6476683353535353, 'x2': 0.5328812045751633, 'y2': 0.6602473757575759}}, {'text': 'Transformer\\nTask\\n', 'bbox': {'x1': 0.5913562091503268, 'y1': 0.5986484638888889, 'x2': 0.6801568349673203, 'y2': 0.6250646255050505}}, {'text': 'Image seg-\\nmentation /\\nclassification\\n', 'bbox': {'x1': 0.5932532679738562, 'y1': 0.6338312141414142, 'x2': 0.6782609431372549, 'y2': 0.674084496969697}}, {'text': 'RAT-Net (Zhu\\net al., 2022)\\n', 'bbox': {'x1': 0.06870424836601306, 'y1': 0.682764294949495, 'x2': 0.1626001254901961, 'y2': 0.7091804565656566}}, {'text': 'Oncology\\n(breast cancer)\\n', 'bbox': {'x1': 0.19655718954248366, 'y1': 0.682764294949495, 'x2': 0.2928134866013072, 'y2': 0.7091804565656566}}, {'text': '2022\\n', 'bbox': {'x1': 0.32276797385620914, 'y1': 0.689683486868687, 'x2': 0.3553254901960784, 'y2': 0.7022625272727273}}, {'text': 'NO\\n', 'bbox': {'x1': 0.40993954248366016, 'y1': 0.689683486868687, 'x2': 0.43344606928104573, 'y2': 0.7022625272727273}}, {'text': '3D\\nultrasound\\n', 'bbox': {'x1': 0.4885702614379085, 'y1': 0.682764294949495, 'x2': 0.5572991784313726, 'y2': 0.7091804565656566}}, {'text': 'Image seg-\\nmentation\\n', 'bbox': {'x1': 0.5999428104575164, 'y1': 0.682764294949495, 'x2': 0.6715693464052288, 'y2': 0.7091804565656566}}, {'text': 'nnFormer\\n(Zhou et al.,\\n2021a)\\n', 'bbox': {'x1': 0.07586764705882353, 'y1': 0.7316973757575758, 'x2': 0.15543821699346405, 'y2': 0.7719506585858585}}, {'text': 'Brain tumor\\nmulti-organ\\ncardiac diagnosis\\n', 'bbox': {'x1': 0.18840196078431373, 'y1': 0.7316973757575758, 'x2': 0.3009695735294118, 'y2': 0.7719506585858585}}, {'text': '2022\\n', 'bbox': {'x1': 0.32276797385620914, 'y1': 0.745534496969697, 'x2': 0.3553254901960784, 'y2': 0.7581135373737374}}, {'text': 'YES\\n', 'bbox': {'x1': 0.4063186274509804, 'y1': 0.745534496969697, 'x2': 0.43706920163398694, 'y2': 0.7581135373737374}}, {'text': '3D\\n', 'bbox': {'x1': 0.5129885620915033, 'y1': 0.745534496969697, 'x2': 0.5328812045751633, 'y2': 0.7581135373737374}}, {'text': 'TransConver\\n(Liang\\net al., 2022)\\nSwinBTS\\n(Jiang et al.,\\n2022b)\\nMTPA Unet\\n(Jiang et al.,\\n2022a)\\n', 'bbox': {'x1': 0.07388888888888889, 'y1': 0.7875483858585859, 'x2': 0.1574151970588235, 'y2': 0.9118294464646465}}, {'text': 'Brain tumor\\n', 'bbox': {'x1': 0.20511928104575164, 'y1': 0.8013855070707071, 'x2': 0.2842503245098039, 'y2': 0.8139645474747474}}, {'text': '2022\\n', 'bbox': {'x1': 0.32276797385620914, 'y1': 0.8013855070707071, 'x2': 0.3553254901960784, 'y2': 0.8139645474747474}}, {'text': 'NO\\n', 'bbox': {'x1': 0.40993954248366016, 'y1': 0.8013855070707071, 'x2': 0.43344606928104573, 'y2': 0.8139645474747474}}, {'text': '2D/3D\\n', 'bbox': {'x1': 0.5007794117647059, 'y1': 0.8013855070707071, 'x2': 0.545090191503268, 'y2': 0.8139645474747474}}, {'text': 'Brain tumor\\n', 'bbox': {'x1': 0.20511928104575164, 'y1': 0.8433993959595959, 'x2': 0.2842503245098039, 'y2': 0.8559784363636365}}, {'text': '2022\\n', 'bbox': {'x1': 0.32276797385620914, 'y1': 0.8433993959595959, 'x2': 0.3553254901960784, 'y2': 0.8559784363636365}}, {'text': 'NO\\n', 'bbox': {'x1': 0.40993954248366016, 'y1': 0.8433993959595959, 'x2': 0.43344606928104573, 'y2': 0.8559784363636365}}, {'text': 'Retinal vessel\\n', 'bbox': {'x1': 0.19936601307189541, 'y1': 0.8854132848484848, 'x2': 0.2900061385620915, 'y2': 0.8979923252525253}}, {'text': '2022\\n', 'bbox': {'x1': 0.32276797385620914, 'y1': 0.8854132848484848, 'x2': 0.3553254901960784, 'y2': 0.8979923252525253}}, {'text': 'NO\\n', 'bbox': {'x1': 0.40993954248366016, 'y1': 0.8854132848484848, 'x2': 0.43344606928104573, 'y2': 0.8979923252525253}}, {'text': '3D\\n', 'bbox': {'x1': 0.5129885620915033, 'y1': 0.8433993959595959, 'x2': 0.5328812045751633, 'y2': 0.8559784363636365}}, {'text': '2D\\n', 'bbox': {'x1': 0.5129885620915033, 'y1': 0.8854132848484848, 'x2': 0.5328812045751633, 'y2': 0.8979923252525253}}, {'text': 'Image seg-\\nmentation\\n', 'bbox': {'x1': 0.5999428104575164, 'y1': 0.738615305050505, 'x2': 0.6715693464052288, 'y2': 0.7650314666666668}}, {'text': 'Image Seg-\\nmentation\\n', 'bbox': {'x1': 0.5985833333333334, 'y1': 0.7944663151515151, 'x2': 0.6729284218954248, 'y2': 0.8208824767676768}}, {'text': 'Image Seg-\\nmentation\\n', 'bbox': {'x1': 0.5985833333333334, 'y1': 0.8364814666666666, 'x2': 0.6729284218954248, 'y2': 0.8628963656565656}}, {'text': 'Image seg-\\nmentation\\n', 'bbox': {'x1': 0.5999428104575164, 'y1': 0.8784953555555556, 'x2': 0.6715693464052288, 'y2': 0.9049115171717171}}, {'text': 'Dataset\\n', 'bbox': {'x1': 0.7895212418300653, 'y1': 0.6055663931818182, 'x2': 0.8419551218954249, 'y2': 0.6181454335858586}}, {'text': 'ISIC 2018 dataset\\n', 'bbox': {'x1': 0.7574117647058823, 'y1': 0.6476683353535353, 'x2': 0.8740653457516341, 'y2': 0.6602473757575759}}, {'text': 'a dataset of 256 subjects(330\\nAutomatic Breast Ultrasound\\nimages for each patient)\\nMedical Segmentation De-\\ncathlon (MSD), Synapse\\nmultiorgan segmentation,\\nAutomatic Cardiac Diag-\\nnosis Challenge (ACDC)\\n', 'bbox': {'x1': 0.7203382352941177, 'y1': 0.6758463656565656, 'x2': 0.9111415598039216, 'y2': 0.7857865171717171}}, {'text': 'MICCAI BraTS2019,\\nMICCAI BraTS2018\\n', 'bbox': {'x1': 0.7447467320261438, 'y1': 0.7944663151515151, 'x2': 0.8867300607843137, 'y2': 0.8208824767676768}}, {'text': 'BraTS 2019, BraTS\\n2020, BraTS 2021\\n', 'bbox': {'x1': 0.7508513071895425, 'y1': 0.8364814666666666, 'x2': 0.8806255673202614, 'y2': 0.8628963656565656}}, {'text': 'DRIVE, CHASE DB1,\\nand STARE Datasets\\n', 'bbox': {'x1': 0.7406764705882353, 'y1': 0.8784953555555556, 'x2': 0.8907991784313726, 'y2': 0.9049115171717171}}, {'text': 'Continued on next page\\n', 'bbox': {'x1': 0.7627941176470588, 'y1': 0.9140938404040404, 'x2': 0.9176376653594772, 'y2': 0.9266728808080809}}]}\n",
      "{'type': 'table', 'bbox': [0.052346200382008275, 0.10352206143465909, 0.9302425608915441, 0.2968751387162642], 'properties': {'score': 0.791662335395813, 'title': None, 'columns': None, 'rows': None, 'page_number': 29}, 'text_representation': 'Transformer\\nName\\n Dilated\\nTransformer\\n(Shen et al.,\\n2022b)\\nTFNet (Wang\\net al., 2021b)\\nChest L-\\nTransformer\\n(Gu et al.,\\n2022)\\n Field of\\napplication\\n Year\\n Fully\\nTransformer\\nArchitecture\\n Image\\ntype\\n Transformer\\nTask\\n Dataset\\n Oncology\\n(Breast Cancer)\\n Oncology\\n(Breast lesion)\\n Chest radiograph\\n/ Thoracic\\ndiseases\\n 2022\\n NO\\n 2022\\n NO\\n 2D\\nultrasound\\n 2D\\nultrasound\\n 2022\\n NO\\n 2D\\n Image seg-\\nmentation\\n Image Seg-\\nmentation\\n Image Clas-\\nsification /\\nSegmentation\\n 2 small breast ultra-\\nsound image datasets\\n BUSI Dataset DDTI Dataset\\n SIIM-ACR Pneumoth-\\norax Segmentation\\ndataset contains 12,047\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e040f460>, 'tokens': [{'text': 'Transformer\\nName\\n', 'bbox': {'x1': 0.07125326797385621, 'y1': 0.12046538308080808, 'x2': 0.16005389379084967, 'y2': 0.14688154469696957}}, {'text': 'Dilated\\nTransformer\\n(Shen et al.,\\n2022b)\\nTFNet (Wang\\net al., 2021b)\\nChest L-\\nTransformer\\n(Gu et al.,\\n2022)\\n', 'bbox': {'x1': 0.07086928104575163, 'y1': 0.15564939595959598, 'x2': 0.160435008496732, 'y2': 0.2937675777777776}}, {'text': 'Field of\\napplication\\n', 'bbox': {'x1': 0.20578758169934638, 'y1': 0.12046538308080808, 'x2': 0.28358376699346405, 'y2': 0.14688154469696957}}, {'text': 'Year\\n', 'bbox': {'x1': 0.32276797385620914, 'y1': 0.12738331237373743, 'x2': 0.3553092114379085, 'y2': 0.13996235277777777}}, {'text': 'Fully\\nTransformer\\nArchitecture\\n', 'bbox': {'x1': 0.37729411764705884, 'y1': 0.11354619116161614, 'x2': 0.4660947434640523, 'y2': 0.15379947398989877}}, {'text': 'Image\\ntype\\n', 'bbox': {'x1': 0.501235294117647, 'y1': 0.12046538308080808, 'x2': 0.5446344633986928, 'y2': 0.14688154469696957}}, {'text': 'Transformer\\nTask\\n', 'bbox': {'x1': 0.5913562091503268, 'y1': 0.12046538308080808, 'x2': 0.6801568349673203, 'y2': 0.14688154469696957}}, {'text': 'Dataset\\n', 'bbox': {'x1': 0.7895212418300653, 'y1': 0.12738331237373743, 'x2': 0.8419551218954249, 'y2': 0.13996235277777777}}, {'text': 'Oncology\\n(Breast Cancer)\\n', 'bbox': {'x1': 0.19338235294117645, 'y1': 0.16948525454545452, 'x2': 0.2959873656862745, 'y2': 0.19590141616161616}}, {'text': 'Oncology\\n(Breast lesion)\\n', 'bbox': {'x1': 0.19698856209150328, 'y1': 0.21150040606060597, 'x2': 0.29238208496732027, 'y2': 0.2379165676767676}}, {'text': 'Chest radiograph\\n/ Thoracic\\ndiseases\\n', 'bbox': {'x1': 0.18884967320261437, 'y1': 0.24659510303030305, 'x2': 0.300521954248366, 'y2': 0.2868483858585858}}, {'text': '2022\\n', 'bbox': {'x1': 0.32276797385620914, 'y1': 0.17640444646464645, 'x2': 0.3553254901960784, 'y2': 0.18898348686868693}}, {'text': 'NO\\n', 'bbox': {'x1': 0.40993954248366016, 'y1': 0.17640444646464645, 'x2': 0.43344606928104573, 'y2': 0.18898348686868693}}, {'text': '2022\\n', 'bbox': {'x1': 0.32276797385620914, 'y1': 0.21841833535353533, 'x2': 0.3553254901960784, 'y2': 0.2309973757575758}}, {'text': 'NO\\n', 'bbox': {'x1': 0.40993954248366016, 'y1': 0.21841833535353533, 'x2': 0.43344606928104573, 'y2': 0.2309973757575758}}, {'text': '2D\\nultrasound\\n', 'bbox': {'x1': 0.4885702614379085, 'y1': 0.16948525454545452, 'x2': 0.5572991784313726, 'y2': 0.19590141616161616}}, {'text': '2D\\nultrasound\\n', 'bbox': {'x1': 0.4885702614379085, 'y1': 0.21150040606060597, 'x2': 0.5572991784313726, 'y2': 0.2379165676767676}}, {'text': '2022\\n', 'bbox': {'x1': 0.32276797385620914, 'y1': 0.2604322242424242, 'x2': 0.3553254901960784, 'y2': 0.2730112646464647}}, {'text': 'NO\\n', 'bbox': {'x1': 0.40993954248366016, 'y1': 0.2604322242424242, 'x2': 0.43344606928104573, 'y2': 0.2730112646464647}}, {'text': '2D\\n', 'bbox': {'x1': 0.5129885620915033, 'y1': 0.2604322242424242, 'x2': 0.5328812045751633, 'y2': 0.2730112646464647}}, {'text': 'Image seg-\\nmentation\\n', 'bbox': {'x1': 0.5999428104575164, 'y1': 0.16948525454545452, 'x2': 0.6715693464052288, 'y2': 0.19590141616161616}}, {'text': 'Image Seg-\\nmentation\\n', 'bbox': {'x1': 0.5985833333333334, 'y1': 0.21150040606060597, 'x2': 0.6729284218954248, 'y2': 0.2379165676767676}}, {'text': 'Image Clas-\\nsification /\\nSegmentation\\n', 'bbox': {'x1': 0.5911127450980392, 'y1': 0.24659510303030305, 'x2': 0.6804017336601308, 'y2': 0.2868483858585858}}, {'text': '2 small breast ultra-\\nsound image datasets\\n', 'bbox': {'x1': 0.746562091503268, 'y1': 0.16948525454545452, 'x2': 0.8849152571895424, 'y2': 0.19590141616161616}}, {'text': 'BUSI Dataset DDTI Dataset\\n', 'bbox': {'x1': 0.7229183006535947, 'y1': 0.21841833535353533, 'x2': 0.9085612588235296, 'y2': 0.2309973757575758}}, {'text': 'SIIM-ACR Pneumoth-\\norax Segmentation\\ndataset contains 12,047\\n', 'bbox': {'x1': 0.7395539215686274, 'y1': 0.24659510303030305, 'x2': 0.8919230980392158, 'y2': 0.2868483858585858}}]}\n",
      "{'type': 'Caption', 'bbox': [0.2947087007410386, 0.3066684237393466, 0.7082816090303309, 0.32093658447265627], 'properties': {'score': 0.6040757894515991, 'page_number': 29}, 'text_representation': 'Table 14: Transformer models for medical image segmentation\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09076410630170037, 0.355851357199929, 0.9122515510110294, 0.45284914883700284], 'properties': {'score': 0.7599169015884399, 'page_number': 29}, 'text_representation': '• FTN: FTN is a transformer-based architecture developed specifically for segmenting and classifying 2D images of skin le-\\nsions. It comprises of 5 layers, where each layer has a tokenization module known as SWT ”Sliding Window Tokenization”\\nand a transformer module. The model is segregated into encoders and decoders for the segmentation task, while only an en-\\ncoder is needed for classification tasks. To improve computational efficiency and storage optimization, MSPA ”Multi-head\\nSpatial Pyramid Attention” is utilized in the ”transformer” module instead of the traditional multi-head attention (MHA). In\\ncomparison to CNN, FTN has demonstrated superior performance on 10,025 images extracted from the publicly available\\nISIC 2018 dataset (He et al., 2022).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09150621301987592, 0.4660931396484375, 0.9115016802619486, 0.5358927778764204], 'properties': {'score': 0.8261763453483582, 'page_number': 29}, 'text_representation': '• RAT-Net: The primary objective of RAT-Net (Region Aware Transformer Network) is to replace the laborious and time-\\nconsuming manual task of detecting lesion contours in 3D ABUS (Automatic Breast Ultrasound) images. Compared to\\nother state-of-the-art models proposed for medical image segmentation, RAT-Net has shown excellent performance. It is\\nbased on the SegFormer Transformer model, which is used to encode input images and determine the regions that are more\\nrelevant for lesion segmentation (Zhu et al., 2022).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09109375, 0.5489499733664772, 0.9116826315487132, 0.632225508256392], 'properties': {'score': 0.835257887840271, 'page_number': 29}, 'text_representation': '• nnFormer:\\n is a model that uses Transformer architecture for segmentation of 3D medical images. The experiments\\nwere performed on 484 brain tumor images, 30 multi-organ scans, and 100 cardiac diagnosis images. Instead of using\\nthe conventional attention mechanism, nnFormer introduced LV-MSA “Volume-based Multi-head Self-attention” and GV-\\nMSA ”Global Volume-based Multi-head Self-attention” to reduce the computational complexity. Additionally, nnFormer\\nemploys multiple convolution layers with small kernels in the encoder instead of large convolution kernels as in other visual\\ntransformers (Zhou et al., 2021a).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0914314988080193, 0.6451234019886364, 0.9117415843290441, 0.7145279208096591], 'properties': {'score': 0.8537349104881287, 'page_number': 29}, 'text_representation': '• TransConver:\\n It combines CNN and SWIN transformers in parallel to extract global and local features simultaneously\\nis proposed. The transformer block employs a cross-attention mechanism to merge semantically different global and local\\nfeatures. The network is designed to process both 2D and 3D brain tumor images and is trained on 335 cases from the\\ntraining dataset of MICCAI BraTS2019. It is evaluated on 66 cases from MICCAI BraTS2018 and 125 cases from MICCAI\\nBraTS2019 (Liang et al., 2022).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.091469825295841, 0.7271889981356534, 0.91184326171875, 0.7831942471590909], 'properties': {'score': 0.8347207307815552, 'page_number': 29}, 'text_representation': '• SwinBTS: is a recently developed model that addresses the segmentation of 3D medical images by combining the Swin\\nTransformer with CNN. It adopts an encoder-decoder architecture that applies the Swin Transformer to both the encoder and\\ndecoder. In addition, SwinBTS incorporates an advanced feature extraction module called ETrans (Enhanced Transformer)\\nthat follows the transformer approach and leverages convolution techniques (Jiang et al., 2022b).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09082064460305607, 0.7960651189630682, 0.9117821547564339, 0.8937525523792613], 'properties': {'score': 0.8574399948120117, 'page_number': 29}, 'text_representation': '• MTPA Unet:\\n (Multi-scale Transformer-Position Attention Unet) is a model that has been evaluated on several publicly\\nrecognized retinal datasets to enhance the performance of retinal image segmentation tasks. This model combines CNN\\nand transformer architectures sequentially to accurately capture local and global image information. To capture long-term\\ndependencies between pixels as well as contextual information about each pixel location, this model employs TPA (Trans-\\nformer Position Attention), which is a combination of MSA (Multi-headed Self-Attention) and ”Position Attention Module”.\\nAdditionally, to optimize the model’s extraction ability, feature map inputs of different resolutions are implemented due to\\nthe detailed information contained in retinal images (Jiang et al., 2022a).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09202548756318933, 0.9063573663884943, 0.9102841366038603, 0.9479901677911932], 'properties': {'score': 0.8230828046798706, 'page_number': 29}, 'text_representation': '• TFNet: TFNet, aims to segment 2D ultrasound images of breast lesions by combining CNN with a transformer architecture.\\nTo address the challenge of lesions with different scales and variable intensities, CNN is employed as a backbone to extract\\nfeatures from the images, resulting in 3 high-level features containing semantic information and 1 low-level feature. These\\n'}\n",
      "{'type': 'Text', 'bbox': [0.1057441801183364, 0.10056990189985796, 0.9120888384650735, 0.1842424843528054], 'properties': {'score': 0.8860158920288086, 'page_number': 30}, 'text_representation': 'high-level features are fused through a Transformer Fuse Module (TFM), while the low-level features are fused via skip\\nconnection. The transformer module includes two main parts: Vanilla Multi-Head Self-Attention to capture the long-range\\ndependency between sequences and MultiHead Channel-Attention (MCA) to detect dependencies between channels. To\\nenhance the model’s performance, novel loss functions are introduced, resulting in superior segmentation performance\\ncompared to other models. This approach is evaluated on a range of ultrasound image datasets, demonstrating excellent\\nsegmentation results (Wang et al., 2021b).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09144782571231617, 0.19614131580699573, 0.9114594582950367, 0.2796623091264204], 'properties': {'score': 0.776703417301178, 'page_number': 30}, 'text_representation': '• Dilated Transformer: The (DT) model has been developed for the segmentation of 2D ultrasound images from small\\ndatasets of breast cancer using transformer architecture. Standard transformer models require large pre-training datasets\\nto generate high-quality segmentation results, but DT overcomes this challenge by implementing the ”Residual Axial At-\\ntention” mechanism for segmenting images from small breast ultrasound datasets. This approach applies attention to a\\nsingle axis, namely the height axis and the width axis, instead of the whole feature map, which saves time and enhances\\ncomputation efficiency (Shen et al., 2022b).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0913119327320772, 0.29121559836647726, 0.9115367934283088, 0.3610838734019886], 'properties': {'score': 0.7620993256568909, 'page_number': 30}, 'text_representation': '• Chest L-Transformer: This model is developed for the segmentation and classification of chest radiograph images. It uses\\na combination of CNN and transformer architecture, where the CNN is used as a backbone to extract local features from\\nthe 2D images and the transformer block is applied to detect the location of lesions using attention mechanisms. By using\\ntransformers in chest radiograph images, the model focuses more on areas where the disease may be more likely to occur,\\nas opposed to treating them similarly using CNN alone (Gu et al., 2022).\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09466266407686122, 0.3700211681019176, 0.3877825568704044, 0.3837576016512784], 'properties': {'score': 0.6739804148674011, 'page_number': 30}}\n",
      "{'type': 'Text', 'bbox': [0.09395596672506894, 0.38745472301136363, 0.9128763355928309, 0.4706448641690341], 'properties': {'score': 0.8968415260314941, 'page_number': 30}, 'text_representation': '6.3.2 MEDICAL IMAGE CLASSIFICATION\\nImage classification refers to the process of recognizing, extracting, and selecting different types of features from an image for\\nclassification using labels (Wang et al., 2020b). Features in an image can be categorized into three types: low-level features,\\nmid-level features, and high-level features (Wang et al., 2020b). Deep learning networks are designed to extract high-level\\nfeatures. Common applications of medical image classification include the detection of lesions, the identification of cancers\\nas benign or malignant, and the prediction of disease risk (Khan & Lee, 2023, Jungiewicz et al., 2023). Table 15, provides an\\noverview of several relevant examples of Transformers used in medical image classification.\\n'}\n",
      "{'type': 'table', 'bbox': [0.05240830814137178, 0.4904867276278409, 0.9301356416590073, 0.6343010919744319], 'properties': {'score': 0.6597973704338074, 'title': None, 'columns': None, 'rows': None, 'page_number': 30}, 'text_representation': 'Transformer\\nName\\n CCT-based\\nModel (Islam\\net al., 2022)\\nChest L-\\nTransformer\\n(Gu et al.,\\n2022)\\n Field of\\napplication\\n Year\\n Fully\\nTransformer\\nArchitecture\\n Malaria Disease\\n 2022\\n NO\\n Image\\ntype\\n 2D\\nimages\\n Chest radiograph\\n/ Thoracic\\ndiseases\\n 2022\\n NO\\n 2D\\n Transformer\\nTask\\n Dataset\\n Image Clas-\\nsification\\n National Library of\\nMedicine malaria dataset\\n Image Clas-\\nsification /\\nSegmentation\\n SIIM-ACR Pneumoth-\\norax Segmentation\\ndataset contains 12,047\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e024ca00>, 'tokens': [{'text': 'Transformer\\nName\\n', 'bbox': {'x1': 0.07125326797385621, 'y1': 0.4993062921717172, 'x2': 0.16005389379084967, 'y2': 0.5257224537878789}}, {'text': 'CCT-based\\nModel (Islam\\net al., 2022)\\nChest L-\\nTransformer\\n(Gu et al.,\\n2022)\\n', 'bbox': {'x1': 0.0715702614379085, 'y1': 0.5344903050505051, 'x2': 0.15973601568627452, 'y2': 0.630594597979798}}, {'text': 'Field of\\napplication\\n', 'bbox': {'x1': 0.20578758169934638, 'y1': 0.4993062921717172, 'x2': 0.28358376699346405, 'y2': 0.5257224537878789}}, {'text': 'Year\\n', 'bbox': {'x1': 0.32276797385620914, 'y1': 0.5062254840909091, 'x2': 0.3553092114379085, 'y2': 0.5188045244949495}}, {'text': 'Fully\\nTransformer\\nArchitecture\\n', 'bbox': {'x1': 0.37729411764705884, 'y1': 0.49238836287878784, 'x2': 0.4660947434640523, 'y2': 0.5326416457070707}}, {'text': 'Malaria Disease\\n', 'bbox': {'x1': 0.19202450980392158, 'y1': 0.5483274262626262, 'x2': 0.29734807516339873, 'y2': 0.5609064666666667}}, {'text': '2022\\n', 'bbox': {'x1': 0.32276797385620914, 'y1': 0.5483274262626262, 'x2': 0.3553254901960784, 'y2': 0.5609064666666667}}, {'text': 'NO\\n', 'bbox': {'x1': 0.40993954248366016, 'y1': 0.5483274262626262, 'x2': 0.43344606928104573, 'y2': 0.5609064666666667}}, {'text': 'Image\\ntype\\n', 'bbox': {'x1': 0.501235294117647, 'y1': 0.4993062921717172, 'x2': 0.5446344633986928, 'y2': 0.5257224537878789}}, {'text': '2D\\nimages\\n', 'bbox': {'x1': 0.49987745098039216, 'y1': 0.5414082343434343, 'x2': 0.545995172875817, 'y2': 0.5678243959595959}}, {'text': 'Chest radiograph\\n/ Thoracic\\ndiseases\\n', 'bbox': {'x1': 0.18884967320261437, 'y1': 0.5834233858585859, 'x2': 0.300521954248366, 'y2': 0.6236766686868687}}, {'text': '2022\\n', 'bbox': {'x1': 0.32276797385620914, 'y1': 0.597260507070707, 'x2': 0.3553254901960784, 'y2': 0.6098395474747474}}, {'text': 'NO\\n', 'bbox': {'x1': 0.40993954248366016, 'y1': 0.597260507070707, 'x2': 0.43344606928104573, 'y2': 0.6098395474747474}}, {'text': '2D\\n', 'bbox': {'x1': 0.5129885620915033, 'y1': 0.597260507070707, 'x2': 0.5328812045751633, 'y2': 0.6098395474747474}}, {'text': 'Transformer\\nTask\\n', 'bbox': {'x1': 0.5913562091503268, 'y1': 0.4993062921717172, 'x2': 0.6801568349673203, 'y2': 0.5257224537878789}}, {'text': 'Dataset\\n', 'bbox': {'x1': 0.7895212418300653, 'y1': 0.5062254840909091, 'x2': 0.8419551218954249, 'y2': 0.5188045244949495}}, {'text': 'Image Clas-\\nsification\\n', 'bbox': {'x1': 0.5961993464052288, 'y1': 0.5414082343434343, 'x2': 0.6753141111111112, 'y2': 0.5678243959595959}}, {'text': 'National Library of\\nMedicine malaria dataset\\n', 'bbox': {'x1': 0.7339133986928105, 'y1': 0.5414082343434343, 'x2': 0.8975637545751636, 'y2': 0.5678243959595959}}, {'text': 'Image Clas-\\nsification /\\nSegmentation\\n', 'bbox': {'x1': 0.5911127450980392, 'y1': 0.5834233858585859, 'x2': 0.6804017336601308, 'y2': 0.6236766686868687}}, {'text': 'SIIM-ACR Pneumoth-\\norax Segmentation\\ndataset contains 12,047\\n', 'bbox': {'x1': 0.7395539215686274, 'y1': 0.5834233858585859, 'x2': 0.8919230980392158, 'y2': 0.6236766686868687}}]}\n",
      "{'type': 'Caption', 'bbox': [0.29622811710133273, 0.6435190651633522, 0.7074593577665441, 0.6580454323508522], 'properties': {'score': 0.618433952331543, 'page_number': 30}, 'text_representation': 'Table 15: Transformer models for medical image classification\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0909138578527114, 0.6764380437677557, 0.91231689453125, 0.7769745982776989], 'properties': {'score': 0.6629397869110107, 'page_number': 30}, 'text_representation': '• CCT-based Model (Islam et al., 2022): The model presented in this work is designed for classifying red blood cell (RBC)\\nimages as containing malaria parasites or not, by using Compact Convolutional Transformers (CCTs). The model input\\nconsists of image patches generated through convolutional operations and preprocessed by reshaping them to a fixed size.\\nUnlike other vision transformer models, this model performs classification using sequence pooling instead of class tokens.\\nCompared to other deep learning models such as CNN, this model shows good performance in classifying RBC images.\\nThis satisfactory result was achieved by implementing a transformer architecture, using GRAD-CAM techniques to validate\\nthe learning process, and fine-tuning hyperparameters.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09123605167164522, 0.67658203125, 0.9125879624310662, 0.7325302956321023], 'properties': {'score': 0.35570645332336426, 'page_number': 30}, 'text_representation': '• CCT-based Model (Islam et al., 2022): The model presented in this work is designed for classifying red blood cell (RBC)\\nimages as containing malaria parasites or not, by using Compact Convolutional Transformers (CCTs). The model input\\nconsists of image patches generated through convolutional operations and preprocessed by reshaping them to a fixed size.\\nUnlike other vision transformer models, this model performs classification using sequence pooling instead of class tokens.\\nCompared to other deep learning models such as CNN, this model shows good performance in classifying RBC images.\\nThis satisfactory result was achieved by implementing a transformer architecture, using GRAD-CAM techniques to validate\\nthe learning process, and fine-tuning hyperparameters.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09087168974034926, 0.7802212801846591, 0.9125936351102941, 0.8505467640269886], 'properties': {'score': 0.7475379109382629, 'page_number': 30}, 'text_representation': '• Chest L-Transformer (Gu et al., 2022): is a model designed for segmenting and classifying chest radiograph images (Gu\\net al., 2022). The model utilizes a CNN backbone to extract local features from 2D images and a transformer block to\\napply attention mechanisms for detecting lesion locations. By incorporating transformers into the chest radiograph image\\nanalysis, the model is better able to attend to areas where disease may be more likely to occur, as opposed to traditional\\nCNNs which treat all areas similarly.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09392304364372703, 0.8730565296519887, 0.37186365464154414, 0.8865263227982955], 'properties': {'score': 0.7048540711402893, 'page_number': 30}}\n",
      "{'type': 'Text', 'bbox': [0.09432378432329963, 0.8897976407137784, 0.9107102338005515, 0.945572509765625], 'properties': {'score': 0.846420407295227, 'page_number': 30}, 'text_representation': '6.3.3 MEDICAL IMAGE TRANSLATION\\nThe field of research that involves altering the context (or domain) of an image without changing its original content is\\ngaining traction. One example of this involves applying cartoon-style effects to images to change their appearance (Pang\\net al., 2022). Image-to-image translation is a promising technique that can be utilized to synthesize medical images from non-\\ncorrupted sources with less cost and time, and it is also helpful for preparing medical images for registration or segmentation.\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09400588091681986, 0.1003342715176669, 0.9109477682674633, 0.14274941184303977], 'properties': {'score': 0.8647176623344421, 'page_number': 31}, 'text_representation': 'Some of the most popular deep learning models developed for this area include “Pix2Pix” and “cyclic-consistency generative\\nadversarial network” (GAN) (Yan et al., 2022b). Table 16 provides an overview of some relevant examples of ”Transformers”\\ndesigned for medical image-to-image translation.\\n'}\n",
      "{'type': 'table', 'bbox': [0.05219232895795037, 0.162661826393821, 0.9205534811580882, 0.29290213844992896], 'properties': {'score': 0.6748709678649902, 'title': None, 'columns': None, 'rows': None, 'page_number': 31}, 'text_representation': 'Transformer\\nName\\n Field of\\napplication\\n Year\\n Fully\\nTransformer\\nArchitecture\\n MMTrans\\n(Yan et al.,\\n2022b)\\nTransCBCT\\n(Chen et al.,\\n2022c)\\n Magnetic\\nresonance\\nimaging (MRI)\\n Oncology\\n(prostate Cancer)\\n 2022\\n NO\\n 2022\\n NO\\n Image\\ntype\\n Transformer\\nTask\\n Dataset\\n 2D\\n 2D\\n Medical image-\\nto-image\\ntranslation\\n Image\\nTranslation\\n BraTs2018, fastMRI, The\\nclinical brain MRI dataset\\n 91 patients with\\nprostate cancer\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e024d930>, 'tokens': [{'text': 'Transformer\\nName\\n', 'bbox': {'x1': 0.06662091503267974, 'y1': 0.1722469487373737, 'x2': 0.15542154084967322, 'y2': 0.19866311035353518}}, {'text': 'Field of\\napplication\\n', 'bbox': {'x1': 0.19652450980392155, 'y1': 0.1722469487373737, 'x2': 0.27432069509803925, 'y2': 0.19866311035353518}}, {'text': 'Year\\n', 'bbox': {'x1': 0.31350490196078434, 'y1': 0.17916487803030304, 'x2': 0.34604613954248364, 'y2': 0.19174391843434338}}, {'text': 'Fully\\nTransformer\\nArchitecture\\n', 'bbox': {'x1': 0.3680294117647059, 'y1': 0.1653290194444445, 'x2': 0.4568300375816994, 'y2': 0.20558103964646451}}, {'text': 'MMTrans\\n(Yan et al.,\\n2022b)\\nTransCBCT\\n(Chen et al.,\\n2022c)\\n', 'bbox': {'x1': 0.07123529411764705, 'y1': 0.2074309616161616, 'x2': 0.1508058640522876, 'y2': 0.2896981333333332}}, {'text': 'Magnetic\\nresonance\\nimaging (MRI)\\n', 'bbox': {'x1': 0.18591013071895424, 'y1': 0.2074309616161616, 'x2': 0.2849338166666667, 'y2': 0.24768424444444437}}, {'text': 'Oncology\\n(prostate Cancer)\\n', 'bbox': {'x1': 0.1791470588235294, 'y1': 0.2563627797979798, 'x2': 0.2916983928104575, 'y2': 0.28277894141414145}}, {'text': '2022\\n', 'bbox': {'x1': 0.31350490196078434, 'y1': 0.22126808282828273, 'x2': 0.3460624183006536, 'y2': 0.2338471232323232}}, {'text': 'NO\\n', 'bbox': {'x1': 0.4006764705882353, 'y1': 0.22126808282828273, 'x2': 0.42418299738562093, 'y2': 0.2338471232323232}}, {'text': '2022\\n', 'bbox': {'x1': 0.31350490196078434, 'y1': 0.2632819717171717, 'x2': 0.3460624183006536, 'y2': 0.2758610121212122}}, {'text': 'NO\\n', 'bbox': {'x1': 0.4006764705882353, 'y1': 0.2632819717171717, 'x2': 0.42418299738562093, 'y2': 0.2758610121212122}}, {'text': 'Image\\ntype\\n', 'bbox': {'x1': 0.4919722222222222, 'y1': 0.1722469487373737, 'x2': 0.535371391503268, 'y2': 0.19866311035353518}}, {'text': 'Transformer\\nTask\\n', 'bbox': {'x1': 0.582093137254902, 'y1': 0.1722469487373737, 'x2': 0.6708937630718954, 'y2': 0.19866311035353518}}, {'text': 'Dataset\\n', 'bbox': {'x1': 0.7802581699346405, 'y1': 0.17916487803030304, 'x2': 0.8326920499999999, 'y2': 0.19174391843434338}}, {'text': '2D\\n', 'bbox': {'x1': 0.5037254901960784, 'y1': 0.22126808282828273, 'x2': 0.5236181326797384, 'y2': 0.2338471232323232}}, {'text': '2D\\n', 'bbox': {'x1': 0.5037254901960784, 'y1': 0.2632819717171717, 'x2': 0.5236181326797384, 'y2': 0.2758610121212122}}, {'text': 'Medical image-\\nto-image\\ntranslation\\n', 'bbox': {'x1': 0.5751830065359477, 'y1': 0.2074309616161616, 'x2': 0.6778042980392157, 'y2': 0.24768424444444437}}, {'text': 'Image\\nTranslation\\n', 'bbox': {'x1': 0.5897042483660131, 'y1': 0.2563627797979798, 'x2': 0.6632842352941176, 'y2': 0.28277894141414145}}, {'text': 'BraTs2018, fastMRI, The\\nclinical brain MRI dataset\\n', 'bbox': {'x1': 0.7217042483660131, 'y1': 0.21434889090909093, 'x2': 0.8912475147058824, 'y2': 0.24076505252525257}}, {'text': '91 patients with\\nprostate cancer\\n', 'bbox': {'x1': 0.7544722222222222, 'y1': 0.2563627797979798, 'x2': 0.8584772081699348, 'y2': 0.28277894141414145}}]}\n",
      "{'type': 'Caption', 'bbox': [0.30322193818933824, 0.3024322509765625, 0.6991014188878677, 0.3166981922496449], 'properties': {'score': 0.7232653498649597, 'page_number': 31}, 'text_representation': 'Table 16: Transformer models for medical image translation\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09121403413660387, 0.3503583318536932, 0.911185733570772, 0.4479120150479403], 'properties': {'score': 0.7533119916915894, 'page_number': 31}, 'text_representation': '• MMTrans (Yan et al., 2022b): The MMtrans (Multi-Modal Medical Image Translation) model is proposed based on the\\nGAN architecture and Swin transformer structure for performing medical image-to-image translation on Magnetic Reso-\\nnance Imaging (MRI). Unlike other image-to-image translation frameworks, MMtrans utilizes the transformer to model\\nlong global dependencies to ensure accurate translation results. Moreover, MMtrans does not require images to be paired\\nand pixel-aligned since it employs SWIN as a registration module adapted for paired and unpaired images, which makes\\nit different from other architectures like Pix2Pix. The remaining modules of GAN use SwinIR as a generator module, and\\nCNN as a discriminator module.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09142741483800551, 0.4602272727272727, 0.910989990234375, 0.557149658203125], 'properties': {'score': 0.790729284286499, 'page_number': 31}, 'text_representation': '• TransCBCT (Chen et al., 2022c): A new architecture called TransCBT is proposed for the purpose of performing ac-\\ncurate radiotherapy by improving the quality of 2D images, specifically cone-beam computed tomography (CBCT), and\\ngenerating synthetic 2D images (sCT) without damaging their structures. TransCBT integrates pure-transformer modeling\\nand convolution approaches to facilitate the extraction of global information and enhances performance by introducing the\\nmulti-head self-attention method (SW-MSA). Another model that can improve the quality of CT images reconstructed via\\nsinograms is the CCTR (Shi et al., 2022a). In comparison to TransCBCT, CCTR experiments utilized a lung image database\\nwith 1010 patients, rather than the 91 patients used in TransCBCT.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09405165728400736, 0.5674092240767046, 0.26417247099034924, 0.5815322043678978], 'properties': {'score': 0.6000348925590515, 'page_number': 31}, 'text_representation': '6.4 MULTI-MODALITY\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09407391716452206, 0.5871463844992898, 0.9119154268152574, 0.6835124622691762], 'properties': {'score': 0.9300796389579773, 'page_number': 31}, 'text_representation': 'The transformer has demonstrated its potential in multi-modality, which stems from the human ability to perceive and process\\ninformation from various senses such as vision, hearing, and language. Multi-modality machine learning models are capable\\nof processing and combining different types of data simultaneously. Natural language, vision, and speech are among the\\nmost common types of data handled by multi-modal models. Several popular tasks in multi-modality include visual question\\nanswering, classification and segmentation, visual captioning, commonsense reasoning, and text/image/video/speech gener-\\nation. In this section, we present a selection of transformer-based multi-modal models for each of these tasks providing an\\noverview of their key features and working methods.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09433327169979319, 0.6933128218217329, 0.37146347943474267, 0.7068635697798296], 'properties': {'score': 0.6850249767303467, 'page_number': 31}}\n",
      "{'type': 'Text', 'bbox': [0.09415957282571231, 0.7102805952592329, 0.9113211598115809, 0.779732666015625], 'properties': {'score': 0.9315009713172913, 'page_number': 31}, 'text_representation': '6.4.1 VISUAL QUESTION ANSWERING\\nVisual question answering is a popular task that can be accomplished using multi-modal models. It involves combining NLP\\nand computer vision to answer questions about an image or video. The goal is to understand the features of both textual and\\nvisual information and provide the correct answer. Typically, the models take an image or video and text as input and deliver\\ntext as output answers (Antol et al., 2015, Shih et al., 2016). In this context, we have identified and discussed the significant\\ntransformer models for visual question-answering tasks in Table 17.\\n'}\n",
      "{'type': 'table', 'bbox': [0.05240428251378677, 0.09713184703480114, 0.9450442325367647, 0.7274024547230113], 'properties': {'score': 0.7448669075965881, 'title': None, 'columns': None, 'rows': None, 'page_number': 32}, 'text_representation': 'Transformer\\nModels\\n Processed\\nData\\ntype (i/o)\\n Task Ac-\\ncomplished\\n Year\\n Architecture\\n(Encoder/\\nDecoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training\\nDataset\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n BERT-\\nVerients\\n(Huang et al.,\\n2020, Tan\\n& Bansal,\\n2019, Lu\\net al., 2019,\\nSu et al.,\\n2020, Chen\\net al., 2020c)\\n Text and\\nImage\\n Question\\nAnswering,\\nCommon sense\\nreasoning\\n 2019-\\n2020\\n Encoder\\n Yes\\n VIOLET (Fu\\net al., 2021)\\n Video\\nand Text\\n GIT (Wang\\net al., 2022a)\\n Image\\nand Text\\n SIMVLM\\n(Wang et al.,\\n2022d)\\n Image\\nand Text\\n BLIP (Li\\net al., 2022)\\n Image,\\nVideo\\nand Text\\n Video Question\\nAnswering,\\nText-to-video\\nretrieval,\\nVisual-Text\\nMatching\\n Image\\nClassification,\\nImage/video\\ncaptioning,\\nQuestion\\nanswering\\nVisual\\nQuestion\\nanswering,\\nimage\\ncaptioning\\nQuestion\\nAnswering,\\nImage\\nCaptioning,\\nimage-text\\nretrieval\\n 2022\\n Encoder\\n Yes\\n 2022\\n Encoder &\\nDecoder\\n 2022\\n Encoder &\\nDecoder\\n 2022\\n Encoder &\\nDecoder\\n Yes\\n Yes\\n Yes\\n Pixel-BERT:\\nMS-COCO,\\nVisual Genome\\nLX-MERT: MS\\nCOCO,Visual\\nGenome,VQA\\nv2.0,GQA,VG-\\nQA ViLBERT:\\nVisual Genome,\\nCOCO VL-\\nBERT: Concep-\\ntual Captions,\\nBooksCor-\\npus, English\\nWikipedia\\nUniter: COCO,\\nVG, CC, SBU\\n Conceptual\\nCaptions-3M,\\nWebVid-\\n2.5M, YT-\\nTemporal-180M\\n combination of\\nCOCO, SBU,\\nCC3M, VG,\\nGITL, ALT200M\\nand CC12M\\n ALIGN &\\nColossal Clean\\nCrawled Corpus\\n(C4) datasets\\n Bootstrapped\\ndataset-\\nCOCO, VG,\\nSBU, CC3M,\\nCC12M, LAION\\n Pixel-BERT: VQA 2.0\\nNLVR2, Flickr30K MS-\\nCOCO LX-MERT:\\nVQA,GQA,NLVR\\nViLBERT: Con-\\nceptual Captions,\\nFlickr30k VL-BERT:\\nVCR dataset, Re-\\nfCOCO Uniter:\\nCOCO, Flickr30K,\\nVG, CC, SBU\\n MSRVTT, DiDeMo,\\nYouCook2, LSMDC,\\nTGIF-Action, TGI-\\nTransition, TGIF-\\nFrame, MSRVTT-\\nMC, MSRVTT-QA,\\nMSVD-QA, LSMDC-\\nMC, LSMDC-FiB\\n Karpathy split-\\nCOCO, Flickr30K,\\nno caps, TextCaps,\\nVizWiz-Captions,\\nCUTE, TextOCR\\n SNLI-VE, SNLI,\\nMNLI, Multi30k,\\n10% ALIGN , CC-3M\\n COCO, Flickr30K,\\nNoCaps, MSRVTT\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e024e860>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06662091503267974, 'y1': 0.10662826186868693, 'x2': 0.15542154084967322, 'y2': 0.13304442348484843}}, {'text': 'Processed\\nData\\ntype (i/o)\\n', 'bbox': {'x1': 0.17805228758169936, 'y1': 0.099709069949495, 'x2': 0.24647190816993464, 'y2': 0.13996235277777763}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.2787908496732026, 'y1': 0.10662826186868693, 'x2': 0.3574823666666667, 'y2': 0.13304442348484843}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.11354619116161614, 'x2': 0.4218124794117647, 'y2': 0.12612523156565647}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\n', 'bbox': {'x1': 0.44187908496732026, 'y1': 0.099709069949495, 'x2': 0.529882051633987, 'y2': 0.13996235277777763}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.5485294117647059, 'y1': 0.099709069949495, 'x2': 0.6118212235294118, 'y2': 0.13996235277777763}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.6501911764705882, 'y1': 0.10662826186868693, 'x2': 0.7358011656862745, 'y2': 0.13304442348484843}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7784346405228758, 'y1': 0.10662826186868693, 'x2': 0.925838795751634, 'y2': 0.13304442348484843}}, {'text': 'BERT-\\nVerients\\n(Huang et al.,\\n2020, Tan\\n& Bansal,\\n2019, Lu\\net al., 2019,\\nSu et al.,\\n2020, Chen\\net al., 2020c)\\n', 'bbox': {'x1': 0.06671895424836602, 'y1': 0.19024156767676761, 'x2': 0.15532423496732028, 'y2': 0.3273534363636358}}, {'text': 'Text and\\nImage\\n', 'bbox': {'x1': 0.18424673202614378, 'y1': 0.24558878989898988, 'x2': 0.24027821764705884, 'y2': 0.2720049515151515}}, {'text': 'Question\\nAnswering,\\nCommon sense\\nreasoning\\n', 'bbox': {'x1': 0.26816830065359476, 'y1': 0.23175293131313132, 'x2': 0.36810359705882356, 'y2': 0.28584207272727263}}, {'text': '2019-\\n2020\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.24558878989898988, 'x2': 0.4272495846405229, 'y2': 0.2720049515151515}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.45644444444444443, 'y1': 0.2525079818181818, 'x2': 0.5106852666666666, 'y2': 0.2650870222222223}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5683333333333334, 'y1': 0.2525079818181818, 'x2': 0.5920189264705882, 'y2': 0.2650870222222223}}, {'text': 'VIOLET (Fu\\net al., 2021)\\n', 'bbox': {'x1': 0.06829738562091503, 'y1': 0.41905469898989894, 'x2': 0.15374458725490198, 'y2': 0.44547086060606056}}, {'text': 'Video\\nand Text\\n', 'bbox': {'x1': 0.18424673202614378, 'y1': 0.41905469898989894, 'x2': 0.24027821764705884, 'y2': 0.44547086060606056}}, {'text': 'GIT (Wang\\net al., 2022a)\\n', 'bbox': {'x1': 0.06852614379084966, 'y1': 0.5164170727272727, 'x2': 0.1535175401960784, 'y2': 0.5428332343434343}}, {'text': 'Image\\nand Text\\n', 'bbox': {'x1': 0.18424673202614378, 'y1': 0.5164170727272727, 'x2': 0.24027821764705878, 'y2': 0.5428332343434343}}, {'text': 'SIMVLM\\n(Wang et al.,\\n2022d)\\n', 'bbox': {'x1': 0.06963235294117648, 'y1': 0.586105204040404, 'x2': 0.15240983823529414, 'y2': 0.6263584868686868}}, {'text': 'Image\\nand Text\\n', 'bbox': {'x1': 0.18424673202614378, 'y1': 0.5930243959595959, 'x2': 0.24027821764705878, 'y2': 0.6194392949494949}}, {'text': 'BLIP (Li\\net al., 2022)\\n', 'bbox': {'x1': 0.07213888888888889, 'y1': 0.6696304565656566, 'x2': 0.14990251666666665, 'y2': 0.6960466181818182}}, {'text': 'Image,\\nVideo\\nand Text\\n', 'bbox': {'x1': 0.18424673202614378, 'y1': 0.6627125272727272, 'x2': 0.24027821764705884, 'y2': 0.7029645474747473}}, {'text': 'Video Question\\nAnswering,\\nText-to-video\\nretrieval,\\nVisual-Text\\nMatching\\n', 'bbox': {'x1': 0.2673055555555556, 'y1': 0.3913804565656565, 'x2': 0.3689664003267974, 'y2': 0.473145103030303}}, {'text': 'Image\\nClassification,\\nImage/video\\ncaptioning,\\nQuestion\\nanswering\\nVisual\\nQuestion\\nanswering,\\nimage\\ncaptioning\\nQuestion\\nAnswering,\\nImage\\nCaptioning,\\nimage-text\\nretrieval\\n', 'bbox': {'x1': 0.2717826797385621, 'y1': 0.48874283030303023, 'x2': 0.36449020751633987, 'y2': 0.7237208606060607}}, {'text': '2022\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.4259738909090909, 'x2': 0.42182875816993465, 'y2': 0.43855293131313133}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.45644444444444443, 'y1': 0.4259738909090909, 'x2': 0.5106852666666666, 'y2': 0.43855293131313133}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5683333333333334, 'y1': 0.4259738909090909, 'x2': 0.5920189264705882, 'y2': 0.43855293131313133}}, {'text': '2022\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.5233362646464647, 'x2': 0.42182875816993465, 'y2': 0.535915305050505}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.448078431372549, 'y1': 0.5164170727272727, 'x2': 0.5190538169934641, 'y2': 0.5428332343434343}}, {'text': '2022\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.5999423252525252, 'x2': 0.42182875816993465, 'y2': 0.6125213656565656}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.448078431372549, 'y1': 0.5930243959595959, 'x2': 0.5190538169934641, 'y2': 0.6194392949494949}}, {'text': '2022\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.6765483858585859, 'x2': 0.42182875816993465, 'y2': 0.6891274262626262}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.448078431372549, 'y1': 0.6696304565656566, 'x2': 0.5190538169934641, 'y2': 0.6960466181818182}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5683333333333334, 'y1': 0.5233362646464647, 'x2': 0.5920189264705882, 'y2': 0.535915305050505}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5683333333333334, 'y1': 0.5999423252525252, 'x2': 0.5920189264705882, 'y2': 0.6125213656565656}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5683333333333334, 'y1': 0.6765483858585859, 'x2': 0.5920189264705882, 'y2': 0.6891274262626262}}, {'text': 'Pixel-BERT:\\nMS-COCO,\\nVisual Genome\\nLX-MERT: MS\\nCOCO,Visual\\nGenome,VQA\\nv2.0,GQA,VG-\\nQA ViLBERT:\\nVisual Genome,\\nCOCO VL-\\nBERT: Concep-\\ntual Captions,\\nBooksCor-\\npus, English\\nWikipedia\\nUniter: COCO,\\nVG, CC, SBU\\n', 'bbox': {'x1': 0.6389836601307191, 'y1': 0.14172422146464644, 'x2': 0.7470097336601308, 'y2': 0.37578272929292833}}, {'text': 'Conceptual\\nCaptions-3M,\\nWebVid-\\n2.5M, YT-\\nTemporal-180M\\n', 'bbox': {'x1': 0.6397647058823529, 'y1': 0.39829964848484845, 'x2': 0.7462277843137255, 'y2': 0.46622717373737377}}, {'text': 'combination of\\nCOCO, SBU,\\nCC3M, VG,\\nGITL, ALT200M\\nand CC12M\\n', 'bbox': {'x1': 0.635874183006536, 'y1': 0.49566202222222217, 'x2': 0.7501185078431373, 'y2': 0.5635882848484849}}, {'text': 'ALIGN &\\nColossal Clean\\nCrawled Corpus\\n(C4) datasets\\n', 'bbox': {'x1': 0.6399934640522875, 'y1': 0.5791872747474748, 'x2': 0.7460007372549019, 'y2': 0.6332764161616161}}, {'text': 'Bootstrapped\\ndataset-\\nCOCO, VG,\\nSBU, CC3M,\\nCC12M, LAION\\n', 'bbox': {'x1': 0.6373807189542483, 'y1': 0.648875406060606, 'x2': 0.7486134735294117, 'y2': 0.7168016686868687}}, {'text': 'Pixel-BERT: VQA 2.0\\nNLVR2, Flickr30K MS-\\nCOCO LX-MERT:\\nVQA,GQA,NLVR\\nViLBERT: Con-\\nceptual Captions,\\nFlickr30k VL-BERT:\\nVCR dataset, Re-\\nfCOCO Uniter:\\nCOCO, Flickr30K,\\nVG, CC, SBU\\n', 'bbox': {'x1': 0.772687908496732, 'y1': 0.18323432247474744, 'x2': 0.931584866993464, 'y2': 0.334271365656565}}, {'text': 'MSRVTT, DiDeMo,\\nYouCook2, LSMDC,\\nTGIF-Action, TGI-\\nTransition, TGIF-\\nFrame, MSRVTT-\\nMC, MSRVTT-QA,\\nMSVD-QA, LSMDC-\\nMC, LSMDC-FiB\\n', 'bbox': {'x1': 0.7793366013071895, 'y1': 0.37754459797979795, 'x2': 0.9249338143790848, 'y2': 0.4869822242424243}}, {'text': 'Karpathy split-\\nCOCO, Flickr30K,\\nno caps, TextCaps,\\nVizWiz-Captions,\\nCUTE, TextOCR\\n', 'bbox': {'x1': 0.7899591503267973, 'y1': 0.49566202222222217, 'x2': 0.914312583986928, 'y2': 0.5635882848484849}}, {'text': 'SNLI-VE, SNLI,\\nMNLI, Multi30k,\\n10% ALIGN , CC-3M\\n', 'bbox': {'x1': 0.7788888888888889, 'y1': 0.586105204040404, 'x2': 0.9253814336601307, 'y2': 0.6263584868686868}}, {'text': 'COCO, Flickr30K,\\nNoCaps, MSRVTT\\n', 'bbox': {'x1': 0.7894787581699346, 'y1': 0.6696304565656566, 'x2': 0.9147926385620915, 'y2': 0.6960466181818182}}]}\n",
      "{'type': 'Text', 'bbox': [0.2313616943359375, 0.7376214044744318, 0.7706406537224265, 0.7515091774680398], 'properties': {'score': 0.4499658942222595, 'page_number': 32}, 'text_representation': 'Table 17: Transformer models for multi-modality - visual question answering task\\n'}\n",
      "{'type': 'Caption', 'bbox': [0.2313616943359375, 0.7376214044744318, 0.7706406537224265, 0.7515091774680398], 'properties': {'score': 0.41229498386383057, 'page_number': 32}, 'text_representation': 'Table 17: Transformer models for multi-modality - visual question answering task\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09046515969669118, 0.7719351473721591, 0.9104579790900735, 0.8266889537464489], 'properties': {'score': 0.43003714084625244, 'page_number': 32}, 'text_representation': '• BERT-Variants: Following the successful application of BERT-based models in NLP and computer vision tasks, several\\nBERT-based models have demonstrated significant improvements in multi-modal tasks, particularly in question answering\\nand commonsense reasoning. Currently, there are two distinct types of BERT-based models available in the literature: (i)\\nSingle-Stream Models and (ii) Two-Stream Models.\\nSingle-Stream Models, such as VL-BERT, Uniter, etc., encode both modalities (text and image) within the same module.\\nIn contrast, Two-Stream Models, such as VilBERT, LXMERT, etc., process text and image through separate modules. Both\\ntypes of models have been shown to yield promising results in various multi-modal tasks.\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09046515969669118, 0.7719351473721591, 0.9104579790900735, 0.8266889537464489], 'properties': {'score': 0.35613399744033813, 'page_number': 32}, 'text_representation': '• BERT-Variants: Following the successful application of BERT-based models in NLP and computer vision tasks, several\\nBERT-based models have demonstrated significant improvements in multi-modal tasks, particularly in question answering\\nand commonsense reasoning. Currently, there are two distinct types of BERT-based models available in the literature: (i)\\nSingle-Stream Models and (ii) Two-Stream Models.\\nSingle-Stream Models, such as VL-BERT, Uniter, etc., encode both modalities (text and image) within the same module.\\nIn contrast, Two-Stream Models, such as VilBERT, LXMERT, etc., process text and image through separate modules. Both\\ntypes of models have been shown to yield promising results in various multi-modal tasks.\\n'}\n",
      "{'type': 'Text', 'bbox': [0.10499751371495863, 0.8293120783025568, 0.9097605985753676, 0.8708410089666193], 'properties': {'score': 0.5511401295661926, 'page_number': 32}}\n",
      "{'type': 'List-item', 'bbox': [0.09114226397346048, 0.8769743763316762, 0.9103360523897058, 0.9458710271661932], 'properties': {'score': 0.452614963054657, 'page_number': 32}, 'text_representation': '• ViLBERT: ViLBERT is a two-stream model that is trained on text-image pairs and then passed both of the modules\\nthrough co-attention, which helps to detect the important features of both text and images (Lu et al., 2019). VLBERT, on\\nthe other hand, is a single-stream model that is pre-trained and takes both the image and text embedding features as input,\\nmaking this model simple yet powerful (Su et al., 2020). Uniter represents Universal Image-Text Representation, which is\\na large-scale pre-trained model completed through masking (Chen et al., 2020c). Pixel-BERT is built using a combination\\n'}\n",
      "{'type': 'Text', 'bbox': [0.10560628554400275, 0.10059496792879971, 0.9117287310431985, 0.18406516335227271], 'properties': {'score': 0.8959144949913025, 'page_number': 33}, 'text_representation': 'of convolutional neural network (CNN) to extract image pixels and an encoder to extract text tokens, while the BERT-based\\ntransformer works as the cross-modality module. To capture all the spatial information from the image, Pixel-BERT takes\\nthe whole image as input, whereas other models extract image features from the regions (Huang et al., 2020). Finally,\\nLXMERT stands for Learning Cross-Modality Encoder Representations from Transformers. It processes the image and\\ntext through two different modules and is built with three encoders. This pre-trained model follows masked modeling and\\ncross-modality for pre-training, which captures better relationships between text and images (Tan & Bansal, 2019).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09152513391831342, 0.1885363214666193, 0.9111711569393383, 0.30004813454367896], 'properties': {'score': 0.8488916158676147, 'page_number': 33}, 'text_representation': '• VIOLET: VIOLET is a neural network based on the transformer architecture that is designed to associate video with text.\\nIt consists of three modules: (1) Video Swin Transformer (VT), (2) Language Embedder (LE), and (3) Cross-modal Trans-\\nformer (CT). VIOLET differs from other models in that it processes the temporal-spatial information from the video, rather\\nthan just extracting static images. This model has been evaluated on 12 datasets and has shown outstanding performance\\nin many downstream tasks, such as Text-To-Video Retrieval and Video Question Answering. Furthermore, the authors pro-\\npose a new pre-training task called ”Masked Visual Token Modeling,” which is used to pre-train VIOLET. This approach\\nis combined with two other pre-training approaches, Masked Language Modeling and Visual-Text Matching, to achieve\\nstate-of-the-art performance on various benchmarks (Fu et al., 2021).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09176477768841912, 0.3045394065163352, 0.9114962948069852, 0.37422956986860795], 'properties': {'score': 0.8480302095413208, 'page_number': 33}, 'text_representation': '• GIT: The Generative Image-to-Text Transformer, or GIT, is a multi-modal model designed to generate textual descriptions\\nfrom visual images. This model employs both the encoder and decoder modules of the transformer architecture, using the\\nimage for encoding and decoding the text. To train the model, a large dataset of images paired with textual descriptions\\nis used, allowing GIT to generate textual descriptions for previously unseen images. This approach has shown promising\\nresults in generating high-quality textual descriptions of images (Wang et al., 2022a).\\n'}\n",
      "{'type': 'Text', 'bbox': [0.10551071166992188, 0.39060366543856534, 0.9112907858455882, 0.4183736905184659], 'properties': {'score': 0.8915897607803345, 'page_number': 33}, 'text_representation': 'SimVLM & BLIP: Both SimVLM and BLIP are models that can perform the task of visual captioning, which involves\\ngenerating textual descriptions of visual images. The highlights of these models can be found in that section.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09425886266371783, 0.4278744229403409, 0.39721446317784925, 0.44125701904296877], 'properties': {'score': 0.713283121585846, 'page_number': 33}}\n",
      "{'type': 'Text', 'bbox': [0.09413537418141085, 0.4449550281871449, 0.913174258961397, 0.5835418701171875], 'properties': {'score': 0.9182470440864563, 'page_number': 33}, 'text_representation': '6.4.2 CLASSIFICATION & SEGMENTATION\\nMulti-modal classification and segmentation are often considered related tasks that involve classifying or segmenting data\\nbased on multiple modalities, such as text, image/video, and speech. As segmentation often helps to classify the image,\\ntext, or speech. In multi-modal classification, the task is to classify data based on its similarity and features using multiple\\nmodalities. This can involve taking text, image/video, or speech as input and using all of these modalities to classify the data\\nmore accurately. Similarly, in multi-modal segmentation, the task is to segment data based on its features and use multiple\\nmodalities to achieve a more accurate segmentation. Both of these tasks require a deep understanding of the different forms\\nof data and how they can be used together to achieve better classification or segmentation performance (Liu et al., 2022c,\\nMahesh & Renjit, 2020, Wu et al., 2016, Menze et al., 2015). In recent years, transformer models have shown promising\\nresults in multi-modal classification and segmentation tasks. In Table 18, we highlight some of the significant transformer\\nmodels that have been developed for these tasks.\\n'}\n",
      "{'type': 'table', 'bbox': [0.0523597313376034, 0.6026899303089489, 0.9446458524816177, 0.8447150767933239], 'properties': {'score': 0.7119297385215759, 'title': None, 'columns': None, 'rows': None, 'page_number': 33}, 'text_representation': 'Transformer\\nModels\\n Processed\\nData\\ntype (i/o)\\n Task Ac-\\ncomplished\\n Year\\n Architecture\\n(Encoder/\\nDecoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training\\nDataset\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n CLIP\\n(Radford\\net al., 2021)\\n Image\\nand Text\\n image\\nClassification\\n 2021\\n Encoder\\n Yes\\n Pre-training\\ndataset from\\ninternet for CLIP\\n VATT\\n(Akbari\\net al., 2021)\\n Video,\\nAudio\\nand Text\\n Audio event\\nclassification,\\nImage\\nclassification,\\nVideo action\\nrecognition,\\nText-To-Video\\nretrieval\\n 2021\\n Encoder\\n Yes\\n AudioSet,\\nHowTo100M\\n ImageNet, ImageNet\\nV2, ImageNet Rendi-\\ntion, ObjectNet, Ima-\\ngeNet Sketch, ImageNet\\nAdversarial 30 datasets\\n UCF10,HMDB5,\\nKinetics-400, Kinetics-\\n600,Moments in Time,\\nESC50, AudioSet,\\nYouCook2, MSR-\\nVTT, ImageNet\\n Continued on next page\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e02848e0>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06662091503267974, 'y1': 0.6126888679292929, 'x2': 0.15542154084967322, 'y2': 0.6391050295454546}}, {'text': 'Processed\\nData\\ntype (i/o)\\n', 'bbox': {'x1': 0.17805228758169936, 'y1': 0.605769676010101, 'x2': 0.24647190816993464, 'y2': 0.6460229588383838}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.2787908496732026, 'y1': 0.6126888679292929, 'x2': 0.3574823666666667, 'y2': 0.6391050295454546}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.6196067972222222, 'x2': 0.4218124794117647, 'y2': 0.6321858376262627}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\n', 'bbox': {'x1': 0.44187908496732026, 'y1': 0.605769676010101, 'x2': 0.529882051633987, 'y2': 0.6460229588383838}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.5485294117647059, 'y1': 0.605769676010101, 'x2': 0.6118212235294118, 'y2': 0.6460229588383838}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.6501911764705882, 'y1': 0.6126888679292929, 'x2': 0.7358011656862745, 'y2': 0.6391050295454546}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7784346405228758, 'y1': 0.6126888679292929, 'x2': 0.925838795751634, 'y2': 0.6391050295454546}}, {'text': 'CLIP\\n(Radford\\net al., 2021)\\n', 'bbox': {'x1': 0.07213888888888888, 'y1': 0.661710002020202, 'x2': 0.14990251666666665, 'y2': 0.7019620222222223}}, {'text': 'Image\\nand Text\\n', 'bbox': {'x1': 0.18424673202614378, 'y1': 0.6686279313131314, 'x2': 0.24027821764705878, 'y2': 0.6950440929292929}}, {'text': 'image\\nClassification\\n', 'bbox': {'x1': 0.27381699346405225, 'y1': 0.6686279313131314, 'x2': 0.3624548316993464, 'y2': 0.6950440929292929}}, {'text': '2021\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.6755458606060605, 'x2': 0.42182875816993465, 'y2': 0.688124901010101}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.45644444444444443, 'y1': 0.6755458606060605, 'x2': 0.5106852666666666, 'y2': 0.688124901010101}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5683333333333334, 'y1': 0.6755458606060605, 'x2': 0.5920189264705882, 'y2': 0.688124901010101}}, {'text': 'Pre-training\\ndataset from\\ninternet for CLIP\\n', 'bbox': {'x1': 0.6369330065359478, 'y1': 0.661710002020202, 'x2': 0.7490610928104575, 'y2': 0.7019620222222223}}, {'text': 'VATT\\n(Akbari\\net al., 2021)\\n', 'bbox': {'x1': 0.07213888888888889, 'y1': 0.7521531838383838, 'x2': 0.14990251666666665, 'y2': 0.7924064666666667}}, {'text': 'Video,\\nAudio\\nand Text\\n', 'bbox': {'x1': 0.1842467320261438, 'y1': 0.7521531838383838, 'x2': 0.24027821764705884, 'y2': 0.7924064666666667}}, {'text': 'Audio event\\nclassification,\\nImage\\nclassification,\\nVideo action\\nrecognition,\\nText-To-Video\\nretrieval\\n', 'bbox': {'x1': 0.27069117647058827, 'y1': 0.7175610121212121, 'x2': 0.3655800578431373, 'y2': 0.8269986383838385}}, {'text': '2021\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.765990305050505, 'x2': 0.42182875816993465, 'y2': 0.7785693454545455}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.45644444444444443, 'y1': 0.765990305050505, 'x2': 0.5106852666666666, 'y2': 0.7785693454545455}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5683333333333334, 'y1': 0.765990305050505, 'x2': 0.5920189264705882, 'y2': 0.7785693454545455}}, {'text': 'AudioSet,\\nHowTo100M\\n', 'bbox': {'x1': 0.6495408496732026, 'y1': 0.7590711131313131, 'x2': 0.7364531395424837, 'y2': 0.7854872747474747}}, {'text': 'ImageNet, ImageNet\\nV2, ImageNet Rendi-\\ntion, ObjectNet, Ima-\\ngeNet Sketch, ImageNet\\nAdversarial 30 datasets\\n', 'bbox': {'x1': 0.7724264705882352, 'y1': 0.6478728808080808, 'x2': 0.9318443493464053, 'y2': 0.7157991434343435}}, {'text': 'UCF10,HMDB5,\\nKinetics-400, Kinetics-\\n600,Moments in Time,\\nESC50, AudioSet,\\nYouCook2, MSR-\\nVTT, ImageNet\\n', 'bbox': {'x1': 0.7761797385620914, 'y1': 0.7313981333333334, 'x2': 0.9280931098039215, 'y2': 0.8131615171717171}}, {'text': 'Continued on next page\\n', 'bbox': {'x1': 0.7783480392156863, 'y1': 0.8292630323232324, 'x2': 0.9331915869281047, 'y2': 0.8418420727272727}}]}\n",
      "{'type': 'table', 'bbox': [0.05270972756778493, 0.10510948181152344, 0.944995907054228, 0.7416714200106534], 'properties': {'score': 0.8041775226593018, 'title': None, 'columns': None, 'rows': None, 'page_number': 34}, 'text_representation': 'Year\\n Architecture\\n(Encoder/\\nDecoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training\\nDataset\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n 2020\\n Encoder\\n Yes\\n Conceptual\\nCaptions-3M,\\nSBU Captions\\n MSCOCO, Flickr30K\\n Transformer\\nModels\\n Processed\\nData\\ntype (i/o)\\n Unicoder-VL\\n(Li et al.,\\n2020a)\\n Image\\nand Text\\n ViLT (Kim\\net al., 2021b)\\n Image\\nand Text\\n Task Ac-\\ncomplished\\n Object\\nClassification,\\nVisual-\\nlinguistic\\nMatching,\\nvisual\\ncommonsense\\nreasoning,\\nimage-text\\nretrieval\\nVisual\\nQuestion\\nAnswering,\\nImage text\\nmatching,\\nNatural\\nLanguage\\nfor Visual\\nReasoning\\n 2021\\n Encoder\\n Yes\\n MBT\\n(Nagrani\\net al., 2021)\\n Audio and\\nVisual\\n Audio-visual\\nclassification\\n 2022\\n Encoder\\n Yes\\n ALIGN (Jia\\net al., 2021)\\n Image\\nand Text\\n Visual\\nClassification\\n 2021\\n Encoder\\n Yes\\n MS-\\nCOCO,Visual\\nGenome, SBU\\nCaptions, Google\\nConceptual\\nCaptions\\n VGGSoun,\\nKinetics400\\nand AS-500K,\\nVGGSound\\nALIGN training\\ndata, 0%\\nrandomly\\nsampled ALIGN\\ntraining data,\\nand CC-3M\\n FLD-900M,\\nImageNet (Swin\\ntransformer\\n& CLIP)\\n VQA 2.0, NLVR2,\\nMSCOCO, Flickr30K\\n Audioset-mini and\\nVGGSound, Moments\\nIn Time, Kinetics\\n Conceptual Captions-\\nCC, Flickr30K,\\nMSCOCO,\\nILSVRC-2012\\n Web-scale by data\\ncuration, UniCL,\\nImageNet, COCO,\\nKinetics-600, Flickr30k,\\nMSCOCO, SR-VTT\\n Florence\\n(Yuan\\net al., 2021)\\n Image\\nand Text\\n GIT (Wang\\net al., 2022a)\\n Image\\nand Text\\n Classification,\\nimage caption,\\nvisual action\\nrecognition,\\nText-visual\\n& visual-text\\nretrieval\\nImage\\nClassification,\\nImage/video\\ncaptioning,\\nQuestion\\nanswering\\n 2021\\n Encoder\\n Yes\\n 2022\\n Encoder &\\nDecoder\\n Yes\\n combination of\\nCOCO, SBU,\\nCC3M, VG,\\nGITL, ALT200M\\nand CC12M\\n Karpathy split-\\nCOCO, Flickr30K,\\nno caps, TextCaps,\\nVizWiz-Captions,\\nCUTE, TextOCR\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e0285b10>, 'tokens': [{'text': 'Year\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.12738331237373743, 'x2': 0.4218124794117647, 'y2': 0.13996235277777777}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\n', 'bbox': {'x1': 0.44187908496732026, 'y1': 0.11354619116161614, 'x2': 0.529882051633987, 'y2': 0.15379947398989877}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.5485294117647059, 'y1': 0.11354619116161614, 'x2': 0.6118212235294118, 'y2': 0.15379947398989877}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.6501911764705882, 'y1': 0.12046538308080808, 'x2': 0.7358011656862745, 'y2': 0.14688154469696957}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7784346405228758, 'y1': 0.12046538308080808, 'x2': 0.925838795751634, 'y2': 0.14688154469696957}}, {'text': '2020\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.21791581010101005, 'x2': 0.42182875816993465, 'y2': 0.23049485050505053}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.45644444444444443, 'y1': 0.21791581010101005, 'x2': 0.5106852666666666, 'y2': 0.23049485050505053}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5683333333333334, 'y1': 0.21791581010101005, 'x2': 0.5920189264705882, 'y2': 0.23049485050505053}}, {'text': 'Conceptual\\nCaptions-3M,\\nSBU Captions\\n', 'bbox': {'x1': 0.6462679738562092, 'y1': 0.2040786888888889, 'x2': 0.739724324509804, 'y2': 0.24433197171717166}}, {'text': 'MSCOCO, Flickr30K\\n', 'bbox': {'x1': 0.7802320261437908, 'y1': 0.21791581010101005, 'x2': 0.9240385758169937, 'y2': 0.23049485050505053}}, {'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06662091503267974, 'y1': 0.12046538308080808, 'x2': 0.15542154084967322, 'y2': 0.14688154469696957}}, {'text': 'Processed\\nData\\ntype (i/o)\\n', 'bbox': {'x1': 0.17805228758169936, 'y1': 0.11354619116161614, 'x2': 0.24647190816993464, 'y2': 0.15379947398989877}}, {'text': 'Unicoder-VL\\n(Li et al.,\\n2020a)\\n', 'bbox': {'x1': 0.06733660130718955, 'y1': 0.2040786888888889, 'x2': 0.15470469640522877, 'y2': 0.24433197171717166}}, {'text': 'Image\\nand Text\\n', 'bbox': {'x1': 0.18424673202614378, 'y1': 0.21099661818181809, 'x2': 0.24027821764705878, 'y2': 0.23741277979797973}}, {'text': 'ViLT (Kim\\net al., 2021b)\\n', 'bbox': {'x1': 0.0680702614379085, 'y1': 0.3429511636363636, 'x2': 0.15397326830065358, 'y2': 0.36936732525252525}}, {'text': 'Image\\nand Text\\n', 'bbox': {'x1': 0.18424673202614378, 'y1': 0.3429511636363636, 'x2': 0.24027821764705878, 'y2': 0.36936732525252525}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.2787908496732026, 'y1': 0.12046538308080808, 'x2': 0.3574823666666667, 'y2': 0.14688154469696957}}, {'text': 'Object\\nClassification,\\nVisual-\\nlinguistic\\nMatching,\\nvisual\\ncommonsense\\nreasoning,\\nimage-text\\nretrieval\\nVisual\\nQuestion\\nAnswering,\\nImage text\\nmatching,\\nNatural\\nLanguage\\nfor Visual\\nReasoning\\n', 'bbox': {'x1': 0.2717826797385621, 'y1': 0.15564939595959598, 'x2': 0.36449020751633987, 'y2': 0.417796618181818}}, {'text': '2021\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.3498703555555555, 'x2': 0.42182875816993465, 'y2': 0.36244939595959597}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.45644444444444443, 'y1': 0.3498703555555555, 'x2': 0.5106852666666666, 'y2': 0.36244939595959597}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5683333333333334, 'y1': 0.3498703555555555, 'x2': 0.5920189264705882, 'y2': 0.36244939595959597}}, {'text': 'MBT\\n(Nagrani\\net al., 2021)\\n', 'bbox': {'x1': 0.07213888888888889, 'y1': 0.4264764161616162, 'x2': 0.14990251666666665, 'y2': 0.46672969898989897}}, {'text': 'Audio and\\nVisual\\n', 'bbox': {'x1': 0.17812581699346405, 'y1': 0.4333956080808081, 'x2': 0.24639892875816993, 'y2': 0.4598117696969697}}, {'text': 'Audio-visual\\nclassification\\n', 'bbox': {'x1': 0.2756323529411765, 'y1': 0.4333956080808081, 'x2': 0.36064002810457524, 'y2': 0.4598117696969697}}, {'text': '2022\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.4403135373737373, 'x2': 0.42182875816993465, 'y2': 0.45289257777777775}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.45644444444444443, 'y1': 0.4403135373737373, 'x2': 0.5106852666666666, 'y2': 0.45289257777777775}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5683333333333334, 'y1': 0.4403135373737373, 'x2': 0.5920189264705882, 'y2': 0.45289257777777775}}, {'text': 'ALIGN (Jia\\net al., 2021)\\n', 'bbox': {'x1': 0.07191993464052288, 'y1': 0.5030837393939395, 'x2': 0.1501230888888889, 'y2': 0.529499901010101}}, {'text': 'Image\\nand Text\\n', 'bbox': {'x1': 0.18424673202614378, 'y1': 0.5030837393939395, 'x2': 0.24027821764705878, 'y2': 0.529499901010101}}, {'text': 'Visual\\nClassification\\n', 'bbox': {'x1': 0.27381699346405225, 'y1': 0.5030837393939395, 'x2': 0.3624548316993464, 'y2': 0.529499901010101}}, {'text': '2021\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.5100016686868688, 'x2': 0.42182875816993465, 'y2': 0.5225807090909091}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.45644444444444443, 'y1': 0.5100016686868688, 'x2': 0.5106852666666666, 'y2': 0.5225807090909091}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5683333333333334, 'y1': 0.5100016686868688, 'x2': 0.5920189264705882, 'y2': 0.5225807090909091}}, {'text': 'MS-\\nCOCO,Visual\\nGenome, SBU\\nCaptions, Google\\nConceptual\\nCaptions\\n', 'bbox': {'x1': 0.6360212418300654, 'y1': 0.31527692121212114, 'x2': 0.7499725490196079, 'y2': 0.3970415676767675}}, {'text': 'VGGSoun,\\nKinetics400\\nand AS-500K,\\nVGGSound\\nALIGN training\\ndata, 0%\\nrandomly\\nsampled ALIGN\\ntraining data,\\nand CC-3M\\n', 'bbox': {'x1': 0.6385196078431372, 'y1': 0.41955848686868685, 'x2': 0.7474733362745096, 'y2': 0.5571728808080808}}, {'text': 'FLD-900M,\\nImageNet (Swin\\ntransformer\\n& CLIP)\\n', 'bbox': {'x1': 0.6394232026143791, 'y1': 0.5796898, 'x2': 0.7465699888888888, 'y2': 0.633780204040404}}, {'text': 'VQA 2.0, NLVR2,\\nMSCOCO, Flickr30K\\n', 'bbox': {'x1': 0.7802320261437908, 'y1': 0.3429511636363636, 'x2': 0.9240385758169937, 'y2': 0.36936732525252525}}, {'text': 'Audioset-mini and\\nVGGSound, Moments\\nIn Time, Kinetics\\n', 'bbox': {'x1': 0.7790032679738562, 'y1': 0.4264764161616162, 'x2': 0.925267910130719, 'y2': 0.46672969898989897}}, {'text': 'Conceptual Captions-\\nCC, Flickr30K,\\nMSCOCO,\\nILSVRC-2012\\n', 'bbox': {'x1': 0.7813725490196078, 'y1': 0.48924661818181814, 'x2': 0.9229000725490195, 'y2': 0.5433370222222221}}, {'text': 'Web-scale by data\\ncuration, UniCL,\\nImageNet, COCO,\\nKinetics-600, Flickr30k,\\nMSCOCO, SR-VTT\\n', 'bbox': {'x1': 0.7723300653594771, 'y1': 0.5727718707070707, 'x2': 0.9319432892156863, 'y2': 0.6406981333333334}}, {'text': 'Florence\\n(Yuan\\net al., 2021)\\n', 'bbox': {'x1': 0.07213888888888889, 'y1': 0.5866089919191918, 'x2': 0.14990251666666665, 'y2': 0.6268610121212121}}, {'text': 'Image\\nand Text\\n', 'bbox': {'x1': 0.18424673202614378, 'y1': 0.5935269212121211, 'x2': 0.24027821764705878, 'y2': 0.6199430828282828}}, {'text': 'GIT (Wang\\net al., 2022a)\\n', 'bbox': {'x1': 0.06852614379084966, 'y1': 0.6839701030303029, 'x2': 0.1535175401960784, 'y2': 0.7103862646464646}}, {'text': 'Image\\nand Text\\n', 'bbox': {'x1': 0.18424673202614378, 'y1': 0.6839701030303029, 'x2': 0.24027821764705878, 'y2': 0.7103862646464646}}, {'text': 'Classification,\\nimage caption,\\nvisual action\\nrecognition,\\nText-visual\\n& visual-text\\nretrieval\\nImage\\nClassification,\\nImage/video\\ncaptioning,\\nQuestion\\nanswering\\n', 'bbox': {'x1': 0.27021241830065357, 'y1': 0.5589347494949496, 'x2': 0.36606174640522865, 'y2': 0.738060507070707}}, {'text': '2021\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.6004461131313131, 'x2': 0.42182875816993465, 'y2': 0.6130251535353535}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.45644444444444443, 'y1': 0.6004461131313131, 'x2': 0.5106852666666666, 'y2': 0.6130251535353535}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5683333333333334, 'y1': 0.6004461131313131, 'x2': 0.5920189264705882, 'y2': 0.6130251535353535}}, {'text': '2022\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.6908892949494949, 'x2': 0.42182875816993465, 'y2': 0.7034683353535354}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.448078431372549, 'y1': 0.6839701030303029, 'x2': 0.5190538169934641, 'y2': 0.7103862646464646}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5683333333333334, 'y1': 0.6908892949494949, 'x2': 0.5920189264705882, 'y2': 0.7034683353535354}}, {'text': 'combination of\\nCOCO, SBU,\\nCC3M, VG,\\nGITL, ALT200M\\nand CC12M\\n', 'bbox': {'x1': 0.635874183006536, 'y1': 0.6632150525252525, 'x2': 0.7501185078431373, 'y2': 0.7311425777777778}}, {'text': 'Karpathy split-\\nCOCO, Flickr30K,\\nno caps, TextCaps,\\nVizWiz-Captions,\\nCUTE, TextOCR\\n', 'bbox': {'x1': 0.7899591503267973, 'y1': 0.6632150525252525, 'x2': 0.914312583986928, 'y2': 0.7311425777777778}}]}\n",
      "{'type': 'Caption', 'bbox': [0.23354905072380513, 0.7519239390980114, 0.7673731186810662, 0.7654161487926137], 'properties': {'score': 0.4550057649612427, 'page_number': 34}, 'text_representation': 'Table 18: Multi-modal Transformer models - classification & segmentation tasks\\n'}\n",
      "{'type': 'Text', 'bbox': [0.23354905072380513, 0.7519239390980114, 0.7673731186810662, 0.7654161487926137], 'properties': {'score': 0.44285017251968384, 'page_number': 34}, 'text_representation': 'Table 18: Multi-modal Transformer models - classification & segmentation tasks\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09039378446691176, 0.7890298184481535, 0.9105994370404412, 0.8718878173828125], 'properties': {'score': 0.4048103392124176, 'page_number': 34}, 'text_representation': '• CLIP: CLIP (Constructive Language-Image Pre-Training) is a multi-modal model that is trained in a supervised way with\\ntext and image data. This model simultaneously trains both the text and image through an encoder and predicts proper\\nbatches of text and image. CLIP is capable of understanding the relationship between text and images, and can generate\\nimages based on input text as well as generate text based on input images. CLIP has been shown to perform well on several\\nbenchmark datasets and is considered a state-of-the-art model for multi-modal tasks involving text and images (Radford\\net al., 2021).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09070790010340074, 0.8795933948863637, 0.9104762178308824, 0.9482307572798295], 'properties': {'score': 0.40120142698287964, 'page_number': 34}, 'text_representation': '• VATT: which stands for Video, Audio, Text Transformer, is a multi-modal model based on the traditional transformer\\narchitecture without convolution layers. It is inspired by BERT and ViT and is pre-trained on two datasets using the Drop\\nToken approach to optimize the training process. VATT is evaluated on 10 datasets containing videos, audio data, and text\\nspeech for 4 downstream tasks: Video action recognition, audio event classification, image classification, and text-to-video\\nretrieval (Akbari et al., 2021).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09166535321403953, 0.10029810818758877, 0.9122874540441176, 0.17048242742365058], 'properties': {'score': 0.7881532311439514, 'page_number': 35}, 'text_representation': '• MBT: Multimodal Bottleneck Transformer (MBT) is a transformer-based model designed for processing both audio and\\nvisual data. The MBT model utilizes a bottleneck structure to focus on essential information for processing through the\\ntransformer architecture, thereby reducing the amount of data processed and minimizing the risk of overfitting. The bottle-\\nneck structure reduces the size of the model, training time, and computational cost. MBT has shown promising results in\\nvarious multimodal tasks, such as audio-visual speech recognition and audio-visual event detection (Nagrani et al., 2021).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09162760117474725, 0.17496267145330255, 0.9114878216911765, 0.24465892444957385], 'properties': {'score': 0.7985180616378784, 'page_number': 35}, 'text_representation': '• ALIGN: it stands for Large-scale ImaGe and Noise-text embedding. This model is a large-scale model that uses vision-\\nlanguage representational learning with noisy text annotations. ALIGN is a pre-trained model which uses a dual-encoder\\nand is trained on huge-sized noisy image-text pair datasets. The dataset scale is able to adjust for noise, eliminating the need\\nfor pre-processing. ALIGN uses the contrastive loss to train the model, considering both image-to-text and text-to-image\\nclassification losses (Jia et al., 2021).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09201002233168658, 0.24926014293323864, 0.9108856560202205, 0.30565898548473014], 'properties': {'score': 0.8304482698440552, 'page_number': 35}, 'text_representation': '• Florence: Florence is a visual-language representation model that is capable of handling multiple tasks. It is an encoder-\\nbased pre-trained model trained on web-scale image-text data, and it can handle high-resolution images. This model shows\\nstrong performance on classification tasks, as well as other tasks like object/action detection and question answering (Yuan\\net al., 2021).\\nUnicoder-VL, GIT & ViLT: Unicoder-VL and ViLT models have been described in the Visual Commonsense Reasoning\\nsection. Both models can perform the Commonsense Reasoning task in addition to other tasks. However, the characteristics\\nof GIT model can be found on the visual question-answering section.\\n'}\n",
      "{'type': 'Text', 'bbox': [0.10571566413430607, 0.30742564808238637, 0.9115183392693015, 0.34973236083984377], 'properties': {'score': 0.808838963508606, 'page_number': 35}}\n",
      "{'type': 'Section-header', 'bbox': [0.09434078440946692, 0.35855213512073864, 0.2964871574850643, 0.3726197676225142], 'properties': {'score': 0.6229409575462341, 'page_number': 35}}\n",
      "{'type': 'Text', 'bbox': [0.09410682229434743, 0.37580394398082384, 0.9127399040670956, 0.45958928888494316], 'properties': {'score': 0.8826121687889099, 'page_number': 35}, 'text_representation': '6.4.3 VISUAL CAPTIONING\\nVisual captioning is a multi-modal task that involves both computer vision and NLP. The task aims to generate a textual\\ndescription of an image, which requires a deep understanding of the relationship between image features and text. The visual\\ncaptioning process usually involves several steps, starting with image processing, followed by encoding the features into\\nvectors that can be used by the NLP model. These encoded vectors are then decoded into text, typically through generative\\nNLP models. Although it is a complex process, visual captioning has a wide range of applications (Yu et al., 2020, Hossain\\net al., 2019). In this section, we discuss significant transformer models for visual captioning tasks (see Table 19).\\n'}\n",
      "{'type': 'table', 'bbox': [0.052852926815257356, 0.47911693226207386, 0.94620361328125, 0.8601543634588068], 'properties': {'score': 0.7119682431221008, 'title': None, 'columns': None, 'rows': None, 'page_number': 35}, 'text_representation': 'Transformer\\nModels\\n BLIP (Li\\net al., 2022)\\n Processed\\nData\\ntype (i/o)\\n Image,\\nVideo\\nand Text\\n SIMVLM\\n(Wang et al.,\\n2022d)\\n Image\\nand Text\\n Florence\\n(Yuan\\net al., 2021)\\n Image\\nand Text\\n GIT (Wang\\net al., 2022a)\\n Image\\nand Text\\n Task Ac-\\ncomplished\\n Year\\n Architecture\\n(Encoder/\\nDecoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training\\nDataset\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n Image\\nCaptioning,\\nQuestion\\nAnswering,\\nimage-text\\nretrieval\\nImage\\ncaptioning,\\nVisual\\nQuestion\\nanswering\\nClassification,\\nimage caption,\\nvisual action\\nrecognition,\\nText-visual\\n& visual-text\\nretrieval\\nImage\\nClassification,\\nImage/video\\ncaptioning,\\nQuestion\\nanswering\\n 2022\\n Encoder &\\nDecoder\\n 2022\\n Encoder &\\nDecoder\\n Yes\\n Yes\\n 2021\\n Encoder\\n Yes\\n 2022\\n Encoder &\\nDecoder\\n Yes\\n Bootstrapped\\ndataset-\\nCOCO, VG,\\nSBU, CC3M,\\nCC12M, LAION\\n ALIGN &\\nColossal Clean\\nCrawled Corpus\\n(C4) datasets\\n COCO, Flickr30K,\\nNoCaps, MSRVTT\\n SNLI-VE, SNLI,\\nMNLI, Multi30k,\\n10% ALIGN , CC-3M\\n FLD-900M,\\nImageNet (Swin\\ntransformer\\n& CLIP)\\n Web-scale by data\\ncuration, UniCL,\\nImageNet, COCO,\\nKinetics-600, Flickr30k,\\nMSCOCO, SR-VTT\\n combination of\\nCOCO, SBU,\\nCC3M, VG,\\nGITL, ALT200M\\nand CC12M\\n Karpathy split-\\nCOCO, Flickr30K,\\nno caps, TextCaps,\\nVizWiz-Captions,\\nCUTE, TextOCR\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e0287f10>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06662091503267974, 'y1': 0.489374473989899, 'x2': 0.15542154084967322, 'y2': 0.5157906356060606}}, {'text': 'BLIP (Li\\net al., 2022)\\n', 'bbox': {'x1': 0.07213888888888889, 'y1': 0.5522327292929292, 'x2': 0.14990251666666665, 'y2': 0.5786488909090909}}, {'text': 'Processed\\nData\\ntype (i/o)\\n', 'bbox': {'x1': 0.17805228758169936, 'y1': 0.4824565446969697, 'x2': 0.24647190816993464, 'y2': 0.5227098275252525}}, {'text': 'Image,\\nVideo\\nand Text\\n', 'bbox': {'x1': 0.18424673202614378, 'y1': 0.5453148, 'x2': 0.24027821764705884, 'y2': 0.5855668202020202}}, {'text': 'SIMVLM\\n(Wang et al.,\\n2022d)\\n', 'bbox': {'x1': 0.06963235294117648, 'y1': 0.6219208606060607, 'x2': 0.15240983823529414, 'y2': 0.6621741434343434}}, {'text': 'Image\\nand Text\\n', 'bbox': {'x1': 0.18424673202614378, 'y1': 0.6288400525252524, 'x2': 0.24027821764705878, 'y2': 0.6552549515151515}}, {'text': 'Florence\\n(Yuan\\net al., 2021)\\n', 'bbox': {'x1': 0.07213888888888889, 'y1': 0.7054461131313131, 'x2': 0.14990251666666665, 'y2': 0.745699395959596}}, {'text': 'Image\\nand Text\\n', 'bbox': {'x1': 0.18424673202614378, 'y1': 0.7123640424242424, 'x2': 0.24027821764705878, 'y2': 0.7387802040404041}}, {'text': 'GIT (Wang\\net al., 2022a)\\n', 'bbox': {'x1': 0.06852614379084966, 'y1': 0.8028084868686868, 'x2': 0.1535175401960784, 'y2': 0.8292246484848484}}, {'text': 'Image\\nand Text\\n', 'bbox': {'x1': 0.18424673202614378, 'y1': 0.8028084868686868, 'x2': 0.24027821764705878, 'y2': 0.8292246484848484}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.2787908496732026, 'y1': 0.489374473989899, 'x2': 0.3574823666666667, 'y2': 0.5157906356060606}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.4962936659090909, 'x2': 0.4218124794117647, 'y2': 0.5088727063131313}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\n', 'bbox': {'x1': 0.44187908496732026, 'y1': 0.4824565446969697, 'x2': 0.529882051633987, 'y2': 0.5227098275252525}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.5485294117647059, 'y1': 0.4824565446969697, 'x2': 0.6118212235294118, 'y2': 0.5227098275252525}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.6501911764705882, 'y1': 0.489374473989899, 'x2': 0.7358011656862745, 'y2': 0.5157906356060606}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7784346405228758, 'y1': 0.489374473989899, 'x2': 0.925838795751634, 'y2': 0.5157906356060606}}, {'text': 'Image\\nCaptioning,\\nQuestion\\nAnswering,\\nimage-text\\nretrieval\\nImage\\ncaptioning,\\nVisual\\nQuestion\\nanswering\\nClassification,\\nimage caption,\\nvisual action\\nrecognition,\\nText-visual\\n& visual-text\\nretrieval\\nImage\\nClassification,\\nImage/video\\ncaptioning,\\nQuestion\\nanswering\\n', 'bbox': {'x1': 0.27021241830065357, 'y1': 0.5245584868686869, 'x2': 0.36606174640522865, 'y2': 0.8568988909090909}}, {'text': '2022\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.5591519212121212, 'x2': 0.42182875816993465, 'y2': 0.5717309616161615}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.448078431372549, 'y1': 0.5522327292929292, 'x2': 0.5190538169934641, 'y2': 0.5786488909090909}}, {'text': '2022\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.6357579818181818, 'x2': 0.42182875816993465, 'y2': 0.6483370222222223}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.448078431372549, 'y1': 0.6288400525252524, 'x2': 0.5190538169934641, 'y2': 0.6552549515151515}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5683333333333334, 'y1': 0.5591519212121212, 'x2': 0.5920189264705882, 'y2': 0.5717309616161615}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5683333333333334, 'y1': 0.6357579818181818, 'x2': 0.5920189264705882, 'y2': 0.6483370222222223}}, {'text': '2021\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.7192832343434343, 'x2': 0.42182875816993465, 'y2': 0.7318622747474747}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.45644444444444443, 'y1': 0.7192832343434343, 'x2': 0.5106852666666666, 'y2': 0.7318622747474747}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5683333333333334, 'y1': 0.7192832343434343, 'x2': 0.5920189264705882, 'y2': 0.7318622747474747}}, {'text': '2022\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.8097264161616161, 'x2': 0.42182875816993465, 'y2': 0.8223054565656565}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.448078431372549, 'y1': 0.8028084868686868, 'x2': 0.5190538169934641, 'y2': 0.8292246484848484}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5683333333333334, 'y1': 0.8097264161616161, 'x2': 0.5920189264705882, 'y2': 0.8223054565656565}}, {'text': 'Bootstrapped\\ndataset-\\nCOCO, VG,\\nSBU, CC3M,\\nCC12M, LAION\\n', 'bbox': {'x1': 0.6373807189542483, 'y1': 0.5314776787878788, 'x2': 0.7486134735294117, 'y2': 0.5994039414141414}}, {'text': 'ALIGN &\\nColossal Clean\\nCrawled Corpus\\n(C4) datasets\\n', 'bbox': {'x1': 0.6399934640522875, 'y1': 0.6150029313131313, 'x2': 0.7460007372549019, 'y2': 0.6690920727272727}}, {'text': 'COCO, Flickr30K,\\nNoCaps, MSRVTT\\n', 'bbox': {'x1': 0.7894787581699346, 'y1': 0.5522327292929292, 'x2': 0.9147926385620915, 'y2': 0.5786488909090909}}, {'text': 'SNLI-VE, SNLI,\\nMNLI, Multi30k,\\n10% ALIGN , CC-3M\\n', 'bbox': {'x1': 0.7788888888888889, 'y1': 0.6219208606060607, 'x2': 0.9253814336601307, 'y2': 0.6621741434343434}}, {'text': 'FLD-900M,\\nImageNet (Swin\\ntransformer\\n& CLIP)\\n', 'bbox': {'x1': 0.6394232026143791, 'y1': 0.6985281838383838, 'x2': 0.7465699888888888, 'y2': 0.7526173252525252}}, {'text': 'Web-scale by data\\ncuration, UniCL,\\nImageNet, COCO,\\nKinetics-600, Flickr30k,\\nMSCOCO, SR-VTT\\n', 'bbox': {'x1': 0.7723300653594771, 'y1': 0.691608991919192, 'x2': 0.9319432892156863, 'y2': 0.7595365171717171}}, {'text': 'combination of\\nCOCO, SBU,\\nCC3M, VG,\\nGITL, ALT200M\\nand CC12M\\n', 'bbox': {'x1': 0.635874183006536, 'y1': 0.7820521737373737, 'x2': 0.7501185078431373, 'y2': 0.8499796989898989}}, {'text': 'Karpathy split-\\nCOCO, Flickr30K,\\nno caps, TextCaps,\\nVizWiz-Captions,\\nCUTE, TextOCR\\n', 'bbox': {'x1': 0.7899591503267973, 'y1': 0.7820521737373737, 'x2': 0.914312583986928, 'y2': 0.8499796989898989}}]}\n",
      "{'type': 'Caption', 'bbox': [0.2801809871897978, 0.8701104181463069, 0.7228746122472427, 0.8848204456676136], 'properties': {'score': 0.5429353713989258, 'page_number': 35}, 'text_representation': 'Table 19: Multi-modal Transformer models - visual captioning task\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09153774485868567, 0.9038835005326704, 0.9113956945082721, 0.9455450994318182], 'properties': {'score': 0.7194331884384155, 'page_number': 35}, 'text_representation': '• BLIP: Bootstrapping Language-Image Pre-training (BLIP) is a pre-trained model designed to enhance performance on var-\\nious tasks through fine-tuning for specific tasks. This model utilizes a VLP (Vision and Language Pre-training) framework\\nwith an encoder-decoder module of the Transformer architecture, which uses noisy data with captions and is trained to\\n'}\n",
      "{'type': 'Text', 'bbox': [0.10556474573471968, 0.10032415910200639, 0.9105681295955882, 0.12881264426491476], 'properties': {'score': 0.8658667802810669, 'page_number': 36}, 'text_representation': 'remove noisy captions. BLIP is capable of performing a range of downstream tasks, including image captioning, question\\nanswering, image-text retrieval, and more (Li et al., 2022).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09189244887408088, 0.13291316639293324, 0.9106490550321691, 0.18970796064897016], 'properties': {'score': 0.7726583480834961, 'page_number': 36}}\n",
      "{'type': 'List-item', 'bbox': [0.09204724480124081, 0.1933404541015625, 0.9104784438189338, 0.24973821466619317], 'properties': {'score': 0.7538342475891113, 'page_number': 36}}\n",
      "{'type': 'Text', 'bbox': [0.1052881487678079, 0.2510590154474432, 0.910484188304228, 0.2804508556019176], 'properties': {'score': 0.8070914149284363, 'page_number': 36}}\n",
      "{'type': 'Section-header', 'bbox': [0.09405204324161305, 0.2883015858043324, 0.4063379624310662, 0.30273520729758524], 'properties': {'score': 0.70156329870224, 'page_number': 36}}\n",
      "{'type': 'Text', 'bbox': [0.09333295036764706, 0.3052603981711648, 0.9121053538602941, 0.4179385098544034], 'properties': {'score': 0.8948543667793274, 'page_number': 36}, 'text_representation': '6.4.4 VISUAL COMMONSENSE REASONING\\nVisual commonsense reasoning is a challenging task that requires a model with a deep understanding of visualization and\\ndifferent images or videos containing objects and scenes, inspired by how humans see and visualize things. These models\\ncapture information from different sub-tasks like object recognition and feature extraction. This information is then trans-\\nformed into a vector to be used for reasoning. The reasoning module understands the relationship between the objects in the\\nimage and the output of the inferencing step provides a prediction about the interaction and relationship between the objects.\\nVisual commonsense reasoning helps to improve the performance of various tasks like classification, image captioning, and\\nother deep understanding-related tasks (Zellers et al., 2019, Xing et al., 2021). In this section, we highlight and discuss\\nsignificant transformer models for visual commonsense reasoning tasks that are summarized in Table 20.\\n'}\n",
      "{'type': 'table', 'bbox': [0.05281740525189568, 0.43733861749822445, 0.9490471335018382, 0.9410502485795454], 'properties': {'score': 0.8052060604095459, 'title': None, 'columns': None, 'rows': None, 'page_number': 36}, 'text_representation': 'Transformer\\nModels\\n Processed\\nData\\ntype(i/o)\\n Task Ac-\\ncomplished\\n Year\\n Architecture\\n(Encoder/\\nDecoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n BERT-\\nVerients\\n(Huang et al.,\\n2020, Tan\\n& Bansal,\\n2019, Lu\\net al., 2019,\\nSu et al.,\\n2020, Chen\\net al., 2020c)\\n Text and\\nImage\\n Question\\nAnswering,\\nCommon\\nsense\\nreasoning\\n 2019-\\n2020\\n Encoder\\n Yes\\n Pre-training Dataset\\n Pixel-BERT: MS-\\nCOCO, Visual Genome\\nLX-MERT: MS\\nCOCO,Visual\\nGenome,VQA\\nv2.0,GQA,VG-QA\\nViLBERT: Visual\\nGenome, COCO\\nVL-BERT: Con-\\nceptual Captions,\\nBooksCorpus, English\\nWikipedia Uniter:\\nCOCO, VG, CC, SBU\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n Pixel-BERT:\\nVQA 2.0 NLVR2,\\nFlickr30K MS-\\nCOCO LX-MERT:\\nVQA,GQA,NLVR\\nViLBERT: Con-\\nceptual Captions,\\nFlickr30k VL-BERT:\\nVCR dataset, Re-\\nfCOCO Uniter:\\nCOCO, Flickr30K,\\nVG, CC, SBU\\n ViLT (Kim\\net al., 2021b)\\n Image\\nand Text\\n Unicode-VL\\n(Li et al.,\\n2020a)\\n Image\\nand Text\\n Visual\\nQuestion\\nAnswering,\\nImage text\\nmatching,\\nNatural\\nLanguage\\nfor Visual\\nReasoning\\nObject Clas-\\nsification,\\nVisual-\\nlinguistic\\nMatching,\\nvisual com-\\nmonsense\\nreasoning,\\nimage-text\\nretrieval\\n 2021\\n Encoder\\n Yes\\n MS-COCO,Visual\\nGenome, SBU\\nCaptions, Google\\nConceptual Captions\\n VQA 2.0, NLVR2,\\nMSCOCO, Flickr30K\\n 2020\\n Encoder\\n Yes\\n Conceptual Captions-\\n3M, SBU Captions\\n MSCOCO, Flickr30K\\n Continued on next page\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e02c9990>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06662091503267974, 'y1': 0.446644676010101, 'x2': 0.15542154084967322, 'y2': 0.47306083762626266}}, {'text': 'Processed\\nData\\ntype(i/o)\\n', 'bbox': {'x1': 0.17752450980392157, 'y1': 0.4397267467171717, 'x2': 0.24594413039215687, 'y2': 0.479978766919192}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.2648954248366013, 'y1': 0.446644676010101, 'x2': 0.34358694183006533, 'y2': 0.47306083762626266}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3707450980392157, 'y1': 0.4535626053030303, 'x2': 0.403286335620915, 'y2': 0.4661416457070707}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\n', 'bbox': {'x1': 0.42335294117647054, 'y1': 0.4397267467171717, 'x2': 0.5113559078431373, 'y2': 0.479978766919192}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.5269101307189541, 'y1': 0.4397267467171717, 'x2': 0.59020194248366, 'y2': 0.479978766919192}}, {'text': 'BERT-\\nVerients\\n(Huang et al.,\\n2020, Tan\\n& Bansal,\\n2019, Lu\\net al., 2019,\\nSu et al.,\\n2020, Chen\\net al., 2020c)\\n', 'bbox': {'x1': 0.06671895424836602, 'y1': 0.5025837393939394, 'x2': 0.15532423496732028, 'y2': 0.6396956080808082}}, {'text': 'Text and\\nImage\\n', 'bbox': {'x1': 0.17961437908496733, 'y1': 0.5579322242424242, 'x2': 0.2356458647058824, 'y2': 0.5843483858585858}}, {'text': 'Question\\nAnswering,\\nCommon\\nsense\\nreasoning\\n', 'bbox': {'x1': 0.26649019607843133, 'y1': 0.5371759111111111, 'x2': 0.3419910764705882, 'y2': 0.6051034363636364}}, {'text': '2019-\\n2020\\n', 'bbox': {'x1': 0.3707450980392157, 'y1': 0.5579322242424242, 'x2': 0.40872344084967316, 'y2': 0.5843483858585858}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.4379183006535947, 'y1': 0.5648501535353535, 'x2': 0.4921591228758169, 'y2': 0.577429193939394}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5428578431372548, 'y1': 0.5648501535353535, 'x2': 0.5665434362745096, 'y2': 0.577429193939394}}, {'text': 'Pre-training Dataset\\n', 'bbox': {'x1': 0.6126764705882354, 'y1': 0.4535626053030303, 'x2': 0.7547900294117649, 'y2': 0.4661416457070707}}, {'text': 'Pixel-BERT: MS-\\nCOCO, Visual Genome\\nLX-MERT: MS\\nCOCO,Visual\\nGenome,VQA\\nv2.0,GQA,VG-QA\\nViLBERT: Visual\\nGenome, COCO\\nVL-BERT: Con-\\nceptual Captions,\\nBooksCorpus, English\\nWikipedia Uniter:\\nCOCO, VG, CC, SBU\\n', 'bbox': {'x1': 0.6066781045751635, 'y1': 0.4817406356060606, 'x2': 0.7607891081699347, 'y2': 0.6604519212121213}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7876977124183007, 'y1': 0.446644676010101, 'x2': 0.9351018676470588, 'y2': 0.47306083762626266}}, {'text': 'Pixel-BERT:\\nVQA 2.0 NLVR2,\\nFlickr30K MS-\\nCOCO LX-MERT:\\nVQA,GQA,NLVR\\nViLBERT: Con-\\nceptual Captions,\\nFlickr30k VL-BERT:\\nVCR dataset, Re-\\nfCOCO Uniter:\\nCOCO, Flickr30K,\\nVG, CC, SBU\\n', 'bbox': {'x1': 0.7877140522875818, 'y1': 0.48865856489898996, 'x2': 0.9350842588235294, 'y2': 0.6535327292929294}}, {'text': 'ViLT (Kim\\net al., 2021b)\\n', 'bbox': {'x1': 0.0680702614379085, 'y1': 0.7106418202020202, 'x2': 0.15397326830065358, 'y2': 0.7370579818181818}}, {'text': 'Image\\nand Text\\n', 'bbox': {'x1': 0.17961437908496733, 'y1': 0.7106418202020202, 'x2': 0.23564586470588234, 'y2': 0.7370579818181818}}, {'text': 'Unicode-VL\\n(Li et al.,\\n2020a)\\n', 'bbox': {'x1': 0.06988398692810457, 'y1': 0.8356784363636364, 'x2': 0.15215683071895425, 'y2': 0.8759317191919193}}, {'text': 'Image\\nand Text\\n', 'bbox': {'x1': 0.17961437908496733, 'y1': 0.8425963656565657, 'x2': 0.23564586470588234, 'y2': 0.8690125272727273}}, {'text': 'Visual\\nQuestion\\nAnswering,\\nImage text\\nmatching,\\nNatural\\nLanguage\\nfor Visual\\nReasoning\\nObject Clas-\\nsification,\\nVisual-\\nlinguistic\\nMatching,\\nvisual com-\\nmonsense\\nreasoning,\\nimage-text\\nretrieval\\n', 'bbox': {'x1': 0.26332516339869283, 'y1': 0.6622125272727273, 'x2': 0.3451584807189543, 'y2': 0.9243610121212121}}, {'text': '2021\\n', 'bbox': {'x1': 0.3707450980392157, 'y1': 0.7175610121212121, 'x2': 0.403302614379085, 'y2': 0.7301400525252526}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.4379183006535947, 'y1': 0.7175610121212121, 'x2': 0.4921591228758169, 'y2': 0.7301400525252526}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5428578431372548, 'y1': 0.7175610121212121, 'x2': 0.5665434362745096, 'y2': 0.7301400525252526}}, {'text': 'MS-COCO,Visual\\nGenome, SBU\\nCaptions, Google\\nConceptual Captions\\n', 'bbox': {'x1': 0.6156797385620916, 'y1': 0.696804698989899, 'x2': 0.7517864356209151, 'y2': 0.750895103030303}}, {'text': 'VQA 2.0, NLVR2,\\nMSCOCO, Flickr30K\\n', 'bbox': {'x1': 0.7894967320261438, 'y1': 0.7106418202020202, 'x2': 0.9333032816993465, 'y2': 0.7370579818181818}}, {'text': '2020\\n', 'bbox': {'x1': 0.3707450980392157, 'y1': 0.8495155575757576, 'x2': 0.403302614379085, 'y2': 0.8620945979797979}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.4379183006535947, 'y1': 0.8495155575757576, 'x2': 0.4921591228758169, 'y2': 0.8620945979797979}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5428578431372548, 'y1': 0.8495155575757576, 'x2': 0.5665434362745096, 'y2': 0.8620945979797979}}, {'text': 'Conceptual Captions-\\n3M, SBU Captions\\n', 'bbox': {'x1': 0.612970588235294, 'y1': 0.8425963656565657, 'x2': 0.7544981117647059, 'y2': 0.8690125272727273}}, {'text': 'MSCOCO, Flickr30K\\n', 'bbox': {'x1': 0.7894967320261438, 'y1': 0.8495155575757576, 'x2': 0.9333032816993465, 'y2': 0.8620945979797979}}, {'text': 'Continued on next page\\n', 'bbox': {'x1': 0.7829803921568628, 'y1': 0.926625406060606, 'x2': 0.9378239398692811, 'y2': 0.9392044464646465}}]}\n",
      "{'type': 'Page-header', 'bbox': [0.36009284524356616, 0.09764504172585227, 0.6442051068474265, 0.11221703962846236], 'properties': {'score': 0.38772889971733093, 'page_number': 37}, 'text_representation': 'Table 20 – continued from previous page\\n'}\n",
      "{'type': 'table', 'bbox': [0.05328372282140395, 0.11027537259188565, 0.9486170869715074, 0.17043079723011365], 'properties': {'score': 0.7358534336090088, 'title': None, 'columns': None, 'rows': None, 'page_number': 37}, 'text_representation': 'Transformer\\nModels\\n Processed\\nData\\ntype\\n(i/o)\\n Task Ac-\\ncomplished\\n Year\\n Architecture\\n(Encoder/\\nDecoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training Dataset\\n Dataset(Fine-tuning,\\nTraining, Testing)\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e02cb130>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06662091503267974, 'y1': 0.12738331237373743, 'x2': 0.15542154084967322, 'y2': 0.1537994739898989}}, {'text': 'Processed\\nData\\ntype\\n(i/o)\\n', 'bbox': {'x1': 0.17752450980392157, 'y1': 0.11354619116161614, 'x2': 0.24594413039215687, 'y2': 0.1676365952020199}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.2648954248366013, 'y1': 0.12738331237373743, 'x2': 0.34358694183006533, 'y2': 0.1537994739898989}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3707450980392157, 'y1': 0.13430250429292936, 'x2': 0.403286335620915, 'y2': 0.1468815446969697}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\n', 'bbox': {'x1': 0.42335294117647054, 'y1': 0.12046538308080808, 'x2': 0.5113559078431373, 'y2': 0.1607186659090907}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.5269101307189541, 'y1': 0.12046538308080808, 'x2': 0.59020194248366, 'y2': 0.1607186659090907}}, {'text': 'Pre-training Dataset\\n', 'bbox': {'x1': 0.6126764705882354, 'y1': 0.13430250429292936, 'x2': 0.7547900294117649, 'y2': 0.1468815446969697}}, {'text': 'Dataset(Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7897320261437908, 'y1': 0.12738331237373743, 'x2': 0.9330664918300652, 'y2': 0.1537994739898989}}]}\n",
      "{'type': 'Caption', 'bbox': [0.23218635110294117, 0.18061843872070313, 0.7696953986672794, 0.19512727217240766], 'properties': {'score': 0.4857839047908783, 'page_number': 37}, 'text_representation': 'Table 20: Multi-modal Transformer models - visual commonsense reasoning task\\n'}\n",
      "{'type': 'Text', 'bbox': [0.23218635110294117, 0.18061843872070313, 0.7696953986672794, 0.19512727217240766], 'properties': {'score': 0.46163663268089294, 'page_number': 37}, 'text_representation': 'Table 20: Multi-modal Transformer models - visual commonsense reasoning task\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09139994004193475, 0.21058779629794033, 0.9121256031709559, 0.26701685125177554], 'properties': {'score': 0.7555943131446838, 'page_number': 37}, 'text_representation': '• Unicoder-VL: Unicoder-VL is a large-scale pre-trained encoder-based model that utilizes cross-modeling to build a strong\\nunderstanding of the relationship between image and language. The model employs a masking scheme for pre-training on a\\nlarge corpus of data. These methods enhance the model’s performance on visual commonsense reasoning tasks in addition\\nto visual classification tasks (Li et al., 2020a)\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09139847699333639, 0.27039234508167614, 0.9140752814797795, 0.35469507390802557], 'properties': {'score': 0.7572154998779297, 'page_number': 37}, 'text_representation': '• ViLT (Vision-and-Language Transformer): is a multi-modal architecture based on the ViT (Vision Transformer) model,\\nutilizing a free-convolution approach. Unlike other VLP (Vision-and-Language Pre-training) models, ViLT performs data\\naugmentation during the execution of downstream tasks of classification and retrievals, which improves the model’s perfor-\\nmance. Inspired by Pixel-BERT, ViLT takes the entire image as input instead of just using selected regions. By omitting\\nconvolutional visual embedders, ViLT reduces the model size and achieves remarkable performance compared to other VLP\\nmodels (Kim et al., 2021b).\\nBERT-Variants: The BERT-Variants models have been previously described in the Classification & segmentation section.\\nIt should be noted that these models are also capable of performing the Classification & segmentation task.\\n'}\n",
      "{'type': 'Text', 'bbox': [0.10517874325022977, 0.35560075239701705, 0.9112207749310661, 0.38430883234197444], 'properties': {'score': 0.7062181234359741, 'page_number': 37}}\n",
      "{'type': 'Section-header', 'bbox': [0.09434039845186121, 0.39254011674360795, 0.4052731861787684, 0.40672501997514204], 'properties': {'score': 0.6511117815971375, 'page_number': 37}, 'text_representation': 'IMAGE/VIDEO/SPEECH GENERATION\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09384098726160386, 0.4094254649769176, 0.9140939510569853, 0.47928089488636366], 'properties': {'score': 0.8212869167327881, 'page_number': 37}, 'text_representation': '6.4.5\\nMulti-modal generation tasks have gained a lot of attention in the field of artificial intelligence. These tasks involve gener-\\nating images, text, or speech from inputs of different modalities of input. In recent times, several generative models have\\ndemonstrated outstanding performance, making this field of research even more attractive (Suzuki & Matsuo, 2022). In this\\nsection, we discuss some significant transformer models that have been used for multi-modal generation tasks. These models\\nare summarized in Table 21.\\n'}\n",
      "{'type': 'table', 'bbox': [0.05241581636316636, 0.4932071200284091, 0.9362657255284926, 0.761787275834517], 'properties': {'score': 0.781550943851471, 'title': None, 'columns': None, 'rows': None, 'page_number': 37}, 'text_representation': 'Transformer\\nModels\\n Processed\\nData\\ntype(i/o)\\n Task Ac-\\ncomplished\\n Year\\n Architecture\\n(Encoder/\\nDecoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training\\nDataset\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n DALL-E\\n(Ramesh\\net al., 2021)\\n GLIDE\\n(Nichol\\net al., 2022)\\n Chimera (Li\\n& Hoefler,\\n2021)\\n CogView\\n(Ding et al.,\\n2021)\\n Image\\nand Text\\n Image\\nand Text\\n Audio\\nand Text\\n Image\\nand Text\\n Image\\nGeneration\\nfrom text\\n Image\\ngeneration &\\nedit from text\\n Text\\ngeneration\\nfrom speech\\n Classification,\\nImage\\ngeneration\\nfrom text\\n 2021\\n Encoder &\\nDecoder\\n Yes\\n Not disclosed\\n 2021\\n Encoder &\\nDecoder\\n No\\n NA\\n 2021\\n Encoder &\\nDecoder\\n Yes\\n ALIGN &\\nMT-Dataset\\n 2021\\n Decoder\\n Yes\\n VQ-VAE\\n Conceptual Captions (MS-\\nCOCO extension), Wikipedia\\n(text-image pairs), and\\nYFCC100M(filtered subset)\\nMS-COCO, ViT-B CLIP\\n(noised), CLIP and\\nDALL-E filtered datasets\\nMuST-C, Augmented\\nLibriSpeech Dataset (En-Fr),\\nMachine Translation Datasets\\n(WMT 2016, WMT 2014,\\nOPUS100, OpenSubtitles)\\n MS COCO, Wudao\\nCorpora-extension dataset.\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e02cb910>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06662091503267974, 'y1': 0.5019615952020202, 'x2': 0.15542154084967322, 'y2': 0.5283777568181819}}, {'text': 'Processed\\nData\\ntype(i/o)\\n', 'bbox': {'x1': 0.17752450980392157, 'y1': 0.4950424032828283, 'x2': 0.24594413039215687, 'y2': 0.5352956861111111}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.2648954248366013, 'y1': 0.5019615952020202, 'x2': 0.34358694183006533, 'y2': 0.5283777568181819}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3707450980392157, 'y1': 0.5088795244949496, 'x2': 0.403286335620915, 'y2': 0.5214585648989899}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\n', 'bbox': {'x1': 0.42335294117647054, 'y1': 0.4950424032828283, 'x2': 0.5113559078431373, 'y2': 0.5352956861111111}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.5269101307189541, 'y1': 0.4950424032828283, 'x2': 0.59020194248366, 'y2': 0.5352956861111111}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.6108218954248366, 'y1': 0.5019615952020202, 'x2': 0.6964318846405227, 'y2': 0.5283777568181819}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7506437908496733, 'y1': 0.5019615952020202, 'x2': 0.8980479460784313, 'y2': 0.5283777568181819}}, {'text': 'DALL-E\\n(Ramesh\\net al., 2021)\\n', 'bbox': {'x1': 0.07213888888888889, 'y1': 0.5440635373737374, 'x2': 0.14990251666666665, 'y2': 0.5843168202020201}}, {'text': 'GLIDE\\n(Nichol\\net al., 2022)\\n', 'bbox': {'x1': 0.07213888888888889, 'y1': 0.5929966181818183, 'x2': 0.14990251666666665, 'y2': 0.633249901010101}}, {'text': 'Chimera (Li\\n& Hoefler,\\n2021)\\n', 'bbox': {'x1': 0.07100816993464053, 'y1': 0.6488476282828283, 'x2': 0.1510345450980392, 'y2': 0.6891009111111112}}, {'text': 'CogView\\n(Ding et al.,\\n2021)\\n', 'bbox': {'x1': 0.07213888888888889, 'y1': 0.7116165676767677, 'x2': 0.14990251666666668, 'y2': 0.7518698505050505}}, {'text': 'Image\\nand Text\\n', 'bbox': {'x1': 0.17961437908496733, 'y1': 0.5509827292929294, 'x2': 0.23564586470588234, 'y2': 0.5773976282828284}}, {'text': 'Image\\nand Text\\n', 'bbox': {'x1': 0.17961437908496733, 'y1': 0.5999145474747474, 'x2': 0.23564586470588234, 'y2': 0.626330709090909}}, {'text': 'Audio\\nand Text\\n', 'bbox': {'x1': 0.17961437908496733, 'y1': 0.6557655575757576, 'x2': 0.23564586470588234, 'y2': 0.6821817191919192}}, {'text': 'Image\\nand Text\\n', 'bbox': {'x1': 0.17961437908496733, 'y1': 0.7185357595959595, 'x2': 0.23564586470588234, 'y2': 0.7449519212121213}}, {'text': 'Image\\nGeneration\\nfrom text\\n', 'bbox': {'x1': 0.26807843137254905, 'y1': 0.5440635373737374, 'x2': 0.3404049539215687, 'y2': 0.5843168202020201}}, {'text': 'Image\\ngeneration &\\nedit from text\\n', 'bbox': {'x1': 0.26005228758169935, 'y1': 0.5929966181818183, 'x2': 0.3484296656862746, 'y2': 0.633249901010101}}, {'text': 'Text\\ngeneration\\nfrom speech\\n', 'bbox': {'x1': 0.26423692810457516, 'y1': 0.6488476282828283, 'x2': 0.34424702450980393, 'y2': 0.6891009111111112}}, {'text': 'Classification,\\nImage\\ngeneration\\nfrom text\\n', 'bbox': {'x1': 0.25792320261437907, 'y1': 0.7046986383838384, 'x2': 0.3506307303921568, 'y2': 0.7587890424242424}}, {'text': '2021\\n', 'bbox': {'x1': 0.3707450980392157, 'y1': 0.5579006585858586, 'x2': 0.403302614379085, 'y2': 0.5704796989898989}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.42955065359477124, 'y1': 0.5509827292929294, 'x2': 0.5005260392156863, 'y2': 0.5773976282828284}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5428578431372548, 'y1': 0.5579006585858586, 'x2': 0.5665434362745096, 'y2': 0.5704796989898989}}, {'text': 'Not disclosed\\n', 'bbox': {'x1': 0.6090882352941176, 'y1': 0.5579006585858586, 'x2': 0.6981655999999999, 'y2': 0.5704796989898989}}, {'text': '2021\\n', 'bbox': {'x1': 0.3707450980392157, 'y1': 0.6068337393939394, 'x2': 0.403302614379085, 'y2': 0.6194127797979798}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.42955065359477124, 'y1': 0.5999145474747474, 'x2': 0.5005260392156863, 'y2': 0.626330709090909}}, {'text': 'No\\n', 'bbox': {'x1': 0.5447549019607842, 'y1': 0.6068337393939394, 'x2': 0.5646475444444443, 'y2': 0.6194127797979798}}, {'text': 'NA\\n', 'bbox': {'x1': 0.6421584967320261, 'y1': 0.6068337393939394, 'x2': 0.665095266993464, 'y2': 0.6194127797979798}}, {'text': '2021\\n', 'bbox': {'x1': 0.3707450980392157, 'y1': 0.6626847494949495, 'x2': 0.403302614379085, 'y2': 0.6752637898989898}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.42955065359477124, 'y1': 0.6557655575757576, 'x2': 0.5005260392156863, 'y2': 0.6821817191919192}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5428578431372548, 'y1': 0.6626847494949495, 'x2': 0.5665434362745096, 'y2': 0.6752637898989898}}, {'text': 'ALIGN &\\nMT-Dataset\\n', 'bbox': {'x1': 0.6150457516339869, 'y1': 0.6557655575757576, 'x2': 0.692207065359477, 'y2': 0.6821817191919192}}, {'text': '2021\\n', 'bbox': {'x1': 0.3707450980392157, 'y1': 0.7254536888888888, 'x2': 0.403302614379085, 'y2': 0.7380327292929293}}, {'text': 'Decoder\\n', 'bbox': {'x1': 0.43747058823529417, 'y1': 0.7254536888888888, 'x2': 0.4926067421568628, 'y2': 0.7380327292929293}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5428578431372548, 'y1': 0.7254536888888888, 'x2': 0.5665434362745096, 'y2': 0.7380327292929293}}, {'text': 'VQ-VAE\\n', 'bbox': {'x1': 0.6235359477124183, 'y1': 0.7254536888888888, 'x2': 0.6837185166666666, 'y2': 0.7380327292929293}}, {'text': 'Conceptual Captions (MS-\\nCOCO extension), Wikipedia\\n(text-image pairs), and\\nYFCC100M(filtered subset)\\nMS-COCO, ViT-B CLIP\\n(noised), CLIP and\\nDALL-E filtered datasets\\nMuST-C, Augmented\\nLibriSpeech Dataset (En-Fr),\\nMachine Translation Datasets\\n(WMT 2016, WMT 2014,\\nOPUS100, OpenSubtitles)\\n', 'bbox': {'x1': 0.7274297385620915, 'y1': 0.537145608080808, 'x2': 0.9212609120915032, 'y2': 0.7029380323232323}}, {'text': 'MS COCO, Wudao\\nCorpora-extension dataset.\\n', 'bbox': {'x1': 0.7372140522875816, 'y1': 0.7185357595959595, 'x2': 0.911478158496732, 'y2': 0.7449519212121213}}]}\n",
      "{'type': 'Text', 'bbox': [0.2324272066004136, 0.7717390580610796, 0.7706565946691176, 0.786493974165483], 'properties': {'score': 0.45364004373550415, 'page_number': 37}, 'text_representation': 'Table 21: Multi-modal Transformer models - image/video/speech generation task\\n'}\n",
      "{'type': 'Caption', 'bbox': [0.2324272066004136, 0.7717390580610796, 0.7706565946691176, 0.786493974165483], 'properties': {'score': 0.4479840397834778, 'page_number': 37}, 'text_representation': 'Table 21: Multi-modal Transformer models - image/video/speech generation task\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09054017010857077, 0.8022283935546874, 0.9132711971507353, 0.8857809725674716], 'properties': {'score': 0.7906656265258789, 'page_number': 37}, 'text_representation': '• DALL-E: DALL-E (Ramesh et al., 2021) is a popular transformer-based model for generating images from text. It is\\ntrained on a large and diverse dataset of both text and images, utilizing 12 billion parameters from the GPT-3 architecture.\\nTo reduce memory consumption, DALL-E compresses images without compromising their visual quality. An updated\\nversion of DALL-E, known as DALL-E 2, has been introduced with a higher number of parameters (175 billion) which\\nallows for the generation of higher resolution images. Additionally, DALL-E 2 is capable of generating a wider range of\\nimages.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09090522317325368, 0.8895609352805398, 0.9119060202205882, 0.9458228648792614], 'properties': {'score': 0.7201940417289734, 'page_number': 37}, 'text_representation': '• Chimera: The Chimera end-to-end architecture is designed for Speech-And-Text-Translation (ST). This architecture draws\\ninspiration from Text-Machine-Translation and proposes a new module called the Shared Semantic Projection Module based\\non attention mechanisms. The objective of this module is to reduce latency and errors during the speech translation process\\nby removing dissimilarities between speech and language modalities.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09157463522518382, 0.10017637772993608, 0.9124270450367648, 0.17058958573774857], 'properties': {'score': 0.794408917427063, 'page_number': 38}, 'text_representation': '• CogView: CogView (Ding et al., 2021) is an image generation model that generates images based on input text descriptions,\\nmaking it a challenging task that requires a deep understanding of the contextual relationship between text and image. It\\nutilizes a transformer-GPT-based architecture to encode the text into a vector and decode it into an image. This model\\noutperforms DALL-E in some cases, which also generates images from text descriptions, but uses text-image pairs for\\ntraining the model.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09185052310719209, 0.1746689120205966, 0.9122885311351103, 0.24543684525923296], 'properties': {'score': 0.7464380860328674, 'page_number': 38}, 'text_representation': '• GLIDE: short for Guided Language to Image Diffusion for Generation and Editing, GLIDE is a diffusion model that is\\ndistinct from conventional models in that it can both generate and edit images. Unlike other models, diffusion models\\nare sequentially injected with random noise and trained to remove that noise to construct the original data. GLIDE takes\\ntextual information as input and generates an output image conditioned on that information. In some instances, the images\\ngenerated by GLIDE are more impressive than those generated by DALL-E (Nichol et al., 2022).\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09460653866038603, 0.25481719970703126, 0.2890823543772978, 0.2685777421431108], 'properties': {'score': 0.782660186290741, 'page_number': 38}}\n",
      "{'type': 'Text', 'bbox': [0.0940365151798024, 0.27150595925071025, 0.9129607077205882, 0.36964358243075285], 'properties': {'score': 0.884167492389679, 'page_number': 38}, 'text_representation': '6.4.6 CLOUD COMPUTING\\nCloud computing is a crucial element of modern technology, particularly with regard to the Internet of Things (IoT). It\\nencompasses a wide variety of cloud-based tasks, including server computing, task scheduling, storage, networking, and\\nmore. In wireless networks, cloud computing aims to improve scalability, flexibility, and adaptability, thereby providing\\nseamless connectivity. To achieve this, data or information is retrieved from the network for computation, with various types\\nof data being processed, including text, images, speech, and digits. Due to this multi-modal approach, cloud computing is\\nclassified in this category (Li et al., 2017, Jauro et al., 2020, Yu et al., 2017). In this article, we focus on the significant\\ntransformer models used in cloud computing tasks, which are presented in Table 22.\\n'}\n",
      "{'type': 'table', 'bbox': [0.05248812058392693, 0.39102963534268464, 0.9367214786305147, 0.7705893776633522], 'properties': {'score': 0.6774762272834778, 'title': None, 'columns': None, 'rows': None, 'page_number': 38}, 'text_representation': 'Transformer\\nModels\\n VMD & R-\\nTransformer\\n(Zhou\\net al., 2020)\\n ACT4JS\\n(Xu &\\nZhao, 2022)\\n TEDGE-\\nCatching\\n(Hajiakhondi-\\nMeybodi\\net al., 2022)\\n SACCT\\n(Wang et al.,\\n2022b)\\n Processed\\nData\\ntype(i/o)\\n workload\\nsequence\\n Cloud\\njobs/task\\n Sequential\\nrequest\\npattern\\nof the\\ncontents\\n(Ex:\\nvideo,\\nimage,\\nwebsites,\\netc)\\nNetwork\\nstatus\\n(Band-\\nwidth,\\nstorage,\\nand etc)\\n Task Ac-\\ncomplished\\n Year\\n Architecture\\n(Encoder/\\nDecoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training\\nDataset\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n cloud\\nworkload\\nforecasting\\n Cloud\\ncomputing\\nresource job\\nscheduling.\\n Predict the\\ncontent\\npopularity\\nin proactive\\ncaching\\nschemes.\\n Optimize\\nnetwork\\nbased on\\nnetwork\\nstatus.\\n 2020\\n Encoder &\\nDecoder\\n No\\n NA\\n Google cluster trace,\\nAlibaba cluster trace\\n 2022\\n Encoder\\n No\\n NA\\n Alibaba Cluster-V2018\\n 2021\\n Encoder\\n No\\n NA\\n MovieLens\\n 2021\\n Encoder\\n No\\n NA\\n Not mentioned clearly\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e0301390>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06662091503267974, 'y1': 0.400239372979798, 'x2': 0.15542154084967322, 'y2': 0.426654271969697}}, {'text': 'VMD & R-\\nTransformer\\n(Zhou\\net al., 2020)\\n', 'bbox': {'x1': 0.07062581699346406, 'y1': 0.43542212323232327, 'x2': 0.1514172937908497, 'y2': 0.48951252727272726}}, {'text': 'ACT4JS\\n(Xu &\\nZhao, 2022)\\n', 'bbox': {'x1': 0.07123529411764705, 'y1': 0.4981923252525252, 'x2': 0.15080586405228757, 'y2': 0.5384456080808081}}, {'text': 'TEDGE-\\nCatching\\n(Hajiakhondi-\\nMeybodi\\net al., 2022)\\n', 'bbox': {'x1': 0.06535947712418301, 'y1': 0.5817175777777778, 'x2': 0.15668331045751635, 'y2': 0.6496438404040404}}, {'text': 'SACCT\\n(Wang et al.,\\n2022b)\\n', 'bbox': {'x1': 0.06963235294117646, 'y1': 0.7067529313131313, 'x2': 0.15240983823529414, 'y2': 0.7470062141414141}}, {'text': 'Processed\\nData\\ntype(i/o)\\n', 'bbox': {'x1': 0.17752450980392157, 'y1': 0.39332018106060607, 'x2': 0.24594413039215687, 'y2': 0.4335734638888889}}, {'text': 'workload\\nsequence\\n', 'bbox': {'x1': 0.17752450980392157, 'y1': 0.4492592444444444, 'x2': 0.23884659183006535, 'y2': 0.47567540606060604}}, {'text': 'Cloud\\njobs/task\\n', 'bbox': {'x1': 0.17868790849673202, 'y1': 0.5051102545454546, 'x2': 0.2365751725490196, 'y2': 0.5315264161616161}}, {'text': 'Sequential\\nrequest\\npattern\\nof the\\ncontents\\n(Ex:\\nvideo,\\nimage,\\nwebsites,\\netc)\\nNetwork\\nstatus\\n(Band-\\nwidth,\\nstorage,\\nand etc)\\n', 'bbox': {'x1': 0.17752450980392157, 'y1': 0.547125406060606, 'x2': 0.2462534267973856, 'y2': 0.7677625272727274}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.2648954248366013, 'y1': 0.400239372979798, 'x2': 0.34358694183006533, 'y2': 0.426654271969697}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3707450980392157, 'y1': 0.4071573022727273, 'x2': 0.403286335620915, 'y2': 0.4197363426767677}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\n', 'bbox': {'x1': 0.42335294117647054, 'y1': 0.39332018106060607, 'x2': 0.5113559078431373, 'y2': 0.4335734638888889}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.5269101307189541, 'y1': 0.39332018106060607, 'x2': 0.59020194248366, 'y2': 0.4335734638888889}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.6061895424836601, 'y1': 0.400239372979798, 'x2': 0.6917995316993464, 'y2': 0.426654271969697}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7460114379084968, 'y1': 0.400239372979798, 'x2': 0.8934155931372549, 'y2': 0.426654271969697}}, {'text': 'cloud\\nworkload\\nforecasting\\n', 'bbox': {'x1': 0.26807843137254905, 'y1': 0.4423413151515151, 'x2': 0.34040495392156866, 'y2': 0.4825945979797979}}, {'text': 'Cloud\\ncomputing\\nresource job\\nscheduling.\\n', 'bbox': {'x1': 0.26423692810457516, 'y1': 0.4912731333333333, 'x2': 0.34424702450980393, 'y2': 0.5453635373737373}}, {'text': 'Predict the\\ncontent\\npopularity\\nin proactive\\ncaching\\nschemes.\\n', 'bbox': {'x1': 0.26591339869281044, 'y1': 0.5747983858585859, 'x2': 0.34257007091503267, 'y2': 0.6565630323232322}}, {'text': 'Optimize\\nnetwork\\nbased on\\nnetwork\\nstatus.\\n', 'bbox': {'x1': 0.27394607843137253, 'y1': 0.6929170727272727, 'x2': 0.3345356163398693, 'y2': 0.7608433353535354}}, {'text': '2020\\n', 'bbox': {'x1': 0.3707450980392157, 'y1': 0.45617843636363636, 'x2': 0.403302614379085, 'y2': 0.4687574767676767}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.42955065359477124, 'y1': 0.4492592444444444, 'x2': 0.5005260392156863, 'y2': 0.47567540606060604}}, {'text': 'No\\n', 'bbox': {'x1': 0.5447549019607842, 'y1': 0.45617843636363636, 'x2': 0.5646475444444443, 'y2': 0.4687574767676767}}, {'text': 'NA\\n', 'bbox': {'x1': 0.6375261437908497, 'y1': 0.45617843636363636, 'x2': 0.6604629140522875, 'y2': 0.4687574767676767}}, {'text': 'Google cluster trace,\\nAlibaba cluster trace\\n', 'bbox': {'x1': 0.7521323529411765, 'y1': 0.4492592444444444, 'x2': 0.8872948820261438, 'y2': 0.47567540606060604}}, {'text': '2022\\n', 'bbox': {'x1': 0.3707450980392157, 'y1': 0.5120294464646465, 'x2': 0.403302614379085, 'y2': 0.5246084868686869}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.4379183006535947, 'y1': 0.5120294464646465, 'x2': 0.4921591228758169, 'y2': 0.5246084868686869}}, {'text': 'No\\n', 'bbox': {'x1': 0.5447549019607842, 'y1': 0.5120294464646465, 'x2': 0.5646475444444443, 'y2': 0.5246084868686869}}, {'text': 'NA\\n', 'bbox': {'x1': 0.6375261437908497, 'y1': 0.5120294464646465, 'x2': 0.6604629140522875, 'y2': 0.5246084868686869}}, {'text': 'Alibaba Cluster-V2018\\n', 'bbox': {'x1': 0.7436911764705882, 'y1': 0.5120294464646465, 'x2': 0.8957347777777779, 'y2': 0.5246084868686869}}, {'text': '2021\\n', 'bbox': {'x1': 0.3707450980392157, 'y1': 0.6093918202020202, 'x2': 0.403302614379085, 'y2': 0.6219708606060605}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.4379183006535947, 'y1': 0.6093918202020202, 'x2': 0.4921591228758169, 'y2': 0.6219708606060605}}, {'text': 'No\\n', 'bbox': {'x1': 0.5447549019607842, 'y1': 0.6093918202020202, 'x2': 0.5646475444444443, 'y2': 0.6219708606060605}}, {'text': 'NA\\n', 'bbox': {'x1': 0.6375261437908497, 'y1': 0.6093918202020202, 'x2': 0.6604629140522875, 'y2': 0.6219708606060605}}, {'text': 'MovieLens\\n', 'bbox': {'x1': 0.7827598039215686, 'y1': 0.6093918202020202, 'x2': 0.8566653660130717, 'y2': 0.6219708606060605}}, {'text': '2021\\n', 'bbox': {'x1': 0.3707450980392157, 'y1': 0.7205900525252525, 'x2': 0.403302614379085, 'y2': 0.7331690929292929}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.4379183006535947, 'y1': 0.7205900525252525, 'x2': 0.4921591228758169, 'y2': 0.7331690929292929}}, {'text': 'No\\n', 'bbox': {'x1': 0.5447549019607842, 'y1': 0.7205900525252525, 'x2': 0.5646475444444443, 'y2': 0.7331690929292929}}, {'text': 'NA\\n', 'bbox': {'x1': 0.6375261437908497, 'y1': 0.7205900525252525, 'x2': 0.6604629140522875, 'y2': 0.7331690929292929}}, {'text': 'Not mentioned clearly\\n', 'bbox': {'x1': 0.7469232026143792, 'y1': 0.7205900525252525, 'x2': 0.8925041369281047, 'y2': 0.7331690929292929}}]}\n",
      "{'type': 'Caption', 'bbox': [0.28091615564682904, 0.7807511763139204, 0.7227887321920956, 0.7955206298828125], 'properties': {'score': 0.6712623834609985, 'page_number': 38}, 'text_representation': 'Table 22: Multi-modal Transformer models - cloud computing task\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09060519947725183, 0.8149179354580965, 0.9136244829963235, 0.9130367209694602], 'properties': {'score': 0.753358781337738, 'page_number': 38}, 'text_representation': '• VMD & R-TRANSFORMER: Workload forecasting is a critical task for the cloud, and previous research has focused\\non using recurrent neural networks (RNNs) for this purpose. However, due to the highly complex and dynamic nature of\\nworkloads, RNN-based models struggle to provide accurate forecasts because of the problem of vanishing gradients. In this\\ncontext, the proposed Variational Mode Decomposition-VMD and R-Transformer model offers a more accurate solution\\nby capturing long-term dependencies using multi-head attention and local non-linear relationships of workload sequences\\nwith local techniques (Zhou et al., 2020). Therefore, this model is capable of executing the workload forecasting task with\\ngreater precision than existing RNN-based models.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09163182875689338, 0.9173723255504261, 0.9114324592141544, 0.9456382057883522], 'properties': {'score': 0.6462893486022949, 'page_number': 38}, 'text_representation': '• TEDGE-Caching: TEDGE is an acronym for Transformer-based Edge Caching, which is a critical component of the\\n6G wireless network as it provides a high-bandwidth, low-latency connection. Edge caching stores multimedia content\\n'}\n",
      "{'type': 'Text', 'bbox': [0.10564726885627297, 0.10068910078568892, 0.911372501148897, 0.18409673517400568], 'properties': {'score': 0.8890612721443176, 'page_number': 39}, 'text_representation': 'to deliver it to users with low latency. To achieve this, it is essential to proactively predict popular content. However,\\nconventional models are limited by long-term dependencies, computational complexity, and the inability to compute in\\nparallel. In this context, the Transformer-based Edge (TEDGE) caching framework incorporates a vision transformer (ViT)\\nto overcome these limitations, without requiring data pre-processing or additional contextual information to predict popular\\ncontent at the Mobile Edge. This is the first model to apply a transformer-based approach to execute this task, resulting in\\nsuperior performance (Hajiakhondi-Meybodi et al., 2022).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09113025440889247, 0.1890315801447088, 0.9103988108915441, 0.2719575084339489], 'properties': {'score': 0.8585207462310791, 'page_number': 39}, 'text_representation': '• ACT4JS: The Actor-Critic Transformer for Job Scheduling (ACT4JS) is a transformer-based model designed to allocate\\ncloud computing resources to different tasks in cloud computing. The model consists of an Actor and Critic network, where\\nthe Actor-network selects the best action to take at each step, and the Critic network evaluates the action taken by the Actor-\\nnetwork and provides feedback to improve future steps. This approach allows for a better understanding of the complex\\nrelationship between cloud jobs and enables prediction or scheduling of jobs based on different features such as job priority,\\nnetwork conditions, resource availability, and more (Xu & Zhao, 2022).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09109666712143842, 0.27683590975674716, 0.9101230037913602, 0.3738878839666193], 'properties': {'score': 0.8495878577232361, 'page_number': 39}, 'text_representation': '• SACCT: SACCT refers to the Soft Actor-Critic framework with Communication Transformer, which combines the trans-\\nformer, reinforcement learning, and convex optimization techniques. This model introduces the Communication Trans-\\nformer (CT), which works with reinforcement learning to adapt to different challenges, such as bandwidth limitations,\\nstorage constraints, and more, in the wireless edge network during live streaming. Adapting to changing network conditions\\nis critical during live streaming, and SACCT provides a system that adjusts resources to improve the quality of service\\nbased on user demand and network conditions. The SACCT model’s ability to adapt to changing conditions and optimize\\nresources makes it an important contribution to the field of wireless edge network technology (Wang et al., 2022b).\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09437799790326287, 0.38548647793856533, 0.25794762106502755, 0.3986708484996449], 'properties': {'score': 0.7126166820526123, 'page_number': 39}, 'text_representation': '6.5 AUDIO & SPEECH\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09407871919519761, 0.40418196244673293, 0.9112245088465074, 0.4596656660600142], 'properties': {'score': 0.923000693321228, 'page_number': 39}, 'text_representation': 'Audio and speech processing is one of the most essential tasks in the field of deep learning. Along with NLP, speech process-\\ning has also gained attention from researchers, leading to the application of deep neural network methods. As transformers\\nhave achieved great success in the field of NLP, researchers have also had significant success when applying transformer-based\\nmodels to speech processing.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09463678696576287, 0.46882568359375, 0.3070211971507353, 0.48284290660511364], 'properties': {'score': 0.7562026977539062, 'page_number': 39}}\n",
      "{'type': 'Text', 'bbox': [0.09400641946231618, 0.48584861061789775, 0.9128766228170956, 0.6386360862038353], 'properties': {'score': 0.9164100885391235, 'page_number': 39}, 'text_representation': '6.5.1 SPEECH RECOGNITION\\nSpeech Recognition is one of the most popular tasks in the field of artificial intelligence. It is the ability of a model to identify\\nhuman speech and convert it into a textual or written format. This process is also known as speech-to-text, automatic speech\\nrecognition, or computer-assisted transcription. Speech recognition technology has advanced significantly in the last few\\nyears. It involves two types of models, namely the acoustic model and the language model. Several features contribute to the\\neffectiveness of speech recognition models, including language weighting, speaker labeling, acoustics training, and profanity\\nfiltering. Despite significant advancements in speech recognition technology, there is still room for further improvement (Yu\\n& Deng, 2016, Nassif et al., 2019, Deng et al., 2013). One promising development in this area is the use of transformer-\\nbased models, which have shown significant improvement in various stages of the speech recognition task. The majority\\nof transformer-based audio/speech processing models have focused on speech recognition tasks, and among these, several\\nmodels have made exceptional contributions, exhibited high levels of accuracy, introduced new effective ideas, or created\\nbuzz in the AI field. In Table 23, we highlight the most significant transformer models for speech recognition tasks.\\n'}\n",
      "{'type': 'table', 'bbox': [0.05216005661908318, 0.6581405362215909, 0.9495396513097426, 0.9440894664417614], 'properties': {'score': 0.8266295194625854, 'title': None, 'columns': None, 'rows': None, 'page_number': 39}, 'text_representation': 'Transformer\\nModels\\n Conformer (Peng\\net al., 2021)\\nSpeech Trans-\\nformer (Dong\\net al., 2018)\\nVQ-Wav2vec\\n(Baevski\\net al., 2020a)\\nWav2vec\\n2.0 (Baevski\\net al., 2020b)\\nHuBERT (Hsu\\net al., 2021)\\n Task Ac-\\ncomplished\\n Speech\\nRecognition\\n Speech\\nRecognition\\n Speech\\nRecognition\\n Speech\\nRecognition\\n Speech\\nRecognition\\n Year\\n 2020\\n 2018\\n Architecture\\n(Encoder/\\nDecoder)\\nEncoder &\\nDecoder\\n Encoder &\\nDecoder\\n 2020\\n Encoder\\n 2020\\n Encoder\\n 2021\\n Encoder\\n BigSSL (Zhang\\net al., 2022)\\n Speech\\nRecognition\\n 2022\\n Encoder &\\nDecoder\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training\\nDataset\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n No\\n No\\n Yes\\n Yes\\n Yes\\n Yes\\n NA\\n NA\\n Librispeech, test/testother.\\n Wall StreetJournal (WSJ),\\nNVIDIA K80 G-PU\\n Librispeech\\n Librispeech,\\nLibriVox\\n Librispeech,\\nLibri-light\\n wav2vec 2.0,\\nYT-U, Libri-Light\\n TIMIT, WSJ-\\nWall StreetJournal\\n Librispeech, Lib-\\nriVox, TIMIT\\n Librispeech(train-clean),\\nLibri-light(train-clean)\\nYouTube, English(US)\\nVoice search(VS), Speech-\\nStew, LibriSpeech,\\nCHiME6, Telephony\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e0302e90>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.08097875816993463, 'y1': 0.6678328073232324, 'x2': 0.1697793839869281, 'y2': 0.6942489689393939}}, {'text': 'Conformer (Peng\\net al., 2021)\\nSpeech Trans-\\nformer (Dong\\net al., 2018)\\nVQ-Wav2vec\\n(Baevski\\net al., 2020a)\\nWav2vec\\n2.0 (Baevski\\net al., 2020b)\\nHuBERT (Hsu\\net al., 2021)\\n', 'bbox': {'x1': 0.068640522875817, 'y1': 0.7035193454545455, 'x2': 0.1821197460784314, 'y2': 0.884155204040404}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.2243300653594771, 'y1': 0.6678328073232324, 'x2': 0.3030215823529412, 'y2': 0.6942489689393939}}, {'text': 'Speech\\nRecognition\\n', 'bbox': {'x1': 0.22388235294117645, 'y1': 0.7035193454545455, 'x2': 0.30346920163398694, 'y2': 0.7299355070707071}}, {'text': 'Speech\\nRecognition\\n', 'bbox': {'x1': 0.22388235294117645, 'y1': 0.738615305050505, 'x2': 0.30346920163398694, 'y2': 0.7650314666666668}}, {'text': 'Speech\\nRecognition\\n', 'bbox': {'x1': 0.22388235294117645, 'y1': 0.780629193939394, 'x2': 0.30346920163398694, 'y2': 0.8070453555555555}}, {'text': 'Speech\\nRecognition\\n', 'bbox': {'x1': 0.22388235294117645, 'y1': 0.8226443454545455, 'x2': 0.30346920163398694, 'y2': 0.8490605070707071}}, {'text': 'Speech\\nRecognition\\n', 'bbox': {'x1': 0.22388235294117645, 'y1': 0.8577390424242424, 'x2': 0.30346920163398694, 'y2': 0.884155204040404}}, {'text': 'Year\\n', 'bbox': {'x1': 0.34175816993464053, 'y1': 0.6747519992424242, 'x2': 0.3742994075163399, 'y2': 0.6873310396464646}}, {'text': '2020\\n', 'bbox': {'x1': 0.34175816993464053, 'y1': 0.7104385373737374, 'x2': 0.3743156862745098, 'y2': 0.7230175777777778}}, {'text': '2018\\n', 'bbox': {'x1': 0.34175816993464053, 'y1': 0.745534496969697, 'x2': 0.3743156862745098, 'y2': 0.7581135373737374}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\nEncoder &\\nDecoder\\n', 'bbox': {'x1': 0.39436601307189545, 'y1': 0.6609148780303031, 'x2': 0.48236897973856213, 'y2': 0.7299355070707071}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.4005653594771242, 'y1': 0.738615305050505, 'x2': 0.4715407450980392, 'y2': 0.7650314666666668}}, {'text': '2020\\n', 'bbox': {'x1': 0.34175816993464053, 'y1': 0.7875483858585859, 'x2': 0.3743156862745098, 'y2': 0.8001274262626262}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.40893137254901957, 'y1': 0.7875483858585859, 'x2': 0.46317219477124183, 'y2': 0.8001274262626262}}, {'text': '2020\\n', 'bbox': {'x1': 0.34175816993464053, 'y1': 0.8295622747474747, 'x2': 0.3743156862745098, 'y2': 0.8421413151515151}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.40893137254901957, 'y1': 0.8295622747474747, 'x2': 0.46317219477124183, 'y2': 0.8421413151515151}}, {'text': '2021\\n', 'bbox': {'x1': 0.34175816993464053, 'y1': 0.8646582343434344, 'x2': 0.3743156862745098, 'y2': 0.8772372747474748}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.40893137254901957, 'y1': 0.8646582343434344, 'x2': 0.46317219477124183, 'y2': 0.8772372747474748}}, {'text': 'BigSSL (Zhang\\net al., 2022)\\n', 'bbox': {'x1': 0.07405228758169935, 'y1': 0.8997541939393939, 'x2': 0.17670613660130718, 'y2': 0.9261690929292928}}, {'text': 'Speech\\nRecognition\\n', 'bbox': {'x1': 0.22388235294117645, 'y1': 0.8997541939393939, 'x2': 0.30346920163398694, 'y2': 0.9261690929292928}}, {'text': '2022\\n', 'bbox': {'x1': 0.34175816993464053, 'y1': 0.9066721232323232, 'x2': 0.3743156862745098, 'y2': 0.9192511636363636}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.4005653594771242, 'y1': 0.8997541939393939, 'x2': 0.4715407450980392, 'y2': 0.9261690929292928}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.5033333333333333, 'y1': 0.6609148780303031, 'x2': 0.5666251450980391, 'y2': 0.7011681608585859}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.618890522875817, 'y1': 0.6678328073232324, 'x2': 0.7045005120915033, 'y2': 0.6942489689393939}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7702908496732026, 'y1': 0.6678328073232324, 'x2': 0.9176950049019607, 'y2': 0.6942489689393939}}, {'text': 'No\\n', 'bbox': {'x1': 0.525032679738562, 'y1': 0.7104385373737374, 'x2': 0.5449253222222221, 'y2': 0.7230175777777778}}, {'text': 'No\\n', 'bbox': {'x1': 0.525032679738562, 'y1': 0.745534496969697, 'x2': 0.5449253222222221, 'y2': 0.7581135373737374}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5231356209150326, 'y1': 0.7875483858585859, 'x2': 0.5468212140522875, 'y2': 0.8001274262626262}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5231356209150326, 'y1': 0.8295622747474747, 'x2': 0.5468212140522875, 'y2': 0.8421413151515151}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5231356209150326, 'y1': 0.8646582343434344, 'x2': 0.5468212140522875, 'y2': 0.8772372747474748}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5231356209150326, 'y1': 0.9066721232323232, 'x2': 0.5468212140522875, 'y2': 0.9192511636363636}}, {'text': 'NA\\n', 'bbox': {'x1': 0.6502271241830065, 'y1': 0.7104385373737374, 'x2': 0.6731638944444445, 'y2': 0.7230175777777778}}, {'text': 'NA\\n', 'bbox': {'x1': 0.6502271241830065, 'y1': 0.745534496969697, 'x2': 0.6731638944444445, 'y2': 0.7581135373737374}}, {'text': 'Librispeech, test/testother.\\n', 'bbox': {'x1': 0.7583104575163399, 'y1': 0.7104385373737374, 'x2': 0.9296769447712419, 'y2': 0.7230175777777778}}, {'text': 'Wall StreetJournal (WSJ),\\nNVIDIA K80 G-PU\\n', 'bbox': {'x1': 0.758968954248366, 'y1': 0.738615305050505, 'x2': 0.9290168620915032, 'y2': 0.7650314666666668}}, {'text': 'Librispeech\\n', 'bbox': {'x1': 0.6232696078431372, 'y1': 0.7875483858585859, 'x2': 0.7001216251633986, 'y2': 0.8001274262626262}}, {'text': 'Librispeech,\\nLibriVox\\n', 'bbox': {'x1': 0.6212352941176471, 'y1': 0.8226443454545455, 'x2': 0.7021570009803922, 'y2': 0.8490605070707071}}, {'text': 'Librispeech,\\nLibri-light\\n', 'bbox': {'x1': 0.6212352941176471, 'y1': 0.8577390424242424, 'x2': 0.7021570009803922, 'y2': 0.884155204040404}}, {'text': 'wav2vec 2.0,\\nYT-U, Libri-Light\\n', 'bbox': {'x1': 0.6023104575163399, 'y1': 0.8997541939393939, 'x2': 0.7210802771241831, 'y2': 0.9261690929292928}}, {'text': 'TIMIT, WSJ-\\nWall StreetJournal\\n', 'bbox': {'x1': 0.7838349673202615, 'y1': 0.780629193939394, 'x2': 0.9041512689542485, 'y2': 0.8070453555555555}}, {'text': 'Librispeech, Lib-\\nriVox, TIMIT\\n', 'bbox': {'x1': 0.7874820261437909, 'y1': 0.8226443454545455, 'x2': 0.900505444117647, 'y2': 0.8490605070707071}}, {'text': 'Librispeech(train-clean),\\nLibri-light(train-clean)\\nYouTube, English(US)\\nVoice search(VS), Speech-\\nStew, LibriSpeech,\\nCHiME6, Telephony\\n', 'bbox': {'x1': 0.7566748366013072, 'y1': 0.8577390424242424, 'x2': 0.9313133542483658, 'y2': 0.9400062141414142}}]}\n",
      "{'type': 'table', 'bbox': [0.052782597261316636, 0.10880206021395597, 0.9484051872702206, 0.2668438165838068], 'properties': {'score': 0.7095443606376648, 'title': None, 'columns': None, 'rows': None, 'page_number': 40}, 'text_representation': 'Whisper (Radford\\net al., 2022)\\n Transformer\\nTransducer\\n(Zhang\\net al., 2020b)\\nXLSR-Wav2Vec2\\n(Conneau\\net al., 2021)\\n Speech recogni-\\ntion, Translation,\\nLanguage\\nIdentification\\n Speech\\nrecognition\\n Speech\\nrecognition\\n 2022\\n Encoder &\\nDecoder\\n Yes\\n Not Mentioned\\n VoxLingua107, Lib-\\nriSpeeech, CoVoST2,\\nFleurs, Kincaid46\\n 2020\\n Encoder\\n No\\n NA\\n LibriSpeeech\\n 2020\\n Encoder\\n Yes\\n 53 languages\\ndatasets\\n CommonVoice, BA-\\nBEL,Multilingual\\nLibriSpeech(MLS)\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e013cb20>, 'tokens': [{'text': 'Whisper (Radford\\net al., 2022)\\n', 'bbox': {'x1': 0.06638562091503268, 'y1': 0.12571126464646465, 'x2': 0.18437406013071897, 'y2': 0.15212616363636366}}, {'text': 'Transformer\\nTransducer\\n(Zhang\\net al., 2020b)\\nXLSR-Wav2Vec2\\n(Conneau\\net al., 2021)\\n', 'bbox': {'x1': 0.06697058823529412, 'y1': 0.1677251535353535, 'x2': 0.1837869568627451, 'y2': 0.2638294464646464}}, {'text': 'Speech recogni-\\ntion, Translation,\\nLanguage\\nIdentification\\n', 'bbox': {'x1': 0.20811601307189542, 'y1': 0.11187414343434335, 'x2': 0.3192348163398692, 'y2': 0.1659632848484847}}, {'text': 'Speech\\nrecognition\\n', 'bbox': {'x1': 0.2266013071895425, 'y1': 0.18156227474747463, 'x2': 0.3007510506535948, 'y2': 0.20797843636363628}}, {'text': 'Speech\\nrecognition\\n', 'bbox': {'x1': 0.2266013071895425, 'y1': 0.23049409292929285, 'x2': 0.3007510506535948, 'y2': 0.25691025454545446}}, {'text': '2022\\n', 'bbox': {'x1': 0.34175816993464053, 'y1': 0.13262919393939385, 'x2': 0.3743156862745098, 'y2': 0.14520823434343433}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.4005653594771242, 'y1': 0.12571126464646465, 'x2': 0.4715407450980392, 'y2': 0.15212616363636366}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5231356209150326, 'y1': 0.13262919393939385, 'x2': 0.5468212140522875, 'y2': 0.14520823434343433}}, {'text': 'Not Mentioned\\n', 'bbox': {'x1': 0.6121830065359477, 'y1': 0.13262919393939385, 'x2': 0.7112066924836601, 'y2': 0.14520823434343433}}, {'text': 'VoxLingua107, Lib-\\nriSpeeech, CoVoST2,\\nFleurs, Kincaid46\\n', 'bbox': {'x1': 0.7738316993464052, 'y1': 0.1187920727272727, 'x2': 0.9141545947712417, 'y2': 0.15904535555555546}}, {'text': '2020\\n', 'bbox': {'x1': 0.34175816993464053, 'y1': 0.188480204040404, 'x2': 0.3743156862745098, 'y2': 0.20105924444444448}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.40893137254901957, 'y1': 0.188480204040404, 'x2': 0.46317219477124183, 'y2': 0.20105924444444448}}, {'text': 'No\\n', 'bbox': {'x1': 0.525032679738562, 'y1': 0.188480204040404, 'x2': 0.5449253222222221, 'y2': 0.20105924444444448}}, {'text': 'NA\\n', 'bbox': {'x1': 0.6502271241830065, 'y1': 0.188480204040404, 'x2': 0.6731638944444445, 'y2': 0.20105924444444448}}, {'text': 'LibriSpeeech\\n', 'bbox': {'x1': 0.8005947712418301, 'y1': 0.188480204040404, 'x2': 0.8873931098039216, 'y2': 0.20105924444444448}}, {'text': '2020\\n', 'bbox': {'x1': 0.34175816993464053, 'y1': 0.23741328484848478, 'x2': 0.3743156862745098, 'y2': 0.24999232525252527}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.40893137254901957, 'y1': 0.23741328484848478, 'x2': 0.46317219477124183, 'y2': 0.24999232525252527}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5231356209150326, 'y1': 0.23741328484848478, 'x2': 0.5468212140522875, 'y2': 0.24999232525252527}}, {'text': '53 languages\\ndatasets\\n', 'bbox': {'x1': 0.6189722222222221, 'y1': 0.23049409292929285, 'x2': 0.704419423856209, 'y2': 0.25691025454545446}}, {'text': 'CommonVoice, BA-\\nBEL,Multilingual\\nLibriSpeech(MLS)\\n', 'bbox': {'x1': 0.7775032679738562, 'y1': 0.22357616363636365, 'x2': 0.9104844434640521, 'y2': 0.2638294464646464}}]}\n",
      "{'type': 'Caption', 'bbox': [0.2821293550379136, 0.2760244196111506, 0.7190610638786765, 0.2911314530806108], 'properties': {'score': 0.7784678936004639, 'page_number': 40}, 'text_representation': 'Table 23: Transformer models for audio & speech recognition task\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0906041852165671, 0.30716411243785513, 0.9112943761488971, 0.3775090997869318], 'properties': {'score': 0.7845906019210815, 'page_number': 40}}\n",
      "{'type': 'List-item', 'bbox': [0.09083708819221048, 0.3807852727716619, 0.9114353314568014, 0.46504824551669033], 'properties': {'score': 0.8062103390693665, 'page_number': 40}, 'text_representation': '• Conformer: The Conformer architecture is a model that combines the advantages of both the Transformer and convolutional\\nneural network (CNN) for automatic speech recognition tasks. While the Transformer is proficient at capturing global\\nfeatures, and CNN excels at capturing local features, the Conformer architecture leverages the strengths of both to achieve\\nsuperior performance. Recent studies have shown that the Conformer architecture outperforms both CNN and Transformer\\nmodels individually, thereby setting a new state-of-the-art in automatic speech recognition performance (Gulati et al., 2020).\\n• Speech Transformer: It is a speech recognition model published in 2018 which is one of the earliest Transformer inspired\\nspeech models. It eliminates the conventional Recurrent Neural Network (RNN)-based approach in speech processing and\\ninstead applies an attention mechanism. The introduction of the Transformer model into speech recognition has led to a\\nnumber of benefits. For instance, the training time and memory usage become lower, allowing for better scalability. This is\\nespecially helpful for tasks that require a long-term dependency due to the elimination of recurrence sequence-to-sequence\\nprocessing (Dong et al., 2018).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0914743221507353, 0.4681787109375, 0.9108656939338236, 0.5386963445490057], 'properties': {'score': 0.8341629505157471, 'page_number': 40}, 'text_representation': '• Wav2vec 2.0: Wav2Vec 2.0 is a self-supervised model which uses discretization algorithms to capture the vocabulary from\\nraw speech representation. The learned vocabulary is passed through an architecture consisting of multi-layer convolutional\\nfeature encoder. This encoder has multiple convolution layers, layer normalization and an activation function, with the\\naudio representations being masked during the training process. Wav2Vec 2.0 offers the advantage of performing well on\\nspeech recognition tasks with a small amount of supervised data (Baevski et al., 2020b).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09128952026367188, 0.5418716708096591, 0.9118166934742648, 0.6254194779829545], 'properties': {'score': 0.8231377005577087, 'page_number': 40}, 'text_representation': '• VQ-Wav2vec: VQ-Wav2Vec is a Transformer based model that enables Vector Quantization-VQ tasks to be executed in a\\nself-supervised way. It is built on the Wav2Vec model, which is an effective way of compressing continuous signals into\\ndiscrete symbols. Unlike other models, VQ-Wav2Vec trains without the need of unlabeled data. Instead, corrupted speech\\nis used, and the model learns by predicting the missing parts of the speech. This process of training has been proven to be\\nhighly effective, and the model is capable of achieving a higher accuracy when compared to other models (Baevski et al.,\\n2020a).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09101319256950827, 0.6298955189098011, 0.9117518525965074, 0.7139610706676136], 'properties': {'score': 0.8414577841758728, 'page_number': 40}}\n",
      "{'type': 'List-item', 'bbox': [0.09054926255170037, 0.7167802290482954, 0.9118204273897059, 0.8291245339133523], 'properties': {'score': 0.8543652892112732, 'page_number': 40}, 'text_representation': '• BigSSL: BigSSL is a large-scale semi-supervised Transformer-based model designed specifically for speech recognition\\ntasks. Obtaining labeled speech data is a challenging and time-consuming process, while the availability of unlabeled data\\nis considerably vast. In this context, the BigSSL model proposes a novel approach to enhance the performance of speech\\nrecognition tasks. By leveraging a smaller portion of labeled data in conjunction with a substantial amount of unlabeled\\ndata, this model achieves improved performance. Furthermore, the utilization of a larger quantity of unlabeled data helps\\nalleviate the overfitting issue, thereby further enhancing the overall performance of the BigSSL model (Zhang et al., 2022).\\n• HuBERT: HuBERT, short for Hidden-Unit BERT, is a self-supervised speech representation model. Its approach involves\\noffline clustering for feature representation, with the loss calculation restricted to the masked regions. This emphasis allows\\nthe model to effectively learn a combination of acoustic and language models over the input data. HuBERT consists of a\\nconvolutional waveform encoder, a projection layer, a BERT encoder, and a code embedding layer. The CNN component\\ngenerates feature representations, which are then subjected to random masking. These masked representations are subse-\\nquently passed through the BERT encoder, yielding another set of feature representations. HuBERT’s functioning resembles\\nthat of a mask language model and has demonstrated notable performance in speech representation tasks, particularly speech\\nrecognition (Hsu et al., 2021).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09094145830939798, 0.8318069735440341, 0.9105739458869485, 0.9164458118785511], 'properties': {'score': 0.8942064642906189, 'page_number': 40}, 'text_representation': '• Transformer Transducer:\\n It is a speech recognition model that capitalizes on the strengths of both the self-attention\\nmechanism of the Transformer and the recurrent neural network (RNN). This model is constructed by integrating the en-\\ncoder module of the transformer with the RNN-T loss function. The encoder module is responsible for extracting speech\\nrepresentations, while the RNN-T component utilizes this information to make real-time predictions of the transcript, fa-\\ncilitating a swift response—an essential requirement for speech recognition tasks (Transformer-Transducer (Zhang et al.,\\n2020b).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09193289364085477, 0.9201323353160511, 0.910235164866728, 0.9483827903053977], 'properties': {'score': 0.8338748216629028, 'page_number': 40}, 'text_representation': '• Whisper: It is a noteworthy speech recognition model that emerged in late 2022, specifically designed to address the\\nchallenging task of recognizing speech with low volume. The uniqueness of this model lies in its dedicated efforts to\\n'}\n",
      "{'type': 'Text', 'bbox': [0.10590419096105239, 0.10035756197842685, 0.9114410041360295, 0.15630773370916193], 'properties': {'score': 0.8998981714248657, 'page_number': 41}, 'text_representation': 'improve low-volume speech recognition. Whisper adopts a training approach that incorporates a lower level of speech\\ndata and leverages weak supervision methods, enabling training on a larger corpus of data. This strategic approach has\\nproven instrumental in enhancing the performance of the Whisper model, enabling it to effectively capture and comprehend\\nlow-level speech phenomena (Radford et al., 2022).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09222743314855239, 0.1609354053844105, 0.9111582318474265, 0.2311314669522372], 'properties': {'score': 0.7889519929885864, 'page_number': 41}, 'text_representation': '• XLSR-Wav2Vec2: This model demonstrates the capability to recognize speech across multiple languages, eliminating\\nthe need for extensive labeled data in each language for training. By learning the relationships and shared characteristics\\namong different languages, this model surpasses the requirement of training on specific language-labeled speech data.\\nConsequently, the XLSR-Wav2Vec2 model offers an efficient solution for multiple-language speech recognition, requiring\\nsignificantly less data for training while adhering to the architectural principles of Wav2Vec2 (Conneau et al., 2021).\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09441204295438879, 0.23937463933771308, 0.2956419641831342, 0.2535175947709517], 'properties': {'score': 0.6949031949043274, 'page_number': 41}}\n",
      "{'type': 'Text', 'bbox': [0.09410160737879136, 0.2572884576970881, 0.9129672420726103, 0.3964405684037642], 'properties': {'score': 0.9291884899139404, 'page_number': 41}, 'text_representation': '6.5.2 SPEECH SEPARATION\\nIt poses a considerable challenge within the field of audio signal processing. It involves the task of separating the desired\\nspeech signal, which may include various sources such as different speakers or human voices, from additional sounds such\\nas background noise or interfering sources. In the domain of speech separation, three commonly employed methods are\\nfollowed: (i) Blind source separation, (ii) Beamforming, and (iii) Single-channel speech separation. The significance of\\nspeech separation has grown with the increasing popularity of automatic speech recognition (ASR) systems.\\nIt is often\\nemployed as a preprocessing step for speech recognition tasks. The accurate distinction between the desired speech signal\\nand unwanted noise is crucial to ensure precise speech recognition results. Failure to properly segregate the desired speech\\nfrom interfering noise can lead to erroneous speech recognition outcomes (Wang & Chen, 2018, Huang et al., 2014). In this\\ncontext, we present several Transformer-based models that have showcased noteworthy advancements in audio and speech\\nseparation tasks. The details of these models are presented in Table 24.\\n'}\n",
      "{'type': 'table', 'bbox': [0.05266857371610754, 0.4161614990234375, 0.9395407284007353, 0.6301320578835228], 'properties': {'score': 0.6300013661384583, 'title': None, 'columns': None, 'rows': None, 'page_number': 41}, 'text_representation': 'Transformer\\nModels\\n DPTNeT (Chen\\net al., 2020a)\\nSepformer\\n(Subakan\\net al., 2021)\\n WavLM (Chen\\net al., 2022a)\\n Task Ac-\\ncomplished\\n Speech Sep-\\naration\\n Speech Sep-\\naration\\n Speech sepa-\\nration, speech\\ndenoising, speech\\nprediction,\\nSpeaker Verifi-\\ncation, Speech\\nrecognition\\n Year\\n 2020\\n 2021\\n Architecture\\n(Encoder/\\nDecoder)\\nEncoder &\\nDecoder\\n Encoder &\\nDecoder\\n Pre-\\ntrained\\n(Yes/NO)\\n No\\n No\\n Pre-training\\nDataset\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n NA\\n NA\\n WSJ0-2mix, LS-2mix\\n WSJ0-2mix, WSJ0-3mix\\n 2022\\n Encoder\\n Yes\\n GigaSpeech,\\nVoxPopuli\\n VoxCeleb1, Vox-\\nCeleb2, Switchboard-2\\nCALLHOME, LIB-\\nRICSS, LibriSpeech\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e013dc90>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.07634640522875817, 'y1': 0.4253264941919192, 'x2': 0.16514703104575165, 'y2': 0.4517426558080808}}, {'text': 'DPTNeT (Chen\\net al., 2020a)\\nSepformer\\n(Subakan\\net al., 2021)\\n', 'bbox': {'x1': 0.06898039215686275, 'y1': 0.46101303232323226, 'x2': 0.17251329411764707, 'y2': 0.5294430828282829}}, {'text': 'WavLM (Chen\\net al., 2022a)\\n', 'bbox': {'x1': 0.07205718954248365, 'y1': 0.5657971232323232, 'x2': 0.16943672091503267, 'y2': 0.5922132848484849}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.21506535947712418, 'y1': 0.4253264941919192, 'x2': 0.29375687647058824, 'y2': 0.4517426558080808}}, {'text': 'Speech Sep-\\naration\\n', 'bbox': {'x1': 0.21395098039215685, 'y1': 0.46101303232323226, 'x2': 0.29487268725490196, 'y2': 0.48742919393939393}}, {'text': 'Speech Sep-\\naration\\n', 'bbox': {'x1': 0.21395098039215685, 'y1': 0.49610899191919194, 'x2': 0.29487268725490196, 'y2': 0.5225251535353534}}, {'text': 'Speech sepa-\\nration, speech\\ndenoising, speech\\nprediction,\\nSpeaker Verifi-\\ncation, Speech\\nrecognition\\n', 'bbox': {'x1': 0.1965408496732026, 'y1': 0.5312049515151515, 'x2': 0.3122828202614379, 'y2': 0.6268054565656566}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3324934640522876, 'y1': 0.43224442348484854, 'x2': 0.3650347016339869, 'y2': 0.44482346388888894}}, {'text': '2020\\n', 'bbox': {'x1': 0.3324934640522876, 'y1': 0.4679322242424242, 'x2': 0.36505098039215683, 'y2': 0.48051126464646465}}, {'text': '2021\\n', 'bbox': {'x1': 0.3324934640522876, 'y1': 0.5030269212121213, 'x2': 0.36505098039215683, 'y2': 0.5156059616161617}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\nEncoder &\\nDecoder\\n', 'bbox': {'x1': 0.3851029411764706, 'y1': 0.4184085648989899, 'x2': 0.47310590784313733, 'y2': 0.48742919393939393}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.39130065359477123, 'y1': 0.49610899191919194, 'x2': 0.4622760392156863, 'y2': 0.5225251535353534}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.4940686274509804, 'y1': 0.4184085648989899, 'x2': 0.5573604392156862, 'y2': 0.45866058510101015}}, {'text': 'No\\n', 'bbox': {'x1': 0.5157679738562091, 'y1': 0.4679322242424242, 'x2': 0.5356606163398692, 'y2': 0.48051126464646465}}, {'text': 'No\\n', 'bbox': {'x1': 0.5157679738562091, 'y1': 0.5030269212121213, 'x2': 0.5356606163398692, 'y2': 0.5156059616161617}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.6096258169934641, 'y1': 0.4253264941919192, 'x2': 0.6952358062091503, 'y2': 0.4517426558080808}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7610277777777779, 'y1': 0.4253264941919192, 'x2': 0.9084319330065359, 'y2': 0.4517426558080808}}, {'text': 'NA\\n', 'bbox': {'x1': 0.6409624183006536, 'y1': 0.4679322242424242, 'x2': 0.6638991885620915, 'y2': 0.48051126464646465}}, {'text': 'NA\\n', 'bbox': {'x1': 0.6409624183006536, 'y1': 0.5030269212121213, 'x2': 0.6638991885620915, 'y2': 0.5156059616161617}}, {'text': 'WSJ0-2mix, LS-2mix\\n', 'bbox': {'x1': 0.7628251633986928, 'y1': 0.4679322242424242, 'x2': 0.9066317130718954, 'y2': 0.48051126464646465}}, {'text': 'WSJ0-2mix, WSJ0-3mix\\n', 'bbox': {'x1': 0.7528790849673203, 'y1': 0.5030269212121213, 'x2': 0.916578277124183, 'y2': 0.5156059616161617}}, {'text': '2022\\n', 'bbox': {'x1': 0.3324934640522876, 'y1': 0.5727163151515152, 'x2': 0.36505098039215683, 'y2': 0.5852953555555556}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.39966830065359477, 'y1': 0.5727163151515152, 'x2': 0.45390912287581703, 'y2': 0.5852953555555556}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5138709150326797, 'y1': 0.5727163151515152, 'x2': 0.5375565081699345, 'y2': 0.5852953555555556}}, {'text': 'GigaSpeech,\\nVoxPopuli\\n', 'bbox': {'x1': 0.6111078431372549, 'y1': 0.5657971232323232, 'x2': 0.6937550983660131, 'y2': 0.5922132848484849}}, {'text': 'VoxCeleb1, Vox-\\nCeleb2, Switchboard-2\\nCALLHOME, LIB-\\nRICSS, LibriSpeech\\n', 'bbox': {'x1': 0.7596764705882353, 'y1': 0.5519600020202021, 'x2': 0.9097828996732027, 'y2': 0.6060504060606061}}]}\n",
      "{'type': 'Caption', 'bbox': [0.2867856014476103, 0.6398324307528409, 0.7167930692784926, 0.6542923805930397], 'properties': {'score': 0.7121231555938721, 'page_number': 41}, 'text_representation': 'Table 24: Transformer models for audio & speech separation task\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09132527968462775, 0.6729914439808239, 0.9124070111443014, 0.7424973921342329], 'properties': {'score': 0.8444838523864746, 'page_number': 41}, 'text_representation': '• Sepformer: The sepformer model was published in a paper titled “Attention is all you need in speech separation” which\\nuses an attention mechanism to separate speeches that are overlapped. This model does not contain any kind of recurrence\\nscheme and it follows the self-attention mechanisms. Sepformer uses a binary mask prediction scheme for training while\\nthis masking network captures both short and long-term dependencies and provide higher accuracy in performance (Subakan\\net al., 2021).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09136280732996324, 0.7474283669211648, 0.9120557358685661, 0.8168196244673296], 'properties': {'score': 0.8963066339492798, 'page_number': 41}, 'text_representation': '• DPTNeT: DPTNeT stands for Dual-Path Transformer Network for monaural speech separation tasks. This model trained\\ndirectly minimizes the error between estimated and target value which is called end-to-end processing. This model uses\\ndual-path architecture, replacing positional encoding with the RNN in the transformer architecture which helps to capture\\ncomplex features of the signal and improves the performance of the speech separation from the overlapped speech (Chen\\net al., 2020a).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09150129430434283, 0.8217519309303978, 0.9118117388556986, 0.8770825750177557], 'properties': {'score': 0.8874329328536987, 'page_number': 41}, 'text_representation': '• WavLM: WavLM is a large-scale pre-trained model that can execute a range of tasks for speech. WavLM follows BERT\\ninspired speech processing model-HuBERT, whereas, with the help of mask speech prediction, the model predicts the actual\\nspeech by removing the noise from the corrupted speech. By this way, this model is trained for a variety of tasks besides\\nautomatic speech recognition-ASR task (Chen et al., 2022a).\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09396763521082262, 0.8868061412464489, 0.32392732508042277, 0.9002487460049716], 'properties': {'score': 0.7161052823066711, 'page_number': 41}}\n",
      "{'type': 'Text', 'bbox': [0.09387442196116728, 0.9042770663174716, 0.9108545639935661, 0.9456000310724432], 'properties': {'score': 0.9072309732437134, 'page_number': 41}, 'text_representation': '6.5.3 SPEECH CLASSIFICATION\\nThe speech classification task refers to the ability to categorize input speech or audio into distinct categories based on various\\nfeatures, including speaker, words, phrases, language, and more. There exist several speech classifiers, such as voice activ-\\nity detection (binary/multi-class), speech detection (multi-class), language identification, speech enhancement, and speaker\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09357511632582721, 0.10047584533691406, 0.9116572122012868, 0.156309550892223], 'properties': {'score': 0.8958585262298584, 'page_number': 42}, 'text_representation': 'identification. Speech classification plays a crucial role in identifying important speech signals, enabling the extraction of\\nrelevant information from large speech datasets (Livezey et al., 2019, Gu et al., 2017). In this context, we present a compi-\\nlation of transformer-based models, which have demonstrated superior accuracy in speech classification tasks compared to\\nconventional models. The details of these models are depicted in Table 25.\\n'}\n",
      "{'type': 'table', 'bbox': [0.0525463238884421, 0.18075736305930398, 0.9396456370634191, 0.42161923495205966], 'properties': {'score': 0.6714600324630737, 'title': None, 'columns': None, 'rows': None, 'page_number': 42}, 'text_representation': 'Transformer\\nModels\\n AST (Gong\\net al., 2021)\\n Mockingjay\\n(Liu et al., 2020)\\n XLS-R (Babu\\net al., 2022)\\n UniSpeech-\\nSAT (Chen\\net al., 2022b)\\n Task Ac-\\ncomplished\\n Audio Clas-\\nsification\\nSpeech Clas-\\nsification &\\nrecognition\\n Speech Clas-\\nsification,\\nSpeech Trans-\\nlation, Speech\\nRecognition\\n Speech Clas-\\nsification &\\nRecognition\\n Year\\n Architecture\\n(Encoder/\\nDecoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n 2021\\n Encoder\\n 2020\\n Encoder\\n 2021\\n Encoder &\\nDecoder\\n Yes\\n Yes\\n Yes\\n 2022\\n Encoder\\n Yes\\n Pre-training\\nDataset\\n ImageNeT\\n LibriSpeech\\n VoxPopuli (VP-\\n400K), Multilingual\\nLibrispeech\\n(MLS), Common-\\nVoice, VoxLin-\\ngua107, BABEL\\nLibriVox, Lib-\\nrispeech Gi-\\ngaSpeech, VoxPopuli\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n AudioSet, ESC-50,\\nSpeech Commands\\n LibriSpeech test\\nclean, MOSEI\\n VoxPopuli, Multilingual\\nLibrispeech (MLS),\\nCommonVoice, BABEL\\n SUPERB\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e013efb0>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.07634640522875817, 'y1': 0.18986437297979805, 'x2': 0.16514703104575165, 'y2': 0.21628053459595953}}, {'text': 'AST (Gong\\net al., 2021)\\n', 'bbox': {'x1': 0.08186601307189544, 'y1': 0.2250483858585858, 'x2': 0.15962964084967318, 'y2': 0.25146454747474745}}, {'text': 'Mockingjay\\n(Liu et al., 2020)\\n', 'bbox': {'x1': 0.06581535947712418, 'y1': 0.26014434545454546, 'x2': 0.17568069836601305, 'y2': 0.28656050707070707}}, {'text': 'XLS-R (Babu\\net al., 2022)\\n', 'bbox': {'x1': 0.0754673202614379, 'y1': 0.3229132848484848, 'x2': 0.16602605196078435, 'y2': 0.3493294464646464}}, {'text': 'UniSpeech-\\nSAT (Chen\\net al., 2022b)\\n', 'bbox': {'x1': 0.07779575163398693, 'y1': 0.3787655575757575, 'x2': 0.16369875849673202, 'y2': 0.4190175777777778}}, {'text': 'Task Ac-\\ncomplished\\n', 'bbox': {'x1': 0.21506535947712418, 'y1': 0.18986437297979805, 'x2': 0.29375687647058824, 'y2': 0.21628053459595953}}, {'text': 'Audio Clas-\\nsification\\nSpeech Clas-\\nsification &\\nrecognition\\n', 'bbox': {'x1': 0.21168790849673202, 'y1': 0.2250483858585858, 'x2': 0.29713511013071897, 'y2': 0.2934784363636363}}, {'text': 'Speech Clas-\\nsification,\\nSpeech Trans-\\nlation, Speech\\nRecognition\\n', 'bbox': {'x1': 0.20791176470588232, 'y1': 0.3021582343434343, 'x2': 0.3009123101307189, 'y2': 0.37008575959595935}}, {'text': 'Speech Clas-\\nsification &\\nRecognition\\n', 'bbox': {'x1': 0.21168790849673202, 'y1': 0.3787655575757575, 'x2': 0.29713511013071897, 'y2': 0.4190175777777778}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3324934640522876, 'y1': 0.19678356489898985, 'x2': 0.3650347016339869, 'y2': 0.2093626053030302}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\n', 'bbox': {'x1': 0.3851029411764706, 'y1': 0.18294644368686871, 'x2': 0.47310590784313733, 'y2': 0.22319972651515133}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.4940686274509804, 'y1': 0.18294644368686871, 'x2': 0.5573604392156862, 'y2': 0.22319972651515133}}, {'text': '2021\\n', 'bbox': {'x1': 0.3324934640522876, 'y1': 0.23196757777777774, 'x2': 0.36505098039215683, 'y2': 0.24454661818181822}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.39966830065359477, 'y1': 0.23196757777777774, 'x2': 0.45390912287581703, 'y2': 0.24454661818181822}}, {'text': '2020\\n', 'bbox': {'x1': 0.3324934640522876, 'y1': 0.2670622747474747, 'x2': 0.36505098039215683, 'y2': 0.27964131515151514}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.39966830065359477, 'y1': 0.2670622747474747, 'x2': 0.45390912287581703, 'y2': 0.27964131515151514}}, {'text': '2021\\n', 'bbox': {'x1': 0.3324934640522876, 'y1': 0.32983247676767674, 'x2': 0.36505098039215683, 'y2': 0.34241151717171725}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.39130065359477123, 'y1': 0.3229132848484848, 'x2': 0.4622760392156863, 'y2': 0.3493294464646464}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5138709150326797, 'y1': 0.23196757777777774, 'x2': 0.5375565081699345, 'y2': 0.24454661818181822}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5138709150326797, 'y1': 0.2670622747474747, 'x2': 0.5375565081699345, 'y2': 0.27964131515151514}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5138709150326797, 'y1': 0.32983247676767674, 'x2': 0.5375565081699345, 'y2': 0.34241151717171725}}, {'text': '2022\\n', 'bbox': {'x1': 0.3324934640522876, 'y1': 0.39260267878787886, 'x2': 0.36505098039215683, 'y2': 0.4051817191919192}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.39966830065359477, 'y1': 0.39260267878787886, 'x2': 0.45390912287581703, 'y2': 0.4051817191919192}}, {'text': 'Yes\\n', 'bbox': {'x1': 0.5138709150326797, 'y1': 0.39260267878787886, 'x2': 0.5375565081699345, 'y2': 0.4051817191919192}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.6096258169934641, 'y1': 0.18986437297979805, 'x2': 0.6952358062091503, 'y2': 0.21628053459595953}}, {'text': 'ImageNeT\\n', 'bbox': {'x1': 0.6176274509803922, 'y1': 0.23196757777777774, 'x2': 0.6872354209150326, 'y2': 0.24454661818181822}}, {'text': 'LibriSpeech\\n', 'bbox': {'x1': 0.6126454248366013, 'y1': 0.2670622747474747, 'x2': 0.6922159947712418, 'y2': 0.27964131515151514}}, {'text': 'VoxPopuli (VP-\\n400K), Multilingual\\nLibrispeech\\n(MLS), Common-\\nVoice, VoxLin-\\ngua107, BABEL\\nLibriVox, Lib-\\nrispeech Gi-\\ngaSpeech, VoxPopuli\\n', 'bbox': {'x1': 0.5829852941176471, 'y1': 0.29524030505050497, 'x2': 0.7218756588235294, 'y2': 0.4190175777777778}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7610277777777779, 'y1': 0.18986437297979805, 'x2': 0.9084319330065359, 'y2': 0.21628053459595953}}, {'text': 'AudioSet, ESC-50,\\nSpeech Commands\\n', 'bbox': {'x1': 0.7720964052287582, 'y1': 0.2250483858585858, 'x2': 0.8973614493464052, 'y2': 0.25146454747474745}}, {'text': 'LibriSpeech test\\nclean, MOSEI\\n', 'bbox': {'x1': 0.7816029411764706, 'y1': 0.26014434545454546, 'x2': 0.887854395751634, 'y2': 0.28656050707070707}}, {'text': 'VoxPopuli, Multilingual\\nLibrispeech (MLS),\\nCommonVoice, BABEL\\n', 'bbox': {'x1': 0.7555735294117646, 'y1': 0.31599535555555547, 'x2': 0.9138844526143789, 'y2': 0.35624863838383813}}, {'text': 'SUPERB\\n', 'bbox': {'x1': 0.8039705882352941, 'y1': 0.39260267878787886, 'x2': 0.8654880153594772, 'y2': 0.4051817191919192}}]}\n",
      "{'type': 'Caption', 'bbox': [0.2772034948012408, 0.4326425448330966, 0.7248427447150735, 0.4461298162286932], 'properties': {'score': 0.763434112071991, 'page_number': 42}, 'text_representation': 'Table 25: Transformer models for audio & speech classification task\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09081516041475184, 0.46814663973721593, 0.9122252699908088, 0.5506000865589489], 'properties': {'score': 0.731532871723175, 'page_number': 42}, 'text_representation': '• AST: AST - Audio Spectrogram Transformer is a transformer-based model which is applied to an audio spectrogram. AST\\nis the first audio classification model where the convolution was not used and it is capable of capturing long-range frames\\ncontext. It used a transformer encoder to capture the features in the audio spectrogram, a linear projection layer and sigmoid\\nactivation function to capture the audio spectrogram representation for audio classification. As the attention mechanism is\\nrenowned for capturing global features so it shows significant performance in audio/speech classification tasks (Gong et al.,\\n2021).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0908706485523897, 0.5574415172230114, 0.9108246208639705, 0.6266110506924716], 'properties': {'score': 0.8272743821144104, 'page_number': 42}, 'text_representation': '• Mockingjay: Mockingjay is an unsupervised speech representation model that uses multiple layers of bidirectional Trans-\\nformer pre-trained encoders. It uses both past and future features for speech representation rather than only past information,\\nwhich helps it to gather more information about the speech context. Mockingjay also can improve the performance of the\\nsupervised learning tasks as well where the amount of labeled data is low. Capturing more information helped to improve\\nseveral speech representational tasks like speech classification and recognition (Liu et al., 2020).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0908975040211397, 0.6329032759232954, 0.9119765337775735, 0.71596435546875], 'properties': {'score': 0.831623375415802, 'page_number': 42}, 'text_representation': '• XLS-R: XLS-R is a transformer-based self-supervised large-scale speech representation model which is trained with a large\\namount of data. It is built on the wav2vec discretization algorithm, whereas it uses Wav2Vec 2.0 model that is pretrained\\nwith multiple languages. The architecture contains multiple convolution encoders to map raw speech and the output from\\nthis stage is transferred to the transformer model(encoder module) as input which provides better audio representation\\nfinally. A large amount of training data is crucial for this model where a range of public speech is used and it performed\\nwell for multiple downstream multilingual speech tasks (Babu et al., 2022).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0915407517377068, 0.7225008877840909, 0.9109597598805147, 0.7774153275923296], 'properties': {'score': 0.8396921753883362, 'page_number': 42}, 'text_representation': '• UniSpeech: UniSpeech is a semi-supervised unified pre-trained model for speech representation. This model follows\\nthe Wav2Vec 2.0 architecture where it contains convolutional feature encoders that converts the raw audio to a higher-\\ndimensional representation and this output is fed into the Transformer. This model is capable of learning to multitask while\\na quantizer is used in its architecture which helps to capture specific speech recognition information (Chen et al., 2022b).\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09371877333697151, 0.7901172429865057, 0.2808453369140625, 0.8036202170632102], 'properties': {'score': 0.6403748989105225, 'page_number': 42}, 'text_representation': '6.6 SIGNAL PROCESSING\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09454625746783088, 0.8099631569602272, 0.9125731703814338, 0.9480876020951704], 'properties': {'score': 0.9295179843902588, 'page_number': 42}, 'text_representation': 'With the growing recognition of the usability of Transformer-based models across various sectors, researchers have started\\nexploring their application in signal processing. This recent development of utilizing Transformer-based models in signal\\nprocessing represents a novel approach that outperforms conventional methods in terms of performance. Signal processing\\ninvolves the manipulation and analysis of various types of data, including signal status, information, frequency, amplitude,\\nand more. While audio and speech are considered forms of signals, we have segregated the audio and speech sections to\\nhighlight the specific applications of Transformer-based models in those domains. Within the signal processing domain, we\\nhave focused on two distinct areas: wireless network signal processing and medical signal processing. These two fields\\nexhibit distinct processing methods and functionalities due to their inherent differences. Here, we delve into both of these\\ntasks and provide an overview of significant Transformer-based models that have demonstrated effectiveness in these specific\\ndomains.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09412108477424173, 0.10034482782537286, 0.4688758401309743, 0.11483108520507812], 'properties': {'score': 0.7046164870262146, 'page_number': 43}}\n",
      "{'type': 'Text', 'bbox': [0.09374889598173254, 0.118455810546875, 0.9134535845588235, 0.2439912275834517], 'properties': {'score': 0.909462571144104, 'page_number': 43}, 'text_representation': '6.6.1 WIRELESS NETWORK & SIGNAL PROCESSING\\nIn the current era of the 21st century, wireless network communication has emerged as a prominent technology. However, the\\napplication of transformers in wireless network signal processing has not received substantial attention thus far. Consequently,\\nthe number of Transformer-inspired models developed for this field remains limited. Wireless network signal processing\\nencompasses various tasks, including signal denoising, signal interface detection, wireless signal channel estimation, interface\\nidentification, signal classification, and more. Deep neural networks offer great potential for tackling these tasks effectively,\\nand Transformer-based models have introduced significant advancements in this domain (Sun et al., 2017, Clerckx et al.,\\n2021, Zhang et al., 2019, Chen et al., 2019b). In this section, we present several models that have made notable contributions\\nto the enhancements in wireless communication networks and signal processing. The details of these models are provided in\\nTable 26.\\n'}\n",
      "{'type': 'table', 'bbox': [0.05319684645708869, 0.2647647372159091, 0.9438789636948529, 0.5902231112393466], 'properties': {'score': 0.7037003040313721, 'title': None, 'columns': None, 'rows': None, 'page_number': 43}, 'text_representation': 'Transformer\\nModels\\n Task Accomplished Year\\n Architecture\\n(Encoder/\\nDecoder)\\n Pre-\\ntrained\\n(Yes/NO)\\n Pre-training\\nDataset\\n Dataset (Fine-tuning,\\nTraining, Testing)\\n Signal detection,\\nchannel estimation,\\ninterference\\nsuppression, and\\ndata decoding in\\nMIMO-OFDM\\nRemove Interference\\nand nose from\\nwireless signal\\n Wireless interface\\nidentification\\n Automatic modula-\\ntion classification\\ncomplex raw\\nradio signals\\n Compress &\\nrecover channel\\nstate information\\n SigT (Ren\\net al., 2022)\\n TSDN (Liu\\net al., 2022b)\\n ACNNT\\n(Wang et al.,\\n2021a)\\n MCformer\\n(Hamidi-Rad\\n& Jain, 2021)\\n Quan-\\nTransformer\\n(Xie et al.,\\n2022)\\n 2022\\n Encoder\\n No\\n 2022\\n Encoder &\\nDecoder\\n 2021\\n Encoder\\n 2021\\n Encoder\\n 2022\\n Encoder &\\nDecoder\\n No\\n No\\n No\\n No\\n NA\\n NA\\n NA\\n NA\\n NA\\n Peng Cheng labora-\\ntory(PCL), local area data\\n Wall NLoS, Foil\\nNLOS, UWB dataset\\n ST, BPSK, AM, NAM,\\nSFM, LFM, 4FSK,\\n2FSK signal dataset\\n RadioML2016.10b\\n CsiNet, CLNet and CRNet\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e01747c0>, 'tokens': [{'text': 'Transformer\\nModels\\n', 'bbox': {'x1': 0.06662091503267974, 'y1': 0.27431008005050506, 'x2': 0.15542154084967322, 'y2': 0.30072624166666656}}, {'text': 'Task Accomplished Year\\n', 'bbox': {'x1': 0.17968954248366012, 'y1': 0.28122800934343445, 'x2': 0.36920463627450983, 'y2': 0.2938070497474748}}, {'text': 'Architecture\\n(Encoder/\\nDecoder)\\n', 'bbox': {'x1': 0.3892712418300654, 'y1': 0.2673908881313131, 'x2': 0.4772742084967321, 'y2': 0.3076441709595958}}, {'text': 'Pre-\\ntrained\\n(Yes/NO)\\n', 'bbox': {'x1': 0.4982385620915032, 'y1': 0.2673908881313131, 'x2': 0.5615303738562091, 'y2': 0.3076441709595958}}, {'text': 'Pre-training\\nDataset\\n', 'bbox': {'x1': 0.6137957516339869, 'y1': 0.27431008005050506, 'x2': 0.6994057408496732, 'y2': 0.30072624166666656}}, {'text': 'Dataset (Fine-tuning,\\nTraining, Testing)\\n', 'bbox': {'x1': 0.7651960784313726, 'y1': 0.27431008005050506, 'x2': 0.9126002336601308, 'y2': 0.30072624166666656}}, {'text': 'Signal detection,\\nchannel estimation,\\ninterference\\nsuppression, and\\ndata decoding in\\nMIMO-OFDM\\nRemove Interference\\nand nose from\\nwireless signal\\n', 'bbox': {'x1': 0.17876960784313725, 'y1': 0.3094928303030302, 'x2': 0.31523443758169933, 'y2': 0.43327136565656565}}, {'text': 'Wireless interface\\nidentification\\n', 'bbox': {'x1': 0.1884232026143791, 'y1': 0.4419511636363636, 'x2': 0.3055814251633987, 'y2': 0.4683673252525252}}, {'text': 'Automatic modula-\\ntion classification\\ncomplex raw\\nradio signals\\n', 'bbox': {'x1': 0.18347385620915033, 'y1': 0.4770471232323233, 'x2': 0.31052956372549023, 'y2': 0.5311362646464647}}, {'text': 'Compress &\\nrecover channel\\nstate information\\n', 'bbox': {'x1': 0.19161274509803922, 'y1': 0.5398160626262626, 'x2': 0.3023896944444444, 'y2': 0.5800693454545455}}, {'text': 'SigT (Ren\\net al., 2022)\\n', 'bbox': {'x1': 0.07213888888888889, 'y1': 0.33716707272727264, 'x2': 0.14990251666666665, 'y2': 0.3635832343434343}}, {'text': 'TSDN (Liu\\net al., 2022b)\\n', 'bbox': {'x1': 0.0680702614379085, 'y1': 0.39993727474747476, 'x2': 0.15397326830065358, 'y2': 0.4263534363636363}}, {'text': 'ACNNT\\n(Wang et al.,\\n2021a)\\n', 'bbox': {'x1': 0.06963235294117648, 'y1': 0.43503323434343444, 'x2': 0.15240983823529414, 'y2': 0.4752852545454546}}, {'text': 'MCformer\\n(Hamidi-Rad\\n& Jain, 2021)\\n', 'bbox': {'x1': 0.06648202614379084, 'y1': 0.48396505252525246, 'x2': 0.15555939084967318, 'y2': 0.5242183353535353}}, {'text': 'Quan-\\nTransformer\\n(Xie et al.,\\n2022)\\n', 'bbox': {'x1': 0.07062581699346406, 'y1': 0.5328981333333332, 'x2': 0.1514172937908497, 'y2': 0.5869885373737374}}, {'text': '2022\\n', 'bbox': {'x1': 0.3366633986928105, 'y1': 0.34408626464646463, 'x2': 0.36922091503267973, 'y2': 0.35666530505050503}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.4038366013071895, 'y1': 0.34408626464646463, 'x2': 0.4580774235294118, 'y2': 0.35666530505050503}}, {'text': 'No\\n', 'bbox': {'x1': 0.519937908496732, 'y1': 0.34408626464646463, 'x2': 0.5398305509803921, 'y2': 0.35666530505050503}}, {'text': '2022\\n', 'bbox': {'x1': 0.3366633986928105, 'y1': 0.4068552040404041, 'x2': 0.36922091503267973, 'y2': 0.41943424444444444}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.395468954248366, 'y1': 0.39993727474747476, 'x2': 0.466444339869281, 'y2': 0.4263534363636363}}, {'text': '2021\\n', 'bbox': {'x1': 0.3366633986928105, 'y1': 0.44886909292929295, 'x2': 0.36922091503267973, 'y2': 0.4614481333333333}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.4038366013071895, 'y1': 0.44886909292929295, 'x2': 0.4580774235294118, 'y2': 0.4614481333333333}}, {'text': '2021\\n', 'bbox': {'x1': 0.3366633986928105, 'y1': 0.4978021737373738, 'x2': 0.36922091503267973, 'y2': 0.5103812141414141}}, {'text': 'Encoder\\n', 'bbox': {'x1': 0.4038366013071895, 'y1': 0.4978021737373738, 'x2': 0.4580774235294118, 'y2': 0.5103812141414141}}, {'text': '2022\\n', 'bbox': {'x1': 0.3366633986928105, 'y1': 0.5536531838383838, 'x2': 0.36922091503267973, 'y2': 0.5662322242424241}}, {'text': 'Encoder &\\nDecoder\\n', 'bbox': {'x1': 0.395468954248366, 'y1': 0.5467352545454546, 'x2': 0.466444339869281, 'y2': 0.5731514161616161}}, {'text': 'No\\n', 'bbox': {'x1': 0.519937908496732, 'y1': 0.4068552040404041, 'x2': 0.5398305509803921, 'y2': 0.41943424444444444}}, {'text': 'No\\n', 'bbox': {'x1': 0.519937908496732, 'y1': 0.44886909292929295, 'x2': 0.5398305509803921, 'y2': 0.4614481333333333}}, {'text': 'No\\n', 'bbox': {'x1': 0.519937908496732, 'y1': 0.4978021737373738, 'x2': 0.5398305509803921, 'y2': 0.5103812141414141}}, {'text': 'No\\n', 'bbox': {'x1': 0.519937908496732, 'y1': 0.5536531838383838, 'x2': 0.5398305509803921, 'y2': 0.5662322242424241}}, {'text': 'NA\\n', 'bbox': {'x1': 0.6451323529411765, 'y1': 0.34408626464646463, 'x2': 0.6680691232026144, 'y2': 0.35666530505050503}}, {'text': 'NA\\n', 'bbox': {'x1': 0.6451323529411765, 'y1': 0.4068552040404041, 'x2': 0.6680691232026144, 'y2': 0.41943424444444444}}, {'text': 'NA\\n', 'bbox': {'x1': 0.6451323529411765, 'y1': 0.44886909292929295, 'x2': 0.6680691232026144, 'y2': 0.4614481333333333}}, {'text': 'NA\\n', 'bbox': {'x1': 0.6451323529411765, 'y1': 0.4978021737373738, 'x2': 0.6680691232026144, 'y2': 0.5103812141414141}}, {'text': 'NA\\n', 'bbox': {'x1': 0.6451323529411765, 'y1': 0.5536531838383838, 'x2': 0.6680691232026144, 'y2': 0.5662322242424241}}, {'text': 'Peng Cheng labora-\\ntory(PCL), local area data\\n', 'bbox': {'x1': 0.7543627450980392, 'y1': 0.33716707272727264, 'x2': 0.9234339274509804, 'y2': 0.3635832343434343}}, {'text': 'Wall NLoS, Foil\\nNLOS, UWB dataset\\n', 'bbox': {'x1': 0.7699493464052287, 'y1': 0.39993727474747476, 'x2': 0.9078467068627453, 'y2': 0.4263534363636363}}, {'text': 'ST, BPSK, AM, NAM,\\nSFM, LFM, 4FSK,\\n2FSK signal dataset\\n', 'bbox': {'x1': 0.7635849673202615, 'y1': 0.43503323434343444, 'x2': 0.9142123166666667, 'y2': 0.4752852545454546}}, {'text': 'RadioML2016.10b\\n', 'bbox': {'x1': 0.7767205882352941, 'y1': 0.4978021737373738, 'x2': 0.9010740218954248, 'y2': 0.5103812141414141}}, {'text': 'CsiNet, CLNet and CRNet\\n', 'bbox': {'x1': 0.7516274509803921, 'y1': 0.5536531838383838, 'x2': 0.9261682960784313, 'y2': 0.5662322242424241}}]}\n",
      "{'type': 'Caption', 'bbox': [0.2654889454561121, 0.599866055575284, 0.7369884535845588, 0.6147663463245738], 'properties': {'score': 0.6877686381340027, 'page_number': 43}, 'text_representation': 'Table 26: Transformer models for wireless network & signal processing\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09138911527745865, 0.6356059681285512, 0.91148681640625, 0.7052690540660511], 'properties': {'score': 0.7913159728050232, 'page_number': 43}, 'text_representation': '• SigT: SigT is a wireless communication network signal receiver designed with a transformer architecture, capable of han-\\ndling Multiple-input Multiple-output (MIMO-OFDM) signals. Leveraging the transformer’s encoder module, this innova-\\ntive framework enables parallel data processing and performs essential tasks such as signal detection, channel estimation,\\nand data decoding. Unlike traditional receivers that rely on distinct modules for each task, SigT seamlessly integrates these\\nfunctions, providing a unified solution (Ren et al., 2022).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0910504778693704, 0.7111973987926137, 0.9112960276884191, 0.780755615234375], 'properties': {'score': 0.8465707898139954, 'page_number': 43}, 'text_representation': '• TSDN: TSDN is an abbreviation for Transformer-based Signal Denoising Network. It refers to a signal denoising model\\nbased on transformers that aims to estimate the Angle-of-Arrival (AoA) of signals transmitted by users within a wireless\\ncommunication network. This transformer-based model significantly enhances the accuracy of AoA estimation, especially\\nin challenging non-line-of-sight (NLoS) environments where conventional methods often fall short in delivering the desired\\nprecision (Liu et al., 2022b).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09083744722254136, 0.7867270729758523, 0.9104840446920955, 0.8700150368430398], 'properties': {'score': 0.8378040790557861, 'page_number': 43}, 'text_representation': '• ACNNT: The Augmented Convolution Neural Network with Transformer (ACNNT) is an architectural framework specif-\\nically designed for identifying interference within wireless networks. This model combines the power of Convolutional\\nNeural Networks (CNN) and transformer architectures. The multiple CNN layers in ACNNT extract localized features\\nfrom the input signal, while the transformer component captures global relationships between various elements of the input\\nsequence. By exploiting the strengths of both CNN and Transformer, this model has demonstrated superior accuracy in the\\nidentification of wireless interference compared to conventional approaches (Wang et al., 2021a).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09128471823299632, 0.876478271484375, 0.9106142290900735, 0.9455939275568181], 'properties': {'score': 0.8430774807929993, 'page_number': 43}, 'text_representation': '• MCformer: MCformer, short for Modulation Classification Transformer, refers to a model architecture based on transform-\\ners that performs feature extraction from input signals and subsequently classifies them based on modulation. This architec-\\ntural design combines Convolutional Neural Network (CNN) and self-attention layers, enabling the processing of intricate\\nfeatures within the signal and achieving superior accuracy in comparison to conventional approaches. The introduction\\nof this model has brought about noteworthy advancements in a wireless network and communication signals, particularly\\n'}\n",
      "{'type': 'Text', 'bbox': [0.10571694766773897, 0.10145197088068182, 0.9082164809283089, 0.12893128828568892], 'properties': {'score': 0.8794203996658325, 'page_number': 44}, 'text_representation': 'in the realm of Automatic Modulation Classification, thereby enhancing system security and performance (Hamidi-Rad &\\nJain, 2021).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09169385124655331, 0.16647705078125, 0.909913760914522, 0.24892750133167613], 'properties': {'score': 0.7010103464126587, 'page_number': 44}, 'text_representation': '• Quan-Transformer: It refers to a Transformer-based model specifically designed to perform quantization in wireless\\ncommunication systems. Quantization is the vital process of converting a continuous signal into a discrete signal, and it\\nplays a crucial role in network channel feedback processing. Channel feedback processing is essential for estimating channel\\nstate information, which in turn aids in adjusting signal transmission parameters. This feedback mechanism holds particular\\nsignificance in the context of Reconfigurable Intelligent Surface (RIS)-aided wireless networks, a critical component of the\\n6th-generation communication system (Xie et al., 2022).\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09388041776769301, 0.29472498113458806, 0.36394868738511027, 0.307930908203125], 'properties': {'score': 0.6265340447425842, 'page_number': 44}, 'text_representation': '6.6.2 MEDICAL SIGNAL PROCESSING\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09399262372185202, 0.32654083251953125, 0.9122688562729779, 0.4371261041814631], 'properties': {'score': 0.9223767518997192, 'page_number': 44}, 'text_representation': 'The rise of healthcare data has resulted in the rapid growth of deep learning applications, enabling the automatic detection of\\npathologies, enhanced medical diagnosis, and improved healthcare services. These data can be categorized into three distinct\\nforms: relational data ( symptoms, examinations, and laboratory tests), medical images, and biomedical signals (consisting of\\nraw electronic and sound signals). While the application of deep learning models, particularly transformers, in the context of\\nmedical images has gained considerable attention and yielded promising results, the application of transformers to biomedical\\nsignals is still in its early stages. The majority of relevant studies have been published between the years 2021 and 2022,\\nwith a particular focus on the task of signal classification. We have summarized our findings regarding the application of\\ntransformers to biomedical signals in Table 27.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.13860431446748622, 0.508425126509233, 0.3040986184512868, 0.521783613725142], 'properties': {'score': 0.49336951971054077, 'page_number': 44}, 'text_representation': '• Epilepsy disease case:\\n'}\n",
      "{'type': 'Text', 'bbox': [0.1523919677734375, 0.5406583473899148, 0.91165283203125, 0.609801191850142], 'properties': {'score': 0.9219042658805847, 'page_number': 44}, 'text_representation': 'Epilepsy is a serious debilitating condition for those it affects. Typically, its symptoms are detected through the use\\nof electrical signals, such as electroencephalograms (EEGs) and magnetoencephalograms (MEGs). With the rise of\\ndeep learning, it is now possible to detect and predict epilepsy cases using its architecture. Utilizing transformers as\\ntheir underlying framework, the following models have been developed to analyze electric signals for predicting and\\nclassifying epilepsy.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.16834932215073528, 0.6963146417791193, 0.9108802705652573, 0.7786622758345171], 'properties': {'score': 0.7554895281791687, 'page_number': 44}, 'text_representation': '– Three-tower transformer network (Yan et al., 2022a): The purpose of this model is to predict epileptic\\nseizures from EEG signals. A transformer-based model is used to perform binary classification of EEG signals\\nbased on three EEG features: time, frequency, and channel. This model processes EEG signals as a whole using\\na model that is based on the classic transformer, which contains three encoders: a time encoder, a frequency\\nencoder, and a channel encoder. Compared to other models, such as CNN, the model shows better performance\\nresults in predicting epilepsy attacks.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.16703713809742646, 0.8377195046164773, 0.9101978257123162, 0.9479701926491477], 'properties': {'score': 0.7024044394493103, 'page_number': 44}, 'text_representation': '– TransHFO (Guo et al., 2022b): (Transformer-based HFO) is a deep learning model based on BERT for clas-\\nsifying High-Frequency Oscillation (HFO) from normal control (NC). A transformer is used to detect the pres-\\nence of HFO with high accuracy in one-dimensional magnetoencephalography (MEG) data in order to identify\\nepileptic areas more precisely. Signal classification is performed under k-fold cross-validation and through\\nvarious algorithms, including logistic regression, SMO, and the ResDen model. Due to the small dataset avail-\\nable, the authors propose to use the data augmentation technique “ADASYN”. Nevertheless, even with the\\naddition of data, the dataset remains small, which makes the shallow transformer more efficient than the deep\\ntransformer with more layers.\\n'}\n",
      "{'type': 'table', 'bbox': [0.0508647559670841, 0.09746360778808594, 0.9491458668428309, 0.4088572831587358], 'properties': {'score': 0.6816476583480835, 'title': None, 'columns': None, 'rows': None, 'page_number': 45}, 'text_representation': 'Transformer Name\\n Field of\\napplication\\n Year\\n Fully Trans-\\nformer\\nArchitecture\\n Signal\\ntype\\n Transformer\\nTask\\n Dataset\\n Three-tower trans-\\nformer network\\n(Yan et al., 2022a)\\n TransHFO (Guo\\net al., 2022b)\\n TCN and Transformer-\\nbased model\\n(Casal et al., 2022)\\n Constrained trans-\\nformer network\\n(Che et al., 2021)\\n CRT-NET (Liu\\net al., 2022a)\\n Epilepsy\\n 2022\\n Epilepsy\\n 2022\\n YES\\n YES\\n Sleep\\npathologies\\n 2022\\n No (using\\nTCN which is\\nbased on CNN)\\n EEG\\n MEG\\n Cardiac\\nsignals\\n( Heart\\nRate)\\n Heart disease\\n 2021\\n No (Using CNN)\\n ECG\\n Heart Disease\\n 2022\\n No (Using\\nCNN and Bi-\\ndirectional GRU)\\n ECG\\n CAT (Yang\\net al., 2022)\\n Atrial\\nFibrillation\\n 2022\\n No (Using MLP)\\n ECG\\n Classification\\nof EEG signals\\n Classification\\nof MEG\\nsignals\\n CHB-MIT datasets\\n 20 clinical patients\\n Classification\\nof sleep stages\\n Sleep Heart Health\\nStudy dataset\\n Classification\\nof ECG signals\\n Classification\\nand recognition\\nof ECG signals\\n Classification\\nof ECG signals\\n 6877 patients\\n MIT-BIH, CPSC\\narrhythmia clin-\\nical private data\\nShaoxing database\\n(more than\\n10000 patients)\\n', 'table': <sycamore.data.table.Table object at 0x7fb9e0176500>, 'tokens': [{'text': 'Transformer Name\\n', 'bbox': {'x1': 0.07524451431372549, 'y1': 0.11366532998678792, 'x2': 0.20717355178760788, 'y2': 0.1260916056231516}}, {'text': 'Field of\\napplication\\n', 'bbox': {'x1': 0.2520368439705883, 'y1': 0.10676230547921212, 'x2': 0.32888824207976475, 'y2': 0.13299463013072726}}, {'text': 'Year\\n', 'bbox': {'x1': 0.3728026396568627, 'y1': 0.11366532998678792, 'x2': 0.4049486837707451, 'y2': 0.1260916056231516}}, {'text': 'Fully Trans-\\nformer\\nArchitecture\\n', 'bbox': {'x1': 0.456533065245098, 'y1': 0.09985928097163645, 'x2': 0.543467289597098, 'y2': 0.13989904050951518}}, {'text': 'Signal\\ntype\\n', 'bbox': {'x1': 0.5896720821568627, 'y1': 0.10676230547921212, 'x2': 0.6325763571422744, 'y2': 0.13299463013072726}}, {'text': 'Transformer\\nTask\\n', 'bbox': {'x1': 0.6656779876960783, 'y1': 0.10676230547921212, 'x2': 0.753400184114902, 'y2': 0.13299463013072726}}, {'text': 'Dataset\\n', 'bbox': {'x1': 0.8328931320588233, 'y1': 0.11366532998678792, 'x2': 0.8846902346555292, 'y2': 0.1260916056231516}}, {'text': 'Three-tower trans-\\nformer network\\n(Yan et al., 2022a)\\n', 'bbox': {'x1': 0.08135490200980393, 'y1': 0.14191876043139387, 'x2': 0.20106233191662745, 'y2': 0.18195851996927262}}, {'text': 'TransHFO (Guo\\net al., 2022b)\\n', 'bbox': {'x1': 0.08856828014705882, 'y1': 0.19079566634048484, 'x2': 0.19385099691223526, 'y2': 0.21702799099199999}}, {'text': 'TCN and Transformer-\\nbased model\\n(Casal et al., 2022)\\n', 'bbox': {'x1': 0.06707521112745098, 'y1': 0.23276816187078803, 'x2': 0.21534260819098042, 'y2': 0.2728079214086668}}, {'text': 'Constrained trans-\\nformer network\\n(Che et al., 2021)\\n', 'bbox': {'x1': 0.08203104411764707, 'y1': 0.2816450677798787, 'x2': 0.20038766476705885, 'y2': 0.32168482731775744}}, {'text': 'CRT-NET (Liu\\net al., 2022a)\\n', 'bbox': {'x1': 0.09262871975490196, 'y1': 0.33052197368896963, 'x2': 0.1897904999130196, 'y2': 0.35675429834048483}}, {'text': 'Epilepsy\\n', 'bbox': {'x1': 0.2623206605392157, 'y1': 0.1557261953177576, 'x2': 0.31860437959803917, 'y2': 0.16815247095412128}}, {'text': '2022\\n', 'bbox': {'x1': 0.3727954657352941, 'y1': 0.1557261953177576, 'x2': 0.40495759091176475, 'y2': 0.16815247095412128}}, {'text': 'Epilepsy\\n', 'bbox': {'x1': 0.2623206605392157, 'y1': 0.19769869084806063, 'x2': 0.31860437959803917, 'y2': 0.21012496648442433}}, {'text': '2022\\n', 'bbox': {'x1': 0.3727954657352941, 'y1': 0.19769869084806063, 'x2': 0.40495759091176475, 'y2': 0.21012496648442433}}, {'text': 'YES\\n', 'bbox': {'x1': 0.4848108705882352, 'y1': 0.1557261953177576, 'x2': 0.5151879978174116, 'y2': 0.16815247095412128}}, {'text': 'YES\\n', 'bbox': {'x1': 0.4848108705882352, 'y1': 0.19769869084806063, 'x2': 0.5151879978174116, 'y2': 0.21012496648442433}}, {'text': 'Sleep\\npathologies\\n', 'bbox': {'x1': 0.2533873347058823, 'y1': 0.23967118637836368, 'x2': 0.3275371143002353, 'y2': 0.2659048969010909}}, {'text': '2022\\n', 'bbox': {'x1': 0.3727954657352941, 'y1': 0.2465755967571516, 'x2': 0.40495759091176475, 'y2': 0.2590018723935153}}, {'text': 'No (using\\nTCN which is\\nbased on CNN)\\n', 'bbox': {'x1': 0.44997968789215675, 'y1': 0.23276816187078803, 'x2': 0.5500199782535685, 'y2': 0.2728079214086668}}, {'text': 'EEG\\n', 'bbox': {'x1': 0.5954937195098038, 'y1': 0.1557261953177576, 'x2': 0.6267553051813333, 'y2': 0.16815247095412128}}, {'text': 'MEG\\n', 'bbox': {'x1': 0.5932572494607842, 'y1': 0.19769869084806063, 'x2': 0.628989370531843, 'y2': 0.21012496648442433}}, {'text': 'Cardiac\\nsignals\\n( Heart\\nRate)\\n', 'bbox': {'x1': 0.5861174040196078, 'y1': 0.22586513736321207, 'x2': 0.6361295086690196, 'y2': 0.2797123317874544}}, {'text': 'Heart disease\\n', 'bbox': {'x1': 0.2473737949509804, 'y1': 0.29545111679503017, 'x2': 0.33355220936133334, 'y2': 0.30787739243139384}}, {'text': '2021\\n', 'bbox': {'x1': 0.3727954657352941, 'y1': 0.29545111679503017, 'x2': 0.40495759091176475, 'y2': 0.30787739243139384}}, {'text': 'No (Using CNN)\\n', 'bbox': {'x1': 0.44461718151960783, 'y1': 0.29545111679503017, 'x2': 0.5553835406273725, 'y2': 0.30787739243139384}}, {'text': 'ECG\\n', 'bbox': {'x1': 0.5950435559313726, 'y1': 0.29545111679503017, 'x2': 0.6272056811078431, 'y2': 0.30787739243139384}}, {'text': 'Heart Disease\\n', 'bbox': {'x1': 0.2455874884803922, 'y1': 0.3374249981965455, 'x2': 0.3353358987853333, 'y2': 0.34985127383290915}}, {'text': '2022\\n', 'bbox': {'x1': 0.3727954657352941, 'y1': 0.3374249981965455, 'x2': 0.40495759091176475, 'y2': 0.34985127383290915}}, {'text': 'No (Using\\nCNN and Bi-\\ndirectional GRU)\\n', 'bbox': {'x1': 0.4442710398039215, 'y1': 0.3236175633101819, 'x2': 0.5557288846029803, 'y2': 0.36365870871927275}}, {'text': 'ECG\\n', 'bbox': {'x1': 0.5950435559313726, 'y1': 0.3374249981965455, 'x2': 0.6272056811078431, 'y2': 0.34985127383290915}}, {'text': 'CAT (Yang\\net al., 2022)\\n', 'bbox': {'x1': 0.10279954705882352, 'y1': 0.3724944692192727, 'x2': 0.17961878304282353, 'y2': 0.3987281797420001}}, {'text': 'Atrial\\nFibrillation\\n', 'bbox': {'x1': 0.2542715205392157, 'y1': 0.3724944692192727, 'x2': 0.3266523832488628, 'y2': 0.3987281797420001}}, {'text': '2022\\n', 'bbox': {'x1': 0.3727954657352941, 'y1': 0.3793974937268485, 'x2': 0.40495759091176475, 'y2': 0.39182376936321217}}, {'text': 'No (Using MLP)\\n', 'bbox': {'x1': 0.44506017117647056, 'y1': 0.3793974937268485, 'x2': 0.5549420718418824, 'y2': 0.39182376936321217}}, {'text': 'ECG\\n', 'bbox': {'x1': 0.5950435559313726, 'y1': 0.3793974937268485, 'x2': 0.6272056811078431, 'y2': 0.39182376936321217}}, {'text': 'Classification\\nof EEG signals\\n', 'bbox': {'x1': 0.6608535254411764, 'y1': 0.1488217849389697, 'x2': 0.7582243594129411, 'y2': 0.17505549546169694}}, {'text': 'Classification\\nof MEG\\nsignals\\n', 'bbox': {'x1': 0.6657586943137254, 'y1': 0.18389125596169706, 'x2': 0.7533200801066666, 'y2': 0.2239324013707879}}, {'text': 'CHB-MIT datasets\\n', 'bbox': {'x1': 0.7973786333333334, 'y1': 0.1557261953177576, 'x2': 0.9202057893822746, 'y2': 0.16815247095412128}}, {'text': '20 clinical patients\\n', 'bbox': {'x1': 0.7980458080392155, 'y1': 0.19769869084806063, 'x2': 0.9195382358933333, 'y2': 0.21012496648442433}}, {'text': 'Classification\\nof sleep stages\\n', 'bbox': {'x1': 0.6626470058333332, 'y1': 0.23967118637836368, 'x2': 0.7564317628479215, 'y2': 0.2659048969010909}}, {'text': 'Sleep Heart Health\\nStudy dataset\\n', 'bbox': {'x1': 0.7976117857843137, 'y1': 0.23967118637836368, 'x2': 0.919972591018196, 'y2': 0.2659048969010909}}, {'text': 'Classification\\nof ECG signals\\n', 'bbox': {'x1': 0.6604033618627451, 'y1': 0.2885480922874545, 'x2': 0.758674735339451, 'y2': 0.31478180281018175}}, {'text': 'Classification\\nand recognition\\nof ECG signals\\n', 'bbox': {'x1': 0.6592931974999999, 'y1': 0.3236175633101819, 'x2': 0.7597837576138823, 'y2': 0.36365870871927275}}, {'text': 'Classification\\nof ECG signals\\n', 'bbox': {'x1': 0.6604033618627451, 'y1': 0.3724944692192727, 'x2': 0.758674735339451, 'y2': 0.3987281797420001}}, {'text': '6877 patients\\n', 'bbox': {'x1': 0.8156864811764705, 'y1': 0.29545111679503017, 'x2': 0.901897057712, 'y2': 0.30787739243139384}}, {'text': 'MIT-BIH, CPSC\\narrhythmia clin-\\nical private data\\nShaoxing database\\n(more than\\n10000 patients)\\n', 'bbox': {'x1': 0.7987219501470587, 'y1': 0.3236175633101819, 'x2': 0.9188635687437645, 'y2': 0.40563120424957577}}]}\n",
      "{'type': 'Caption', 'bbox': [0.30279993393841914, 0.41880251797762785, 0.698958309397978, 0.4330811934037642], 'properties': {'score': 0.8326171040534973, 'page_number': 45}, 'text_representation': 'Table 27: Transformer models for medical signal processing\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.1388557523839614, 0.4611964277787642, 0.9110933191636029, 0.5030262340198863], 'properties': {'score': 0.7473951578140259, 'page_number': 45}, 'text_representation': '• Cardiac diseases cases: heart diseases are among the areas in which researchers are interested in applying trans-\\nformers. Using ECG signals, a transformer model can detect long-range dependencies and identify heart disease\\ntypes based on their characteristics.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.16736317354090074, 0.513186922940341, 0.9123488482306985, 0.582518477006392], 'properties': {'score': 0.7264218330383301, 'page_number': 45}, 'text_representation': '– Constrained transformer network (Che et al., 2021): Using CNN and transformer architecture, this model\\nclassifies heart arrhythmia disease based on temporal information in ECG (electrocardiogram) signals. There\\nare also other models that use transformer encoders for classifying heart diseases (such as atrial fibrillation)\\nusing ECG signals, such as CRT-NET (Liu et al., 2022a) and CAT (Yang et al., 2022) “Component-Aware\\nTransformer”.\\nA major advantage of CAT’s model (Yang et al., 2022) is the use of a large database containing data from over\\nten thousand patients for experiments. In contrast, the strength of CRT-NET (Liu et al., 2022a) is the ability to\\nextract different ECG features like waveforms, morphological characteristics, and time domain data, in order to\\nidentify many cardiovascular diseases.\\n'}\n",
      "{'type': 'Text', 'bbox': [0.18510252111098346, 0.5834307306463068, 0.9123888442095588, 0.6383500532670454], 'properties': {'score': 0.6104657053947449, 'page_number': 45}}\n",
      "{'type': 'List-item', 'bbox': [0.16732737821691177, 0.6466386829723011, 0.9118047736672794, 0.7293570778586648], 'properties': {'score': 0.7568596005439758, 'page_number': 45}, 'text_representation': '– TCN and Transformer-based model (Casal et al., 2022): An automatic sleep stage classification system\\nbased on 1-dimensional cardiac signals (Heart Rate). The classification is conducted in two steps: extracting\\nfeatures from signals using TCN ”Temporal Convolution Network”[], and modeling signal sequence dependen-\\ncies using the standard Transformer architecture consisting of two stacks of encoders and a simplified decoder\\nmodule. Based on a dataset of 5000 different participants, this study demonstrated that this new model outper-\\nforms other networks, such as CNN and RNN, which consume more memory and reduce process efficiency.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.0955808931238511, 0.7427165083451704, 0.45630597282858454, 0.7585476962002841], 'properties': {'score': 0.8767908811569214, 'page_number': 45}, 'text_representation': '7 FUTURE PROSPECTS AND CHALLENGES\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09387880213120404, 0.7664672296697443, 0.9122765395220588, 0.89128662109375], 'properties': {'score': 0.9404473304748535, 'page_number': 45}, 'text_representation': 'One of the primary objectives of this survey is to identify and highlight potential research directions for transformer applica-\\ntions, with the goal of expanding their range of applications beyond the currently popular fields of NLP and computer vision.\\nDespite considerable research attention in these areas, there are still a number of areas that remain relatively unexplored with\\nthe potential for significant improvements in the future. In order to expand the application areas of Transformers, we have\\nidentified several potential directions for future research. These directions include but are not limited to the exploration of\\ntransformer-based models for speech recognition, recommendation systems, and natural language generation. In addition,\\nfurther exploration of transformer-based approaches for multimodal tasks, such as combining audio and visual inputs, would\\nbe an interesting direction for future research. By pursuing these research directions, we hope to continue the advancement\\nof transformer-based models and their utility in a broader range of applications.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.0946390937356388, 0.9014818226207386, 0.6032035558363971, 0.9155595814098011], 'properties': {'score': 0.6990808844566345, 'page_number': 45}, 'text_representation': '7.1 TRANSFORMERS IN WIRELESS NETWORK AND CLOUD COMPUTING\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09414365880629595, 0.9203579434481534, 0.9107943905101102, 0.9482539506392046], 'properties': {'score': 0.9136824607849121, 'page_number': 45}, 'text_representation': 'While the majority of transformer applications have been in NLP and Computer Vision, there is an exciting potential for\\ntransformers in the wireless communication and cloud computing domains. Although there have been relatively fewer studies\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09390613331514246, 0.1010163671320135, 0.9125178797104779, 0.22628068403764204], 'properties': {'score': 0.9323601126670837, 'page_number': 46}, 'text_representation': 'in this area, the ones that exist have demonstrated the enormous potential of transformers for improving various aspects of\\nwireless signal communication and cloud workload computing. In this section, we discuss some of the transformer models\\nthat have been developed for wireless signal communication and cloud computing. These models have shown promising\\nresults in areas such as wireless interference recognition, wireless signal communication mitigation, and cloud workload\\nforecasting. Moving forward, there are several potential directions for future research in both the wireless network and cloud\\ndomains. Some of the possible areas of focus for wireless communication include improving network security, enhancing the\\nefficiency of wireless communication, and developing more accurate interference recognition models. On the other hand, for\\ncloud computing, future work could focus on improving resource allocation and workload management, optimizing cloud\\nperformance, and enhancing data privacy and security.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.0939663516773897, 0.23990434126420454, 0.5143126723345588, 0.2538565063476563], 'properties': {'score': 0.814014732837677, 'page_number': 46}, 'text_representation': 'Scope of future work for the wireless Signal communication:\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.13707902796128216, 0.26100638649680397, 0.911834716796875, 0.34374053955078127], 'properties': {'score': 0.9080824851989746, 'page_number': 46}, 'text_representation': '• Detection of Wireless Interference: The global attention capabilities of Transformers present an exciting avenue for\\nfuture research in the field of wireless signal communication. By leveraging the power of Transformers, researchers\\ncan explore and develop more widely used applications for detecting wireless interference in communication sys-\\ntems. This can involve experimenting with various successive Transformer models to reduce complexity and enhance\\nthe efficiency of wireless interference recognition. This research can lead to improved communication systems that\\nare more resilient to interference and provide better overall performance.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.13803150850183823, 0.3511126708984375, 0.9118439079733456, 0.4615625832297585], 'properties': {'score': 0.9119901061058044, 'page_number': 46}, 'text_representation': '• Enhancing 5G & 6G Networks: As 5G and 6G networks gain popularity, there is significant potential for Transform-\\ners to contribute to this field. Advanced networking architectures, such as Reconfigurable Intelligent Surfaces (RIS)\\nand Multiple-Output and Orthogonal Frequency-Division Multiplexing (MIMO-OFDM), play a crucial role in these\\nnetworks. Transformers have shown promise in improving performance in these areas. Additionally, signal state\\nfeedback, which is essential for adjusting and updating networks based on signal state changes, can benefit from\\nthe parallel computational capability of Transformers. The ability of Transformers to handle multiple tasks simul-\\ntaneously, including signal detection, channel estimation, and data decoding, makes them an effective alternative to\\nconventional methods.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.13810968735638787, 0.4693524169921875, 0.912095157398897, 0.5522817715731534], 'properties': {'score': 0.9008234739303589, 'page_number': 46}, 'text_representation': '• Integration of Transformers with Advanced Communication Technologies: Transformers can be integrated with\\nother advanced communication technologies to further improve wireless signal communication. For example,\\ncombining Transformers with technologies like Massive MIMO, millimeter-wave communication, and cognitive\\nradio can enhance the performance, capacity, and spectrum efficiency of wireless networks. Future research can\\nfocus on exploring these synergies and developing innovative solutions that leverage the unique capabilities of\\nTransformers in conjunction with other cutting-edge communication technologies.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09346701229319852, 0.5725687477805398, 0.3293410716337316, 0.586668867631392], 'properties': {'score': 0.8185781240463257, 'page_number': 46}, 'text_representation': 'Future possibilities for the Cloud:\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.13721361945657168, 0.5937260298295455, 0.9115613511029412, 0.6909603049538352], 'properties': {'score': 0.9172912240028381, 'page_number': 46}, 'text_representation': '• Advancements in Cloud Computing: With the increasing application of the Internet of Things (IoT), the cloud plays\\na crucial role in supporting and managing IoT devices. Transformers offer exciting possibilities for advancing cloud\\ncapabilities in various tasks, such as early attack and anomaly detection. By leveraging different Transformer ap-\\nproaches, the cloud can learn and adapt to its behavior, bringing more stability and security. Additionally, Transform-\\ners can be applied to cloud computing tasks like task scheduling and memory allocation. The multi-head attention\\nand long-range attention features of the Transformers model make it well-suited for optimizing resource allocation\\nand improving overall performance in cloud environments.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.1381741422765395, 0.6976896528764205, 0.9121485093060662, 0.7949135520241477], 'properties': {'score': 0.9180614948272705, 'page_number': 46}, 'text_representation': '• Transformation in Mobile Edge Computing (MEC) and Mobile Edge Caching (MEC): In the context of advanced\\n6G networking systems, Mobile Edge Computing (MEC) and Mobile Edge Caching (MEC) play vital roles in\\nreducing communication latency. Transformers have demonstrated significant potential in enhancing MEC and MEC\\nthrough their parallel computational capabilities. Transformers can be applied to predict popular content, improve\\ncontent management, optimize resource allocation, and enhance data transmission in MEC systems. By leveraging\\nTransformers, the mobile cloud can respond and process user requests faster, resulting in reduced network response\\ntimes and faster data transmission.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.13768222584443934, 0.8024943958629261, 0.9109296731387868, 0.8713419411399148], 'properties': {'score': 0.9201791286468506, 'page_number': 46}, 'text_representation': '• Intelligent Resource Management in the Cloud: Transformers offer opportunities for intelligent resource manage-\\nment in cloud environments. By applying Transformers to tasks like workload prediction, resource allocation, and\\nload balancing, cloud systems can optimize resource utilization and enhance performance. Transformers’ ability\\nto capture long-range dependencies and handle complex patterns makes them well-suited for efficiently managing\\ncloud resources and improving overall system efficiency.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.13824689079733457, 0.8792070978338068, 0.9109364947150735, 0.9480256791548295], 'properties': {'score': 0.9139323830604553, 'page_number': 46}, 'text_representation': '• Security and Privacy in the Cloud: Transformers can contribute to enhancing security and privacy in the cloud by\\nenabling advanced threat detection, anomaly detection, and data privacy protection mechanisms. Transformers can\\nanalyze large volumes of data, identify patterns, and detect potential security breaches or anomalies in real-time.\\nAdditionally, Transformers can be utilized for data anonymization and privacy-preserving computations, ensuring\\nthat sensitive information remains protected in cloud-based systems.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09477613561293657, 0.10060252796519886, 0.4237047262752757, 0.11462665211070668], 'properties': {'score': 0.5443385243415833, 'page_number': 47}, 'text_representation': '7.2 MEDICAL IMAGE & SIGNAL PROCESSING\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09424286786247703, 0.12014907836914063, 0.9118510885799632, 0.20280432961203834], 'properties': {'score': 0.6088575720787048, 'page_number': 47}, 'text_representation': 'Two types of medical data are discussed in this paper: images and signals. According to our literature review, segmentation,\\nand classification are the most transformer-based medical applications, followed by image translation (Yan et al., 2022b, Chen\\net al., 2022c). In the context of medical images, we commonly see the reuse of existing transformers, such as BERT, ViT, and\\nSWIN, regardless of how the original model was modified. Further, we observe that various types of medical images are used\\nto conduct transformer-based medical applications, such as 2D images (He et al., 2022, Gu et al., 2022), 3D images (Jiang\\net al., 2022b, Liang et al., 2022, Zhou et al., 2021a, Zhu et al., 2022) and multi-mode images (Sun et al., 2021b).\\nThe selected research papers for this survey span the years 2021 to 2022, indicating that the use of transformer architecture\\nin the medical field is still in its nascent stages. Despite its early adoption, there has been a remarkable influx of excellent\\npublications exploring transformer applications in the analysis of medical images within this relatively short period. However,\\nseveral challenges persist in applying transformers to medical images that need to be addressed and overcome:\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09388968075022978, 0.20431027499112217, 0.9111270680147059, 0.25933821244673294], 'properties': {'score': 0.5554660558700562, 'page_number': 47}}\n",
      "{'type': 'List-item', 'bbox': [0.13847759471220128, 0.2646070723100142, 0.9107760081571691, 0.3060893388227983], 'properties': {'score': 0.8982147574424744, 'page_number': 47}, 'text_representation': '• Limited focus on 3D images: There is a scarcity of studies that specifically address the application of transformers\\nto 3D medical images. Most research has been concentrated on 2D images, indicating the need for further exploration\\nand development in this area.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.13877460255342372, 0.3105525623668324, 0.9113238166360295, 0.3521517112038352], 'properties': {'score': 0.9147699475288391, 'page_number': 47}, 'text_representation': '• Small and private medical image databases: Medical image databases are often small and privately owned due to\\nlegal and ethical concerns regarding patient data privacy (L´opez-Linares et al., 2020). This limits the availability of\\nlarge-scale datasets necessary for training transformer models effectively.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.1385742456772748, 0.3564921153675426, 0.9108953498391544, 0.4119426103071733], 'properties': {'score': 0.9201440215110779, 'page_number': 47}}\n",
      "{'type': 'List-item', 'bbox': [0.13802427404067097, 0.4165280151367188, 0.9103677906709559, 0.4855199085582386], 'properties': {'score': 0.906766951084137, 'page_number': 47}, 'text_representation': '• Computational complexity in high-resolution imaging: Transformer-based architectures encounter computational\\nchallenges when dealing with high-resolution medical images. The self-attention mechanism, which is integral to\\ntransformers, becomes computationally demanding for large images. However, some models, like DI-UNET (Wu\\net al., 2022), have introduced enhanced self-attention mechanisms to handle higher resolution images effectively.\\n• Limited number of fully developed transformer-based models: The development of transformer-based models\\nfor processing medical images is still relatively nascent. Due to the computational complexity and parameter re-\\nquirements of transformers, existing architectures often combine deep learning techniques like CNNs and GANs\\nwith transformers (Ma et al., 2022). Knowledge distillation techniques may offer a viable solution for training\\ntransformer models with limited computational and storage resources (Leng et al., 2022).\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09427228142233456, 0.49111372514204543, 0.9105471622242647, 0.5189230624112215], 'properties': {'score': 0.8559238314628601, 'page_number': 47}, 'text_representation': 'Moreover, the application of transformers to bio-signals is relatively limited compared to medical images. There are two main\\nchallenges that transformers face in the domain of biomedical signals:\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.13763651230755974, 0.5242320667613637, 0.9110299144071691, 0.5933710826526989], 'properties': {'score': 0.9215680360794067, 'page_number': 47}, 'text_representation': '• Small bio-signal databases: Bio-signal databases often have limited sizes, which poses challenges for training and\\nvalidating transformer models effectively. For instance, in a study mentioned by (Guo et al., 2022b), only 20 patients\\nwere included, which is considered insufficient to establish the effectiveness of a model. To mitigate the limitations\\nof small databases, some studies have proposed the use of virtual sample generation techniques like ADASYN (He\\net al., 2008) to augment the dataset.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.13832742130055148, 0.5978215997869318, 0.9114290843290441, 0.6534421608664772], 'properties': {'score': 0.9118350744247437, 'page_number': 47}, 'text_representation': '• Limited availability of transformer-based models: Currently, there is a scarcity of models that are exclusively based\\non transformers for processing biomedical signals. The application of transformers in this context is still relatively\\nunexplored, and more research is needed to develop dedicated transformer architectures for bio-signal analysis and\\nprocessing.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09477528291590073, 0.664259199662642, 0.33456004423253677, 0.6774283669211648], 'properties': {'score': 0.7179331183433533, 'page_number': 47}, 'text_representation': '7.3 REINFORCEMENT LEARNING\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09409054924460018, 0.6829357355291193, 0.9113838465073529, 0.7241699773615057], 'properties': {'score': 0.8608297109603882, 'page_number': 47}}\n",
      "{'type': 'Text', 'bbox': [0.09424927655388327, 0.7256150679154829, 0.9116913918887868, 0.7940571732954546], 'properties': {'score': 0.8257109522819519, 'page_number': 47}}\n",
      "{'type': 'Text', 'bbox': [0.09409273035386029, 0.7952490789240056, 0.9118385225183824, 0.9196368408203125], 'properties': {'score': 0.8891053795814514, 'page_number': 47}}\n",
      "{'type': 'Text', 'bbox': [0.09419625674977022, 0.9210232821377841, 0.9106030991498162, 0.9480595259232955], 'properties': {'score': 0.8003488779067993, 'page_number': 47}}\n",
      "{'type': 'Text', 'bbox': [0.09412350822897518, 0.10130423112349077, 0.9097194536994485, 0.15592559814453125], 'properties': {'score': 0.936241090297699, 'page_number': 48}, 'text_representation': 'et al., 2021). While these methods show promise in RL tasks, there is still significant room for improvement. Treating RL as\\nsequence modeling simplifies certain limitations of traditional RL algorithms but may also overlook their advantages. There-\\nfore, an interesting direction for further exploration is the integration of traditional RL algorithms with sequence modeling\\nusing transformers, combining the strengths of both approaches.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09437960456399357, 0.16700389515269887, 0.2674190925149357, 0.1796222617409446], 'properties': {'score': 0.6946151852607727, 'page_number': 48}, 'text_representation': '7.4 OTHER PROSPECTS\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09382730820599725, 0.18637293035333807, 0.9097561465992647, 0.25439319957386364], 'properties': {'score': 0.6538256406784058, 'page_number': 48}}\n",
      "{'type': 'Text', 'bbox': [0.09395786958582261, 0.25642622514204544, 0.9090174775965073, 0.33830827192826707], 'properties': {'score': 0.6360116600990295, 'page_number': 48}}\n",
      "{'type': 'Text', 'bbox': [0.09396544512580422, 0.3403586925159801, 0.90971435546875, 0.46409998113458806], 'properties': {'score': 0.7389901280403137, 'page_number': 48}}\n",
      "{'type': 'Section-header', 'bbox': [0.09476994233972887, 0.47940285422585227, 0.23757426542394303, 0.4932213800603693], 'properties': {'score': 0.8102984428405762, 'page_number': 48}, 'text_representation': '8 CONCLUSION\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09450310202205882, 0.5035823197798296, 0.9106980267693014, 0.6677620072798296], 'properties': {'score': 0.9428554773330688, 'page_number': 48}, 'text_representation': 'The transformer, as a deep neural network, has demonstrated superior performance compared to traditional recurrence-based\\nmodels in processing sequential data. Its ability to capture long-term dependencies and leverage parallel computation has\\nmade it a dominant force in various fields such as NLP, computer vision, and more. In this survey, we conducted a compre-\\nhensive overview of transformer models’ applications in different deep learning tasks and proposed a new taxonomy based on\\nthe top five fields and respective tasks: NLP, Computer Vision, Multi-Modality, Audio & Speech, and Signal Processing. By\\nexamining the advancements in each field, we provided insights into the current research focus and progress of transformer\\nmodels. This survey serves as a valuable reference for researchers seeking a deeper understanding of transformer applications\\nand aims to inspire further exploration of transformers across various tasks. Additionally, we plan to extend our investigation\\nto emerging fields like wireless networks, cloud computing, reinforcement learning, and others, to uncover new possibilities\\nfor transformer utilization. The rapid expansion of transformer applications in diverse domains showcases its versatility and\\npotential for continued growth. With ongoing advancements and novel use cases, transformers are poised to shape the future\\nof deep learning and contribute to advancements in fields beyond the traditional realms of NLP and computer vision.\\n'}\n",
      "{'type': 'Section-header', 'bbox': [0.09430110258214613, 0.6827141779119318, 0.2047742955824908, 0.6959001575816761], 'properties': {'score': 0.8173967599868774, 'page_number': 48}}\n",
      "{'type': 'List-item', 'bbox': [0.09371379179113051, 0.7041577703302557, 0.9079457720588235, 0.7281537420099432], 'properties': {'score': 0.6289359331130981, 'page_number': 48}, 'text_representation': 'approaches. Artif. Intell. Rev., 54, 5789–5829.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09295841890222886, 0.7379924427379262, 0.9055269847196691, 0.7613383900035512], 'properties': {'score': 0.6389610171318054, 'page_number': 48}}\n",
      "{'type': 'List-item', 'bbox': [0.09251871445599724, 0.7719286554509943, 0.9069670553768382, 0.8211995627663352], 'properties': {'score': 0.713394045829773, 'page_number': 48}, 'text_representation': 'Akbari, H., Yuan, L., Qian, R., Chuang, W., Chang, S., Cui, Y., & Gong, B. (2021). VATT: transformers for multimodal self-supervised\\nlearning from raw video, audio and text. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, & J. W. Vaughan (Eds.), Advances in\\nNeural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems, NeurIPS, December 6-14,\\n2021, virtual (pp. 24206–24221).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09343173756318934, 0.8309245161576705, 0.9077670467601103, 0.8543813254616477], 'properties': {'score': 0.688676655292511, 'page_number': 48}}\n",
      "{'type': 'List-item', 'bbox': [0.09312382417566636, 0.8647654030539773, 0.9077488080193015, 0.8885201194069602], 'properties': {'score': 0.7127795815467834, 'page_number': 48}, 'text_representation': 'International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021 (pp. 6816–6826). IEEE.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09281131519990808, 0.8982749245383522, 0.9073846076516544, 0.94728271484375], 'properties': {'score': 0.6524151563644409, 'page_number': 48}, 'text_representation': 'Babu, A., Wang, C., Tjandra, A., Lakhotia, K., Xu, Q., Goyal, N., Singh, K., von Platen, P., Saraf, Y., Pino, J., Baevski, A., Conneau, A.,\\n& Auli, M. (2022). XLS-R: self-supervised cross-lingual speech representation learning at scale. In H. Ko, & J. H. L. Hansen (Eds.),\\nInterspeech, 23rd Annual Conference of the International Speech Communication Association, Incheon, Korea, 18-22 September (pp.\\n2278–2282). ISCA.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09342497881721047, 0.10332246260209517, 0.9064420094209559, 0.12683319091796874], 'properties': {'score': 0.7330576777458191, 'page_number': 49}}\n",
      "{'type': 'List-item', 'bbox': [0.09350895601160386, 0.1379625077681108, 0.9068980497472426, 0.17359154441139915], 'properties': {'score': 0.8145501613616943, 'page_number': 49}, 'text_representation': 'Baevski, A., Zhou, Y., Mohamed, A., & Auli, M. (2020b). wav2vec 2.0: A framework for self-supervised learning of speech representations.\\nIn H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, & H. Lin (Eds.), Advances in Neural Information Processing Systems 33: Annual\\nConference on Neural Information Processing Systems, NeurIPS, December 6-12, virtual.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09366893992704503, 0.18537918090820313, 0.9067453182444853, 0.20819955999200995], 'properties': {'score': 0.7963085174560547, 'page_number': 49}}\n",
      "{'type': 'List-item', 'bbox': [0.0934886079676011, 0.21970612959428268, 0.9062765682444853, 0.2427559315074574], 'properties': {'score': 0.7352685928344727, 'page_number': 49}}\n",
      "{'type': 'List-item', 'bbox': [0.09373560288373162, 0.25450383966619317, 0.9065920122931985, 0.27772666237571025], 'properties': {'score': 0.7839717268943787, 'page_number': 49}}\n",
      "{'type': 'List-item', 'bbox': [0.09330144545611213, 0.28874594948508525, 0.9062055520450367, 0.3497670953924006], 'properties': {'score': 0.8832001090049744, 'page_number': 49}, 'text_representation': 'Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal,\\nS., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler,\\nE., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., & Amodei, D. (2020). Language\\nmodels are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, & H. Lin (Eds.), Advances in Neural Information\\nProcessing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS, December 6-12, virtual.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09355720968807445, 0.3609134188565341, 0.9067330394071691, 0.39689425381747157], 'properties': {'score': 0.868787944316864, 'page_number': 49}, 'text_representation': 'Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S. (2020). End-to-end object detection with transformers. In\\nA. Vedaldi, H. Bischof, T. Brox, & J. Frahm (Eds.), Computer Vision - ECCV - 16th European Conference, Glasgow, UK, August 23-28,\\nProceedings, Part I (pp. 213–229). Springer volume 12346 of Lecture Notes in Computer Science.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09340778126436121, 0.4082773659446023, 0.9073591164981618, 0.43170376864346593], 'properties': {'score': 0.8779104948043823, 'page_number': 49}}\n",
      "{'type': 'List-item', 'bbox': [0.09335094676298254, 0.44261058460582386, 0.9072559311810662, 0.46593306107954546], 'properties': {'score': 0.885054886341095, 'page_number': 49}}\n",
      "{'type': 'List-item', 'bbox': [0.09422383027918199, 0.4774532803622159, 0.9063905244715074, 0.5258603737571023], 'properties': {'score': 0.905648946762085, 'page_number': 49}, 'text_representation': 'Chen, C., Li, O., Tao, D., Barnett, A., Rudin, C., & Su, J. (2019a). This looks like that: Deep learning for interpretable image recognition.\\nIn H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alch´e-Buc, E. B. Fox, & R. Garnett (Eds.), Advances in Neural Information\\nProcessing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,\\nVancouver, BC, Canada (pp. 8928–8939).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09323585061465993, 0.5368488103693182, 0.907293701171875, 0.5734210205078125], 'properties': {'score': 0.9020447134971619, 'page_number': 49}, 'text_representation': 'Chen, J., Mao, Q., & Liu, D. (2020a). Dual-path transformer network: Direct context-aware modeling for end-to-end monaural speech\\nseparation. In H. Meng, B. Xu, & T. F. Zheng (Eds.), Interspeech, 21st Annual Conference of the International Speech Communication\\nAssociation, Virtual Event, Shanghai, China, 25-29 October (pp. 2642–2646). ISCA.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0937503859576057, 0.5841087757457386, 0.9064804256663603, 0.6326488702947444], 'properties': {'score': 0.9193214774131775, 'page_number': 49}, 'text_representation': 'Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., & Mordatch, I. (2021). Decision transformer:\\nReinforcement learning via sequence modeling. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, & J. W. Vaughan (Eds.),\\nAdvances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems, NeurIPS,\\nDecember 6-14, virtual (pp. 15084–15097).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09325566908892463, 0.6434514271129261, 0.9065011776194853, 0.6671926047585227], 'properties': {'score': 0.9041610360145569, 'page_number': 49}, 'text_representation': 'Chen, M., Challita, U., Saad, W., Yin, C., & Debbah, M. (2019b). Artificial neural networks-based machine learning for wireless networks:\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09390491261201746, 0.6783620383522727, 0.9078270766314338, 0.7146720747514205], 'properties': {'score': 0.914948046207428, 'page_number': 49}, 'text_representation': 'Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., & Sutskever, I. (2020b). Generative pretraining from pixels. In Proceedings\\nof the 37th International Conference on Machine Learning, ICML, 13-18 July, Virtual Event (pp. 1691–1703). PMLR volume 119 of\\nProceedings of Machine Learning Research.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09325853235581343, 0.7255726207386364, 0.9078438074448529, 0.7614873157848011], 'properties': {'score': 0.9155007600784302, 'page_number': 49}, 'text_representation': 'Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., Wu, J., Zhou, L., Ren, S., Qian, Y., Qian,\\nY., Wu, J., Zeng, M., Yu, X., & Wei, F. (2022a). Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE\\nJ. Sel. Top. Signal Process., 16, 1505–1518.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09359303193933824, 0.7728020130504262, 0.9069267721737132, 0.8088861638849432], 'properties': {'score': 0.9181784987449646, 'page_number': 49}, 'text_representation': 'Chen, S., Wu, Y., Wang, C., Chen, Z., Chen, Z., Liu, S., Wu, J., Qian, Y., Wei, F., Li, J., & Yu, X. (2022b). Unispeech-sat: Universal speech\\nrepresentation learning with speaker aware pre-training. In IEEE International Conference on Acoustics, Speech and Signal Processing,\\nICASSP, Virtual and Singapore, 23-27 May (pp. 6152–6156). IEEE.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09341809441061581, 0.820101318359375, 0.9078383501838235, 0.8437787974964489], 'properties': {'score': 0.8965631723403931, 'page_number': 49}, 'text_representation': 'for cone-beam ct-guided adaptive radiotherapy. Frontiers in Oncology, 12.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09344181733972887, 0.854658369584517, 0.9075449505974265, 0.8909454345703125], 'properties': {'score': 0.8931028842926025, 'page_number': 49}, 'text_representation': 'Chen, Y., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z., Cheng, Y., & Liu, J. (2020c). UNITER: universal image-text representation\\nlearning. In Computer Vision - ECCV - 16th European Conference, Glasgow, UK, August 23-28 (pp. 104–120). Springer volume 12375\\nof Lecture Notes in Computer Science.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09352624332203585, 0.9017566472833807, 0.8410653147977941, 0.912691483931108], 'properties': {'score': 0.6119034886360168, 'page_number': 49}, 'text_representation': 'Chowdhary, K., & Chowdhary, K. (2020). Natural language processing. Fundamentals of artificial intelligence, (pp. 603–649).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09354248944450827, 0.9241416792436079, 0.9076030417049632, 0.9475377308238636], 'properties': {'score': 0.8479534983634949, 'page_number': 49}, 'text_representation': 'In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30. OpenReview.net.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09289982515222886, 0.10228587757457386, 0.9081608312270221, 0.1268552745472301], 'properties': {'score': 0.744898796081543, 'page_number': 50}, 'text_representation': 'Clark, P., Tafjord, O., & Richardson, K. (2020b). Transformers as soft reasoners over language. In C. Bessiere (Ed.), Proceedings of the\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09374374389648438, 0.1368086797540838, 0.9079480698529412, 0.1613016024502841], 'properties': {'score': 0.7751932740211487, 'page_number': 50}, 'text_representation': 'Clerckx, B., Huang, K., Varshney, L. R., Ulukus, S., & Alouini, M. (2021). Wireless power transfer for future networks: Signal processing,\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09320258645450367, 0.17130487615411932, 0.908429385914522, 0.20806819568980825], 'properties': {'score': 0.798710823059082, 'page_number': 50}, 'text_representation': 'Conneau, A., Baevski, A., Collobert, R., Mohamed, A., & Auli, M. (2021). Unsupervised cross-lingual representation learning for speech\\nrecognition. In H. Hermansky, H. Cernock´y, L. Burget, L. Lamel, O. Scharenborg, & P. Motl´ıcek (Eds.), Interspeech, 22nd Annual\\nConference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September (pp. 2426–2430). ISCA.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09349164177389706, 0.21866455078125, 0.9075118480009191, 0.25529091574928975], 'properties': {'score': 0.7693103551864624, 'page_number': 50}, 'text_representation': 'Conneau, A., & Lample, G. (2019). Cross-lingual language model pretraining. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alch´e-\\nBuc, E. B. Fox, & R. Garnett (Eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information\\nProcessing Systems 2019, NeurIPS 2019, December 8-14, Vancouver, BC, Canada (pp. 7057–7067).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09346082799574908, 0.26575417258522727, 0.9076566808363971, 0.30254649769176134], 'properties': {'score': 0.8581621646881104, 'page_number': 50}, 'text_representation': 'd’Ascoli, S., Touvron, H., Leavitt, M. L., Morcos, A. S., Biroli, G., & Sagun, L. (2021). Convit: Improving vision transformers with soft\\nconvolutional inductive biases. In M. Meila, & T. Zhang (Eds.), Proceedings of the 38th International Conference on Machine Learning,\\nICML, 18-24 July, Virtual Event (pp. 2286–2296). PMLR volume 139 of Proceedings of Machine Learning Research.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09340494492474724, 0.3126616044477983, 0.9080318675321691, 0.3497960870916193], 'properties': {'score': 0.8412016034126282, 'page_number': 50}, 'text_representation': 'Deng, L., Hinton, G. E., & Kingsbury, B. (2013). New types of deep neural network learning for speech recognition and related applications:\\nan overview. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP, Vancouver, BC, Canada, May\\n26-31 (pp. 8599–8603). IEEE.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09310352998621324, 0.36006838711825284, 0.9082688993566177, 0.40945195978338067], 'properties': {'score': 0.876507043838501, 'page_number': 50}, 'text_representation': 'Devlin, J., Chang, M., Lee, K., & Toutanova, K. (2019). BERT: pre-training of deep bidirectional transformers for language understanding.\\nIn J. Burstein, C. Doran, & T. Solorio (Eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, Volume 1 (Long and\\nShort Papers) (pp. 4171–4186). Association for Computational Linguistics.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09270644244025736, 0.4195351340553977, 0.9084084185431985, 0.46919333718039774], 'properties': {'score': 0.891668438911438, 'page_number': 50}, 'text_representation': 'Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X., Shao, Z., Yang, H., & Tang, J. (2021). Cogview: Mastering\\ntext-to-image generation via transformers. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, & J. W. Vaughan (Eds.), Advances\\nin Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems, NeurIPS, December 6-14,\\nvirtual (pp. 19822–19835).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09297484453986672, 0.4798213334517045, 0.9082947495404412, 0.5159836647727273], 'properties': {'score': 0.8936235904693604, 'page_number': 50}, 'text_representation': 'Dong, L., Xu, S., & Xu, B. (2018). Speech-transformer: A no-recurrence sequence-to-sequence model for speech recognition. In 2018\\nIEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP, Calgary, AB, Canada, April 15-20 (pp. 5884–\\n5888). IEEE.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09300410551183363, 0.5266321355646307, 0.9075859518612133, 0.5638068181818182], 'properties': {'score': 0.8836847543716431, 'page_number': 50}, 'text_representation': 'Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,\\nUszkoreit, J., & Houlsby, N. (2021). An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International\\nConference on Learning Representations, ICLR, Virtual Event, Austria, May 3-7. OpenReview.net.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09307640524471507, 0.5740676047585227, 0.9066585047104779, 0.5982903497869319], 'properties': {'score': 0.8905519247055054, 'page_number': 50}, 'text_representation': 'Fedus, W., Zoph, B., & Shazeer, N. (2021). Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09353998520795037, 0.6086103959517045, 0.9068002498851103, 0.6329037198153409], 'properties': {'score': 0.8836618661880493, 'page_number': 50}, 'text_representation': 'Fournier, Q., Caron, G. M., & Aloise, D. (2021). A practical survey on faster and lighter transformers. CoRR, abs/2103.14636. URL:\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09323090497185202, 0.6428730912642046, 0.9074032054227941, 0.6674488414417613], 'properties': {'score': 0.9044785499572754, 'page_number': 50}, 'text_representation': 'Fu, T., Li, L., Gan, Z., Lin, K., Wang, W. Y., Wang, L., & Liu, Z. (2021). VIOLET : End-to-end video-language transformers with masked\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09389391730813419, 0.6777832586115057, 0.907913818359375, 0.7143937544389205], 'properties': {'score': 0.9071477055549622, 'page_number': 50}, 'text_representation': 'Galanos, T., Liapis, A., & Yannakakis, G. N. (2021). Affectgan: Affect-based generative art driven by semantics. In 9th International\\nConference on Affective Computing and Intelligent Interaction, ACII - Workshops and Demos, Nara, Japan, September 28 - Oct. 1 (pp.\\n1–7). IEEE.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09346455293543199, 0.7249885142933239, 0.906663028492647, 0.7493056418678977], 'properties': {'score': 0.8990820050239563, 'page_number': 50}, 'text_representation': 'Giles, C. L., Chen, D., Sun, G., Chen, H., Lee, Y., & Goudreau, M. W. (1995). Constructive learning of recurrent neural networks:\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0932803165211397, 0.7593855979225852, 0.9070743336397059, 0.796383389559659], 'properties': {'score': 0.906554102897644, 'page_number': 50}, 'text_representation': 'Gong, Y., Chung, Y., & Glass, J. R. (2021). AST: audio spectrogram transformer. In H. Hermansky, H. Cernock´y, L. Burget, L. Lamel,\\nO. Scharenborg, & P. Motl´ıcek (Eds.), Interspeech, 22nd Annual Conference of the International Speech Communication Association,\\nBrno, Czechia, 30 August - 3 September (pp. 571–575). ISCA.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09368449491613051, 0.806986083984375, 0.9075715188419118, 0.8309501509232955], 'properties': {'score': 0.8698117733001709, 'page_number': 50}, 'text_representation': 'Graves, A., & Schmidhuber, J. (2005). Framewise phoneme classification with bidirectional LSTM and other neural network architectures.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09415605432846967, 0.8416758034446022, 0.9083056640625, 0.865638427734375], 'properties': {'score': 0.8473930954933167, 'page_number': 50}}\n",
      "{'type': 'List-item', 'bbox': [0.09357899385340074, 0.8759821666370738, 0.9082871380974264, 0.900404219193892], 'properties': {'score': 0.8825720548629761, 'page_number': 50}, 'text_representation': 'Gu, H., Wang, H., Qin, P., & Wang, J. (2022). Chest l-transformer: local features with position attention for weakly supervised chest\\n radiograph segmentation and classification. Frontiers in Medicine, (p. 1619).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09363490385167739, 0.9109778386896307, 0.9075932760799632, 0.9476267311789772], 'properties': {'score': 0.8935273289680481, 'page_number': 50}, 'text_representation': 'Gu, Y., Li, X., Chen, S., Zhang, J., & Marsic, I. (2017). Speech intention classification with multimodal deep learning. In M. Mouhoub,\\n& P. Langlais (Eds.), Advances in Artificial Intelligence - 30th Canadian Conference on Artificial Intelligence, Canadian AI, Edmonton,\\nAB, Canada, May 16-19, Proceedings (pp. 260–271). volume 10233 of Lecture Notes in Computer Science.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09349822998046875, 0.1036552567915483, 0.9051226447610294, 0.13902901389382102], 'properties': {'score': 0.8009073138237, 'page_number': 51}, 'text_representation': 'Gulati, A., Qin, J., Chiu, C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y., & Pang, R. (2020). Conformer:\\nConvolution-augmented transformer for speech recognition. In H. Meng, B. Xu, & T. F. Zheng (Eds.), Interspeech, 21st Annual Confer-\\nence of the International Speech Communication Association, Virtual Event, Shanghai, China, 25-29 October (pp. 5036–5040). ISCA.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09387182796702666, 0.15138701005415484, 0.90548828125, 0.18651172984730113], 'properties': {'score': 0.8127093315124512, 'page_number': 51}, 'text_representation': 'Guo, J., Han, K., Wu, H., Tang, Y., Chen, X., Wang, Y., & Xu, C. (2022a). CMT: convolutional neural networks meet vision transformers.\\nIn IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, New Orleans, LA, USA, June 18-24 (pp. 12165–12175).\\nIEEE.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09414717730353861, 0.1987879111550071, 0.9055511833639706, 0.2219031316583807], 'properties': {'score': 0.8486694693565369, 'page_number': 51}}\n",
      "{'type': 'List-item', 'bbox': [0.09273562263039982, 0.23386016845703125, 0.9064571605009191, 0.25681013627485794], 'properties': {'score': 0.7682917714118958, 'page_number': 51}}\n",
      "{'type': 'List-item', 'bbox': [0.09345278571633732, 0.26869939630681816, 0.905223819508272, 0.30456548517400567], 'properties': {'score': 0.8081837892532349, 'page_number': 51}, 'text_representation': 'Hajiakhondi-Meybodi, Z., Mohammadi, A., Rahimian, E., Heidarian, S., Abouei, J., & Plataniotis, K. N. (2022). Tedge-caching:\\nIn IEEE International Conference on Communications, ICC Seoul, Korea,\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0933545550178079, 0.31664234508167616, 0.9062857594209559, 0.33985884232954544], 'properties': {'score': 0.804304838180542, 'page_number': 51}, 'text_representation': 'Global Communications Conference, GLOBECOM, Madrid, Spain, December 7-11 (pp. 1–6). IEEE.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0930393892176011, 0.35154951615767044, 0.906024959788603, 0.3745402665571733], 'properties': {'score': 0.7567253708839417, 'page_number': 51}}\n",
      "{'type': 'List-item', 'bbox': [0.09299933938419118, 0.38678538929332384, 0.9063913861443015, 0.42202239990234375], 'properties': {'score': 0.7679837346076965, 'page_number': 51}, 'text_representation': 'Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., & Wang, Y. (2021). Transformer in transformer. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin,\\nP. Liang, & J. W. Vaughan (Eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information\\nProcessing Systems, NeurIPS, December 6-14, virtual (pp. 15908–15919).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09366000007180607, 0.4342472145774148, 0.8337050494025735, 0.44473269375887786], 'properties': {'score': 0.44307565689086914, 'page_number': 51}, 'text_representation': 'Haralick, R. M., & Shapiro, L. G. (1985). Image segmentation techniques. Comput. Vis. Graph. Image Process., 29, 100–132.\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09366000007180607, 0.4342472145774148, 0.8337050494025735, 0.44473269375887786], 'properties': {'score': 0.3934849500656128, 'page_number': 51}, 'text_representation': 'Haralick, R. M., & Shapiro, L. G. (1985). Image segmentation techniques. Comput. Vis. Graph. Image Process., 29, 100–132.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09328309003044577, 0.45681934703480115, 0.9079074994255515, 0.49199390758167616], 'properties': {'score': 0.8643749952316284, 'page_number': 51}, 'text_representation': 'He, H., Bai, Y., Garcia, E. A., & Li, S. (2008). ADASYN: adaptive synthetic sampling approach for imbalanced learning. In Proceedings of\\nthe International Joint Conference on Neural Networks, IJCNN 2008, part of the IEEE World Congress on Computational Intelligence,\\nWCCI, Hong Kong, China, June 1-6 (pp. 1322–1328). IEEE.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09306362376493567, 0.5043362149325284, 0.9059734030330883, 0.5268386563387784], 'properties': {'score': 0.8612022995948792, 'page_number': 51}}\n",
      "{'type': 'List-item', 'bbox': [0.09330615772920496, 0.5395697576349432, 0.9060214412913603, 0.5746474387428977], 'properties': {'score': 0.8432251811027527, 'page_number': 51}, 'text_representation': 'H´enaff, O. J. (2020). Data-efficient image recognition with contrastive predictive coding. In Proceedings of the 37th International Confer-\\nence on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event (pp. 4182–4192). PMLR volume 119 of Proceedings of Machine\\nLearning Research.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09380132338579963, 0.5866184303977273, 0.7254538143382353, 0.597064042524858], 'properties': {'score': 0.8055196404457092, 'page_number': 51}, 'text_representation': 'Hirschberg, J., & Manning, C. D. (2015). Advances in natural language processing. Science, 349, 261–266.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09388872931985294, 0.6093881503018466, 0.8688449994255515, 0.6197873757102272], 'properties': {'score': 0.7800207138061523, 'page_number': 51}, 'text_representation': 'Hirschman, L., & Gaizauskas, R. J. (2001). Natural language question answering: the view from here. Nat. Lang. Eng., 7, 275–300.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09373765833237592, 0.6315762606534091, 0.685694580078125, 0.6420182661576704], 'properties': {'score': 0.7369213700294495, 'page_number': 51}, 'text_representation': 'Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Comput., 9, 1735–1780.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09304666855755975, 0.6539863725142046, 0.907726260914522, 0.6771302934126421], 'properties': {'score': 0.901725709438324, 'page_number': 51}}\n",
      "{'type': 'List-item', 'bbox': [0.09315750122070313, 0.6888699063387784, 0.9067431640625, 0.7121564830433239], 'properties': {'score': 0.8934152126312256, 'page_number': 51}}\n",
      "{'type': 'List-item', 'bbox': [0.09317511165843291, 0.7241176535866477, 0.906558407054228, 0.759773115678267], 'properties': {'score': 0.9068388938903809, 'page_number': 51}, 'text_representation': 'Hu, R., Rohrbach, M., & Darrell, T. (2016). Segmentation from natural language expressions. In B. Leibe, J. Matas, N. Sebe, & M. Welling\\n(Eds.), Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings,\\nPart I (pp. 108–124). Springer volume 9905 of Lecture Notes in Computer Science.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09317201502182905, 0.7718482000177557, 0.906805419921875, 0.7949982799183238], 'properties': {'score': 0.8818885087966919, 'page_number': 51}, 'text_representation': 'Conference on Acoustics, Speech and Signal Processing, ICASSP, Florence, Italy, May 4-9 (pp. 1562–1566). IEEE.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09318641213809743, 0.806805752840909, 0.9062783633961397, 0.8295338023792613], 'properties': {'score': 0.878523051738739, 'page_number': 51}}\n",
      "{'type': 'List-item', 'bbox': [0.0928901762120864, 0.8416741943359375, 0.905980224609375, 0.8650063254616477], 'properties': {'score': 0.87038254737854, 'page_number': 51}, 'text_representation': 'based deep learning model for the detection of malaria parasites from blood cell images. Sensors, 22, 4358.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09254062428193933, 0.876788496537642, 0.9064359777113971, 0.9124538907137784], 'properties': {'score': 0.8659468293190002, 'page_number': 51}, 'text_representation': 'Janner, M., Li, Q., & Levine, S. (2021). Offline reinforcement learning as one big sequence modeling problem. In M. Ranzato, A. Beygelz-\\nimer, Y. N. Dauphin, P. Liang, & J. W. Vaughan (Eds.), Advances in Neural Information Processing Systems 34: Annual Conference on\\nNeural Information Processing Systems, NeurIPS, December 6-14, virtual (pp. 1273–1286).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09271770701688879, 0.9246080433238636, 0.9071103802849265, 0.9476786665482955], 'properties': {'score': 0.8511168956756592, 'page_number': 51}, 'text_representation': 'cloud computing architectures: Recent development, challenges and next research trend. Appl. Soft Comput., 96, 106582.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0933223679486443, 0.10394156716086647, 0.9058755313648897, 0.1521070584383878], 'properties': {'score': 0.8476324677467346, 'page_number': 52}, 'text_representation': 'Jia, C., Yang, Y., Xia, Y., Chen, Y., Parekh, Z., Pham, H., Le, Q. V., Sung, Y., Li, Z., & Duerig, T. (2021). Scaling up visual and vision-\\nlanguage representation learning with noisy text supervision. In M. Meila, & T. Zhang (Eds.), Proceedings of the 38th International\\nConference on Machine Learning, ICML, 18-24 July, Virtual Event (pp. 4904–4916). PMLR volume 139 of Proceedings of Machine\\nLearning Research.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09292918485753676, 0.16418635975230825, 0.9060897288602942, 0.1866160028631037], 'properties': {'score': 0.8176249265670776, 'page_number': 52}}\n",
      "{'type': 'List-item', 'bbox': [0.09316671034869026, 0.19849197387695314, 0.9063546932444853, 0.22142274336381393], 'properties': {'score': 0.8478147387504578, 'page_number': 52}}\n",
      "{'type': 'List-item', 'bbox': [0.09282269646139706, 0.2336678799715909, 0.9059249339384191, 0.25615794788707386], 'properties': {'score': 0.8511430621147156, 'page_number': 52}}\n",
      "{'type': 'List-item', 'bbox': [0.09329747817095588, 0.26822032581676136, 0.9062797277113971, 0.2911959006569602], 'properties': {'score': 0.8876101970672607, 'page_number': 52}, 'text_representation': 'vibration signal classification. Expert Syst. Appl., 171, 114570.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09275793636546416, 0.3028074784712358, 0.905948486328125, 0.32576826615767046], 'properties': {'score': 0.8130637407302856, 'page_number': 52}}\n",
      "{'type': 'List-item', 'bbox': [0.09399673461914063, 0.3375551813299006, 0.9061403521369485, 0.36025684703480115], 'properties': {'score': 0.8567745685577393, 'page_number': 52}}\n",
      "{'type': 'List-item', 'bbox': [0.09335109935087316, 0.3722984175248579, 0.9063473690257353, 0.3953049815784801], 'properties': {'score': 0.8335897326469421, 'page_number': 52}}\n",
      "{'type': 'List-item', 'bbox': [0.09370785881491268, 0.4074605490944602, 0.9069236845128676, 0.4298089322176847], 'properties': {'score': 0.8501800298690796, 'page_number': 52}}\n",
      "{'type': 'List-item', 'bbox': [0.09347349279067095, 0.4418487548828125, 0.9058187327665441, 0.4644650823419744], 'properties': {'score': 0.8425112962722778, 'page_number': 52}}\n",
      "{'type': 'List-item', 'bbox': [0.09380012960994945, 0.4763955411044034, 0.906848934397978, 0.5116063343394887], 'properties': {'score': 0.8627890348434448, 'page_number': 52}, 'text_representation': 'Kim, B., Lee, J., Kang, J., Kim, E., & Kim, H. J. (2021a). HOTR: end-to-end human-object interaction detection with transformers. In\\nIEEE Conference on Computer Vision and Pattern Recognition, CVPR, virtual, June 19-25 (pp. 74–83). Computer Vision Foundation /\\nIEEE.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09358143525965074, 0.5237684770063921, 0.9056193273207721, 0.5592109818892046], 'properties': {'score': 0.8920974135398865, 'page_number': 52}, 'text_representation': 'Kim, W., Son, B., & Kim, I. (2021b). Vilt: Vision-and-language transformer without convolution or region supervision. In M. Meila, &\\nT. Zhang (Eds.), Proceedings of the 38th International Conference on Machine Learning, ICML, 18-24 July, Virtual Event (pp. 5583–\\n5594). PMLR volume 139 of Proceedings of Machine Learning Research.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09402185776654412, 0.5708957741477273, 0.7508322323069853, 0.5811548406427557], 'properties': {'score': 0.6554584503173828, 'page_number': 52}, 'text_representation': 'Kuhn, T. (2014). A survey and classification of controlled natural languages. Comput. Linguistics, 40, 121–170.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09372388951918659, 0.5933329634232954, 0.9058499684053308, 0.6286150568181819], 'properties': {'score': 0.9121780395507812, 'page_number': 52}, 'text_representation': 'Kurin, V., Godil, S., Whiteson, S., & Catanzaro, B. (2020). Can q-learning with graph networks learn a generalizable branching heuristic\\nfor a SAT solver? In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, & H. Lin (Eds.), Advances in Neural Information Processing\\nSystems 33: Annual Conference on Neural Information Processing Systems, NeurIPS, December 6-12, virtual.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09287959379308364, 0.6405288418856534, 0.9068608542049632, 0.6635479181463069], 'properties': {'score': 0.9061137437820435, 'page_number': 52}, 'text_representation': 'Lajk´o, M., Csuvik, V., & Vid´acs, L. (2022). Towards javascript program repair with generative pre-trained transformer (GPT-2). In 3rd\\nIEEE/ACM International Workshop on Automated Program Repair, APR@ICSE, Pittsburgh, PA, USA, May 19 (pp. 61–68). IEEE.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09296112958122703, 0.6753496759588068, 0.9055198759191176, 0.7107126131924716], 'properties': {'score': 0.9088142514228821, 'page_number': 52}, 'text_representation': 'Le, H., Vial, L., Frej, J., Segonne, V., Coavoux, M., Lecouteux, B., Allauzen, A., Crabb´e, B., Besacier, L., & Schwab, D. (2020). Flaubert:\\nUnsupervised language model pre-training for french. In Proceedings of The 12th Language Resources and Evaluation Conference,\\nLREC 2020, Marseille, France, May 11-16, 2020 (pp. 2479–2490). European Language Resources Association.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09285998176125919, 0.7227990167791193, 0.906597110523897, 0.7452867542613636], 'properties': {'score': 0.8871816992759705, 'page_number': 52}}\n",
      "{'type': 'List-item', 'bbox': [0.0925318011115579, 0.7572956431995739, 0.9062144559972426, 0.8055650745738636], 'properties': {'score': 0.9047402739524841, 'page_number': 52}, 'text_representation': 'Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., & Zettlemoyer, L. (2020). BART: denoising\\nsequence-to-sequence pre-training for natural language generation, translation, and comprehension. In D. Jurafsky, J. Chai, N. Schluter,\\n& J. R. Tetreault (Eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online,\\nJuly 5-10, 2020 (pp. 7871–7880). Association for Computational Linguistics.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09243795058306525, 0.8172995272549716, 0.9057317038143382, 0.8526775013316762], 'properties': {'score': 0.8841544389724731, 'page_number': 52}, 'text_representation': 'Li, G., Duan, N., Fang, Y., Gong, M., & Jiang, D. (2020a). Unicoder-vl: A universal encoder for vision and language by cross-modal pre-\\ntraining. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI, New York, NY, USA, February 7-12 (pp. 11336–11344).\\nAAAI Press.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09246546128216912, 0.864522538618608, 0.9063602941176471, 0.8879091020063921], 'properties': {'score': 0.8938189744949341, 'page_number': 52}, 'text_representation': 'review of key properties, current progresses, and future perspectives. Medical image analysis, (p. 102762).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09232253130744485, 0.8990280983664772, 0.9065850471047794, 0.9473822576349432], 'properties': {'score': 0.8622002005577087, 'page_number': 52}, 'text_representation': 'Li, J., Li, D., Xiong, C., & Hoi, S. C. H. (2022). BLIP: bootstrapping language-image pre-training for unified vision-language understanding\\nand generation. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesv´ari, G. Niu, & S. Sabato (Eds.), International Conference on Machine\\nLearning, ICML, 17-23 July, Baltimore, Maryland, USA (pp. 12888–12900). PMLR volume 162 of Proceedings of Machine Learning\\nResearch.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09270179299747243, 0.10382455305619673, 0.907227783203125, 0.12690873579545456], 'properties': {'score': 0.7235301733016968, 'page_number': 53}}\n",
      "{'type': 'List-item', 'bbox': [0.09301462510052849, 0.13847868485884232, 0.9063608685661765, 0.16137419267134234], 'properties': {'score': 0.7496315836906433, 'page_number': 53}}\n",
      "{'type': 'List-item', 'bbox': [0.09240325927734375, 0.17303563898259944, 0.9051885627297794, 0.20806644786487927], 'properties': {'score': 0.7606813907623291, 'page_number': 53}, 'text_representation': 'Li, S., & Hoefler, T. (2021). Chimera: efficiently training large-scale neural networks with bidirectional pipelines. In B. R. de Supinski,\\nM. W. Hall, & T. Gamblin (Eds.), International Conference for High Performance Computing, Networking, Storage and Analysis, SC,\\nSt. Louis, Missouri, USA, November 14-19 (p. 27). ACM.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09278610229492187, 0.21989033092151988, 0.9055386891084559, 0.24290175004438921], 'properties': {'score': 0.7286918759346008, 'page_number': 53}}\n",
      "{'type': 'List-item', 'bbox': [0.0933325554342831, 0.2541343550248579, 0.6417967313878676, 0.26479053844105116], 'properties': {'score': 0.559063196182251, 'page_number': 53}, 'text_representation': 'Lin, T., Wang, Y., Liu, X., & Qiu, X. (2022). A survey of transformers. AI Open, 3, 111–132.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0929652494542739, 0.27623307661576707, 0.9060536822150735, 0.31170301957563923], 'properties': {'score': 0.776702344417572, 'page_number': 53}, 'text_representation': 'Liu, A. T., Yang, S., Chi, P., Hsu, P., & Lee, H. (2020). Mockingjay: Unsupervised speech representation learning with deep bidirectional\\ntransformer encoders. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP, Barcelona, Spain, May\\n4-8 (pp. 6419–6423). IEEE.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09318601720473346, 0.32350216952237215, 0.9059317555147058, 0.34646046031605116], 'properties': {'score': 0.7729437947273254, 'page_number': 53}}\n",
      "{'type': 'List-item', 'bbox': [0.09258735207950368, 0.35811079545454544, 0.906256103515625, 0.38094074596058236], 'properties': {'score': 0.7830362915992737, 'page_number': 53}}\n",
      "{'type': 'List-item', 'bbox': [0.09286507999195773, 0.3927721613103693, 0.90634521484375, 0.42786715420809657], 'properties': {'score': 0.7960891723632812, 'page_number': 53}, 'text_representation': 'Liu, M., Breuel, T. M., & Kautz, J. (2017). Unsupervised image-to-image translation networks. In I. Guyon, U. von Luxburg, S. Bengio,\\nH. M. Wallach, R. Fergus, S. V. N. Vishwanathan, & R. Garnett (Eds.), Advances in Neural Information Processing Systems 30: Annual\\nConference on Neural Information Processing Systems 2017, December 4-9, Long Beach, CA, USA (pp. 700–708).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09291920381433824, 0.43923112349076704, 0.9058682071461397, 0.47506236683238634], 'properties': {'score': 0.8434374332427979, 'page_number': 53}, 'text_representation': 'Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019). RoBERTa:\\nA robustly optimized BERT pretraining approach. CoRR, abs/1907.11692. URL: http://arxiv.org/abs/1907.11692.\\narXiv:1907.11692.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09294426413143382, 0.4866763583096591, 0.9057883588005514, 0.5218541370738636], 'properties': {'score': 0.8494662046432495, 'page_number': 53}, 'text_representation': 'Liu, Y., Qiao, L., Yin, D., Jiang, Z., Jiang, X., Jiang, D., & Ren, B. (2022c). OS-MSL: one stage multimodal sequential link framework for\\nscene segmentation and classification. In J. Magalh˜aes, A. D. Bimbo, S. Satoh, N. Sebe, X. Alameda-Pineda, Q. Jin, V. Oria, & L. Toni\\n(Eds.), MM: The 30th ACM International Conference on Multimedia, Lisboa, Portugal, October 10 - 14 (pp. 6269–6277). ACM.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09277659696691176, 0.5341130482066762, 0.906495792164522, 0.569038252397017], 'properties': {'score': 0.8459954857826233, 'page_number': 53}, 'text_representation': 'Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using\\nshifted windows. In 2021 IEEE/CVF International Conference on Computer Vision ICCV, Montreal, QC, Canada, October 10-17 (pp.\\n9992–10002). IEEE.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09280147776884191, 0.5810057484019886, 0.9062738396139706, 0.6040884676846591], 'properties': {'score': 0.8100905418395996, 'page_number': 53}, 'text_representation': 'cross-frequency coupling in human sensorimotor cortex. PLoS Comput. Biol., 15.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09289535522460937, 0.6169034645774147, 0.9063228113511029, 0.6401221257990057], 'properties': {'score': 0.8071941137313843, 'page_number': 53}, 'text_representation': 'using deep learning. Deep Learning in Healthcare: Paradigms and Applications, (pp. 17–31).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09258773803710937, 0.6515271550958807, 0.9058555692784926, 0.6739286110617898], 'properties': {'score': 0.8493738174438477, 'page_number': 53}}\n",
      "{'type': 'List-item', 'bbox': [0.0928319684196921, 0.6861823064630682, 0.9060672535615809, 0.7341644287109375], 'properties': {'score': 0.8365516066551208, 'page_number': 53}, 'text_representation': 'Lu, J., Batra, D., Parikh, D., & Lee, S. (2019). Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language\\ntasks. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alch´e-Buc, E. B. Fox, & R. Garnett (Eds.), Advances in Neural Information\\nProcessing Systems 32: Annual Conference on Neural Information Processing Systems, NeurIPS, December 8-14, Vancouver, BC,\\nCanada (pp. 13–23).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0929021857766544, 0.7454892245205966, 0.9074763039981618, 0.7685460870916193], 'properties': {'score': 0.8164946436882019, 'page_number': 53}, 'text_representation': 'Syst., 257, 109959.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09313237807329963, 0.7799457341974432, 0.9065810977711397, 0.803195467862216], 'properties': {'score': 0.7886143922805786, 'page_number': 53}, 'text_representation': 'MRI images. IET Image Process., 14, 2541–2552.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09272452859317555, 0.8145589932528409, 0.9065981876148898, 0.837258467240767], 'properties': {'score': 0.7868358492851257, 'page_number': 53}}\n",
      "{'type': 'List-item', 'bbox': [0.09236827177159926, 0.8488809481534091, 0.9065546731387868, 0.9474031205610796], 'properties': {'score': 0.8512923717498779, 'page_number': 53}, 'text_representation': 'Menze, B. H., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J. S., Burren, Y., Porz, N., Slotboom, J., Wiest, R., Lanczi,\\nL., Gerstner, E. R., Weber, M., Arbel, T., Avants, B. B., Ayache, N., Buendia, P., Collins, D. L., Cordier, N., Corso, J. J., Criminisi, A.,\\nDas, T., Delingette, H., Demiralp, C¸ ., Durst, C. R., Dojat, M., Doyle, S., Festa, J., Forbes, F., Geremia, E., Glocker, B., Golland, P., Guo,\\nX., Hamamci, A., Iftekharuddin, K. M., Jena, R., John, N. M., Konukoglu, E., Lashkari, D., Mariz, J. A., Meier, R., Pereira, S., Precup,\\nD., Price, S. J., Raviv, T. R., Reza, S. M. S., Ryan, M. T., Sarikaya, D., Schwartz, L. H., Shin, H., Shotton, J., Silva, C. A., Sousa, N. J.,\\nSubbanna, N. K., Sz´ekely, G., Taylor, T. J., Thomas, O. M., Tustison, N. J., ¨Unal, G. B., Vasseur, F., Wintermark, M., Ye, D. H., Zhao,\\nL., Zhao, B., Zikic, D., Prastawa, M., Reyes, M., & Leemput, K. V. (2015). The multimodal brain tumor image segmentation benchmark\\n(BRATS). IEEE Trans. Medical Imaging, 34, 1993–2024.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09309511970071231, 0.10371728376908736, 0.9060163430606618, 0.13907208529385653], 'properties': {'score': 0.7481966614723206, 'page_number': 54}, 'text_representation': 'In\\nT. Kobayashi, K. Hirose, & S. Nakamura (Eds.), INTERSPEECH 2010, 11th Annual Conference of the International Speech Com-\\nmunication Association, Makuhari, Chiba, Japan, September 26-30, 2010 (pp. 1045–1048). ISCA.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09318912281709558, 0.1515198447487571, 0.9057962574678309, 0.17433689464222302], 'properties': {'score': 0.7407431602478027, 'page_number': 54}}\n",
      "{'type': 'List-item', 'bbox': [0.09355446310604319, 0.18697998046875, 0.5832426542394301, 0.19720069191672585], 'properties': {'score': 0.49271172285079956, 'page_number': 54}, 'text_representation': 'Monroe, D. (2017). Deep learning takes on translation. Commun. ACM, 60, 12–14.\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09355446310604319, 0.18697998046875, 0.5832426542394301, 0.19720069191672585], 'properties': {'score': 0.3514719009399414, 'page_number': 54}, 'text_representation': 'Monroe, D. (2017). Deep learning takes on translation. Commun. ACM, 60, 12–14.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09413522159352022, 0.20958595969460228, 0.7204953182444853, 0.21986594460227274], 'properties': {'score': 0.44968387484550476, 'page_number': 54}, 'text_representation': 'Murtagh, F. (1990). Multilayer perceptrons for classification and regression. Neurocomputing, 2, 183–197.\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09413522159352022, 0.20958595969460228, 0.7204953182444853, 0.21986594460227274], 'properties': {'score': 0.3929832875728607, 'page_number': 54}, 'text_representation': 'Murtagh, F. (1990). Multilayer perceptrons for classification and regression. Neurocomputing, 2, 183–197.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09320781034581802, 0.2321896084872159, 0.9059896312040441, 0.267720420143821], 'properties': {'score': 0.8067719340324402, 'page_number': 54}, 'text_representation': 'Nagrani, A., Yang, S., Arnab, A., Jansen, A., Schmid, C., & Sun, C. (2021). Attention bottlenecks for multimodal fusion. In M. Ranzato,\\nA. Beygelzimer, Y. N. Dauphin, P. Liang, & J. W. Vaughan (Eds.), Advances in Neural Information Processing Systems 34: Annual\\nConference on Neural Information Processing Systems, NeurIPS, December 6-14, virtual (pp. 14200–14213).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09292721916647519, 0.28042144775390626, 0.9053728170955883, 0.3026492032137784], 'properties': {'score': 0.7995155453681946, 'page_number': 54}}\n",
      "{'type': 'List-item', 'bbox': [0.09338069242589614, 0.31516224254261366, 0.9059835994944853, 0.363106855912642], 'properties': {'score': 0.85323166847229, 'page_number': 54}, 'text_representation': 'Nichol, A. Q., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., & Chen, M. (2022). GLIDE: towards\\nphotorealistic image generation and editing with text-guided diffusion models. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesv´ari,\\nG. Niu, & S. Sabato (Eds.), International Conference on Machine Learning, ICML, 17-23 July, Baltimore, Maryland, USA (pp. 16784–\\n16804). PMLR volume 162 of Proceedings of Machine Learning Research.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0934341251148897, 0.37570195978338067, 0.8161486098345588, 0.3862875088778409], 'properties': {'score': 0.6051259636878967, 'page_number': 54}, 'text_representation': 'Niu, Z., Zhong, G., & Yu, H. (2021). A review on the attention mechanism of deep learning. Neurocomputing, 452, 48–62.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0938593157599954, 0.39852039683948864, 0.9064423684512868, 0.4462757734818892], 'properties': {'score': 0.8846153020858765, 'page_number': 54}, 'text_representation': 'van den Oord, A., Kalchbrenner, N., Espeholt, L., Kavukcuoglu, K., Vinyals, O., & Graves, A. (2016). Conditional image generation\\nwith pixelcnn decoders. In D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, & R. Garnett (Eds.), Advances in Neural Information\\nProcessing Systems 29: Annual Conference on Neural Information Processing Systems, December 5-10, Barcelona, Spain (pp. 4790–\\n4798).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09338292738970588, 0.45882335316051137, 0.9045847455193015, 0.48131031383167616], 'properties': {'score': 0.8564772605895996, 'page_number': 54}}\n",
      "{'type': 'List-item', 'bbox': [0.0941937255859375, 0.49378512295809657, 0.9062310431985294, 0.5415824751420455], 'properties': {'score': 0.9033058881759644, 'page_number': 54}, 'text_representation': 'Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton,\\nJ., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., & Lowe, R. (2022). Training language models to\\nfollow instructions with human feedback. CoRR, abs/2203.02155. URL: https://doi.org/10.48550/arXiv.2203.02155.\\ndoi:10.48550/arXiv.2203.02155. arXiv:2203.02155.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09328344906077665, 0.5542191938920454, 0.9048304658777574, 0.5646921053799716], 'properties': {'score': 0.6612836718559265, 'page_number': 54}, 'text_representation': 'Pang, Y., Lin, J., Qin, T., & Chen, Z. (2022). Image-to-image translation: Methods and applications. IEEE Trans. Multim., 24, 3859–3881.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09343254538143382, 0.5769651655717329, 0.9059110035615808, 0.6249733664772728], 'properties': {'score': 0.8915804624557495, 'page_number': 54}, 'text_representation': 'Parisotto, E., Song, H. F., Rae, J. W., Pascanu, R., G¨ulc¸ehre, C¸ ., Jayakumar, S. M., Jaderberg, M., Kaufman, R. L., Clark, A., Noury, S.,\\nBotvinick, M. M., Heess, N., & Hadsell, R. (2020). Stabilizing transformers for reinforcement learning. In Proceedings of the 37th\\nInternational Conference on Machine Learning, ICML, 13-18 July, Virtual Event (pp. 7487–7498). PMLR volume 119 of Proceedings\\nof Machine Learning Research.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09318646599264706, 0.637733154296875, 0.9063523954503676, 0.6727828147194602], 'properties': {'score': 0.8738967180252075, 'page_number': 54}, 'text_representation': 'Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., & Tran, D. (2018). Image transformer. In J. G. Dy, & A. Krause\\n(Eds.), Proceedings of the 35th International Conference on Machine Learning, ICML, Stockholmsm¨assan, Stockholm, Sweden, July\\n10-15 (pp. 4052–4061). PMLR volume 80 of Proceedings of Machine Learning Research.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09342819213867187, 0.685106534090909, 0.907076416015625, 0.72053466796875], 'properties': {'score': 0.8899085521697998, 'page_number': 54}, 'text_representation': 'Peng, Z., Huang, W., Gu, S., Xie, L., Wang, Y., Jiao, J., & Ye, Q. (2021). Conformer: Local features coupling global representations\\nfor visual recognition. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October\\n10-17, 2021 (pp. 357–366). IEEE.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09338474049287684, 0.7331252774325284, 0.9062660845588235, 0.7683937211470171], 'properties': {'score': 0.8755409717559814, 'page_number': 54}, 'text_representation': 'Picco, G., Hoang, T. L., Sbodio, M. L., & L´opez, V. (2021). Neural unification for logic reasoning over natural language. In M. Moens,\\nX. Huang, L. Specia, & S. W. Yih (Eds.), Findings of the Association for Computational Linguistics: EMNLP, Virtual Event / Punta\\nCana, Dominican Republic, 16-20 November (pp. 3939–3950). Association for Computational Linguistics.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09324812945197611, 0.7808784068714488, 0.9061081112132353, 0.8038642467151988], 'properties': {'score': 0.8454747200012207, 'page_number': 54}}\n",
      "{'type': 'List-item', 'bbox': [0.09327435661764706, 0.8163773415305398, 0.9047952090992647, 0.8386835271661932], 'properties': {'score': 0.8418269157409668, 'page_number': 54}}\n",
      "{'type': 'List-item', 'bbox': [0.09378108305089614, 0.8512769109552557, 0.9057167681525735, 0.886563720703125], 'properties': {'score': 0.8928713202476501, 'page_number': 54}, 'text_representation': 'Qi, W., Yan, Y., Gong, Y., Liu, D., Duan, N., Chen, J., Zhang, R., & Zhou, M. (2020). Prophetnet: Predicting future n-gram for sequence-\\nto-sequence pre-training. In T. Cohn, Y. He, & Y. Liu (Eds.), Findings of the Association for Computational Linguistics: EMNLP 2020,\\nOnline Event, 16-20 November (pp. 2401–2410). Association for Computational Linguistics volume EMNLP 2020 of Findings of ACL.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09314235911649817, 0.8988333962180398, 0.9064435891544118, 0.9471503240411931], 'properties': {'score': 0.8601083159446716, 'page_number': 54}, 'text_representation': 'Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G.,\\n& Sutskever, I. (2021). Learning transferable visual models from natural language supervision.\\nIn M. Meila, & T. Zhang (Eds.),\\nProceedings of the 38th International Conference on Machine Learning, ICML, 18-24 July, Virtual Event (pp. 8748–8763). PMLR\\nvolume 139 of Proceedings of Machine Learning Research.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09345236385569852, 0.10397759871049361, 0.9056752642463235, 0.13898438887162642], 'properties': {'score': 0.8562965393066406, 'page_number': 55}, 'text_representation': 'Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (2022). Robust speech recognition via large-scale weak\\nsupervision. CoRR, abs/2212.04356. URL: https://doi.org/10.48550/arXiv.2212.04356. doi:10.48550/arXiv.\\n2212.04356. arXiv:2212.04356.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09369271671070772, 0.15160342129794033, 0.9051075654871323, 0.17494737104936078], 'properties': {'score': 0.8492457866668701, 'page_number': 55}, 'text_representation': 'nical Report OpenAI.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09324133480296415, 0.18670260342684658, 0.905920840992647, 0.20983713323419745], 'properties': {'score': 0.8707168698310852, 'page_number': 55}}\n",
      "{'type': 'List-item', 'bbox': [0.09359309476964614, 0.2216035322709517, 0.9062782197840074, 0.24445301402698863], 'properties': {'score': 0.8830215930938721, 'page_number': 55}}\n",
      "{'type': 'List-item', 'bbox': [0.09331998039694393, 0.25725832852450287, 0.9057429055606617, 0.2922509488192472], 'properties': {'score': 0.8870452642440796, 'page_number': 55}, 'text_representation': 'Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., & Sutskever, I. (2021). Zero-shot text-to-image generation. In\\nM. Meila, & T. Zhang (Eds.), Proceedings of the 38th International Conference on Machine Learning, ICML, 18-24 July, Virtual Event\\n(pp. 8821–8831). PMLR volume 139 of Proceedings of Machine Learning Research.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09424060597139246, 0.3046553455699574, 0.75016357421875, 0.3153341674804688], 'properties': {'score': 0.5137947797775269, 'page_number': 55}, 'text_representation': 'Reiter, E., & Dale, R. (1997). Building applied natural language generation systems. Nat. Lang. Eng., 3, 57–87.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09351346184225644, 0.32717776211825284, 0.9051213522518382, 0.35001104181463066], 'properties': {'score': 0.8714861869812012, 'page_number': 55}}\n",
      "{'type': 'List-item', 'bbox': [0.09389477898092831, 0.3626642400568182, 0.9055807674632353, 0.39761002974076703], 'properties': {'score': 0.8904942274093628, 'page_number': 55}, 'text_representation': 'Ren, Z., Cheng, N., Sun, R., Wang, X., Lu, N., & Xu, W. (2022). Sigt: An efficient end-to-end MIMO-OFDM receiver framework based\\non transformer. In 5th International Conference on Communications, Signal Processing, and their Applications, ICCSPA, Cairo, Egypt,\\nDecember 27-29 (pp. 1–6). IEEE.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09364724551930147, 0.40997411554509944, 0.9053319594439339, 0.4332161365855824], 'properties': {'score': 0.8969106674194336, 'page_number': 55}, 'text_representation': 'forecasting with a comparative analysis to recurrent neural networks. Expert Syst. Appl., 202, 117275.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09365296307732077, 0.4454334328391335, 0.905731201171875, 0.46788341175426135], 'properties': {'score': 0.9016591310501099, 'page_number': 55}, 'text_representation': 'Richardson, K., & Sabharwal, A. (2022). Pushing the limits of rule reasoning in transformers through natural language satisfiability. In\\nThirty-Sixth AAAI Conference on Artificial Intelligence, AAAI, Virtual Event, February 22 - March 1 (pp. 11209–11219). AAAI Press.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09322393080767463, 0.4804757967862216, 0.9056670065487132, 0.5034746204723012], 'properties': {'score': 0.8989776968955994, 'page_number': 55}, 'text_representation': 'large-scale cloud computing systems. Concurrency and Computation: Practice and Experience, 33, e5919.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09346958833582261, 0.5155645751953125, 0.9061046645220588, 0.5383263605291193], 'properties': {'score': 0.907369077205658, 'page_number': 55}, 'text_representation': 'Rjoub, G., Bentahar, J., Wahab, O. A., & Bataineh, A. (2019). Deep smart scheduling: A deep learning approach for automated big data\\nscheduling over the cloud. In 2019 7th International Conference on Future Internet of Things and Cloud (FiCloud) (pp. 189–196). IEEE.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09308680814855239, 0.5503596080433238, 0.9054104434742647, 0.5735007546164773], 'properties': {'score': 0.9000385999679565, 'page_number': 55}}\n",
      "{'type': 'List-item', 'bbox': [0.09407484166762409, 0.5858635364879261, 0.6967514935661765, 0.5962361838600853], 'properties': {'score': 0.5147168040275574, 'page_number': 55}, 'text_representation': 'Ruan, L., & Jin, Q. (2022). Survey: Transformer based video-language pre-training. AI Open, 3, 1–13.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09385165943818934, 0.6081628972833807, 0.906298397288603, 0.6437616521661932], 'properties': {'score': 0.9197226166725159, 'page_number': 55}, 'text_representation': 'Saha, S., Ghosh, S., Srivastava, S., & Bansal, M. (2020). Prover: Proof generation for interpretable reasoning over rules. In B. Webber,\\nT. Cohn, Y. He, & Y. Liu (Eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP\\n2020, Online, November 16-20, 2020 (pp. 122–136). Association for Computational Linguistics.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09388208725873162, 0.6559517045454546, 0.9055325137867647, 0.6786822509765625], 'properties': {'score': 0.9259129166603088, 'page_number': 55}}\n",
      "{'type': 'List-item', 'bbox': [0.09384882309857537, 0.6912663685191761, 0.9064540010340073, 0.7138284024325284], 'properties': {'score': 0.9013983011245728, 'page_number': 55}}\n",
      "{'type': 'List-item', 'bbox': [0.09348764756146599, 0.7264918101917613, 0.9065301154641544, 0.7491303599964488], 'properties': {'score': 0.9046103954315186, 'page_number': 55}}\n",
      "{'type': 'List-item', 'bbox': [0.09334844252642463, 0.7615279873934659, 0.9062077780330883, 0.7843695623224431], 'properties': {'score': 0.9075489044189453, 'page_number': 55}, 'text_representation': 'Medical Image Analysis, (p. 102802).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09345179838292739, 0.796424727006392, 0.9062240780101103, 0.8195333584872159], 'properties': {'score': 0.9065154790878296, 'page_number': 55}, 'text_representation': 'invariance adaptive observer. Journal of Energy Storage, 45, 103768.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09378611845128676, 0.8318611283735795, 0.9059441779641544, 0.8546451083096591], 'properties': {'score': 0.9097552299499512, 'page_number': 55}, 'text_representation': 'segmentation. Quantitative Imaging in Medicine and Surgery, 12, 4512.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09392177806181067, 0.8669181685014204, 0.8466342342601103, 0.8771569269353693], 'properties': {'score': 0.8442521691322327, 'page_number': 55}, 'text_representation': 'Shi, C., Xiao, Y., & Chen, Z. (2022a). Dual-domain sparse-view ct reconstruction with transformers. Physica Medica, 101, 1–7.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09329345703125, 0.889251708984375, 0.9065089326746324, 0.9121923273259943], 'properties': {'score': 0.9215406775474548, 'page_number': 55}}\n",
      "{'type': 'List-item', 'bbox': [0.09373253317440257, 0.9248478005149148, 0.9064603199678308, 0.9470636541193181], 'properties': {'score': 0.9134834408760071, 'page_number': 55}}\n",
      "{'type': 'List-item', 'bbox': [0.09380724738625919, 0.10376946189186789, 0.9072231158088235, 0.12666441483931107], 'properties': {'score': 0.8202146291732788, 'page_number': 56}}\n",
      "{'type': 'List-item', 'bbox': [0.09396364997414981, 0.1406319912997159, 0.9068984805836398, 0.16294197776100852], 'properties': {'score': 0.781067430973053, 'page_number': 56}}\n",
      "{'type': 'List-item', 'bbox': [0.09401718139648438, 0.17693002874200994, 0.9060723517922794, 0.22494645552201706], 'properties': {'score': 0.8768142461776733, 'page_number': 56}, 'text_representation': 'Sinha, K., Sodhani, S., Dong, J., Pineau, J., & Hamilton, W. L. (2019). CLUTRR: A diagnostic benchmark for inductive reasoning from text.\\nIn K. Inui, J. Jiang, V. Ng, & X. Wan (Eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\\nand the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP, Hong Kong, China, November 3-7 (pp.\\n4505–4514). Association for Computational Linguistics.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09374252319335938, 0.23881769353693183, 0.9065970387178309, 0.261918778852983], 'properties': {'score': 0.8838064074516296, 'page_number': 56}}\n",
      "{'type': 'List-item', 'bbox': [0.09444195018095129, 0.2755900712446733, 0.9073983944163603, 0.29810114080255684], 'properties': {'score': 0.8251668214797974, 'page_number': 56}}\n",
      "{'type': 'List-item', 'bbox': [0.09394091437844669, 0.31194435813210225, 0.9060731416590073, 0.33520041725852273], 'properties': {'score': 0.8185426592826843, 'page_number': 56}}\n",
      "{'type': 'List-item', 'bbox': [0.09418735279756434, 0.3489252818714489, 0.9060056439568015, 0.37161260431463067], 'properties': {'score': 0.8422232270240784, 'page_number': 56}}\n",
      "{'type': 'List-item', 'bbox': [0.09408712050494025, 0.3853439053622159, 0.9064336081112132, 0.42076080322265624], 'properties': {'score': 0.887385904788971, 'page_number': 56}, 'text_representation': 'Sun, H., Chen, X., Shi, Q., Hong, M., Fu, X., & Sidiropoulos, N. D. (2017). Learning to optimize: Training deep neural networks for\\nIn 18th IEEE International Workshop on Signal Processing Advances in Wireless Communications,\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09395015043370863, 0.4343963068181818, 0.9068312701056985, 0.45731803200461646], 'properties': {'score': 0.8271592855453491, 'page_number': 56}}\n",
      "{'type': 'List-item', 'bbox': [0.09395918004653032, 0.47123291015625, 0.9068345731847427, 0.49447382146661933], 'properties': {'score': 0.8487735986709595, 'page_number': 56}, 'text_representation': 'segmentation. Journal of Healthcare Engineering, 2021.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09494027530445773, 0.5076149125532671, 0.7512042595358456, 0.5180657404119318], 'properties': {'score': 0.5740978121757507, 'page_number': 56}, 'text_representation': 'Suzuki, M., & Matsuo, Y. (2022). A survey of multimodal deep generative models. Adv. Robotics, 36, 261–278.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0936497407801011, 0.531786221590909, 0.9083769674862132, 0.5549526145241477], 'properties': {'score': 0.8144640922546387, 'page_number': 56}}\n",
      "{'type': 'List-item', 'bbox': [0.09323182947495405, 0.5685439231178977, 0.9067117130055147, 0.6168633478338068], 'properties': {'score': 0.9011738300323486, 'page_number': 56}, 'text_representation': 'Tan, H., & Bansal, M. (2019). LXMERT: learning cross-modality encoder representations from transformers. In K. Inui, J. Jiang, V. Ng,\\n& X. Wan (Eds.), Proceedings of the Conference on Empirical Methods in Natural Language Processing and the 9th International Joint\\nConference on Natural Language Processing, EMNLP-IJCNLP, Hong Kong, China, November 3-7 (pp. 5099–5110). Association for\\nComputational Linguistics.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09397858563591452, 0.6303673206676136, 0.7317652803308824, 0.6407985063032671], 'properties': {'score': 0.7300907373428345, 'page_number': 56}, 'text_representation': 'Tas, O., & Kiyani, F. (2007). A survey automatic text summarization. PressAcademia Procedia, 5, 205–213.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09421017815085019, 0.6542689098011364, 0.8534364947150735, 0.6648146750710228], 'properties': {'score': 0.6892752647399902, 'page_number': 56}, 'text_representation': 'Tay, Y., Dehghani, M., Bahri, D., & Metzler, D. (2023). Efficient transformers: A survey. ACM Comput. Surv., 55, 109:1–109:28.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09404564352596508, 0.6785585715553978, 0.9059906364889706, 0.7136131702769887], 'properties': {'score': 0.9238619208335876, 'page_number': 56}, 'text_representation': 'Tay, Y., Tran, V. Q., Ruder, S., Gupta, J. P., Chung, H. W., Bahri, D., Qin, Z., Baumgartner, S., Yu, C., & Metzler, D. (2022). Charformer:\\nFast character transformers via gradient-based subword tokenization. In The Tenth International Conference on Learning Representa-\\ntions, ICLR 2022, Virtual Event, April 25-29. OpenReview.net.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09366679472081801, 0.7275496604225852, 0.9072227567784926, 0.7630022638494318], 'properties': {'score': 0.9076030254364014, 'page_number': 56}, 'text_representation': 'Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., & J´egou, H. (2021). Training data-efficient image transformers & distillation\\nthrough attention. In M. Meila, & T. Zhang (Eds.), Proceedings of the 38th International Conference on Machine Learning, ICML, 18-24\\nJuly, Virtual Event (pp. 10347–10357). PMLR volume 139 of Proceedings of Machine Learning Research.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09361222211052389, 0.7767247425426136, 0.9067822983685662, 0.8244808127663352], 'properties': {'score': 0.9091683030128479, 'page_number': 56}, 'text_representation': 'Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention is all you need.\\nIn I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, & R. Garnett (Eds.), Advances in Neural\\nInformation Processing Systems 30: Annual Conference on Neural Information Processing Systems, December 4-9, Long Beach, CA,\\nUSA (pp. 5998–6008).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09340146233053769, 0.8384016557173295, 0.9070575310202206, 0.8742363392223012], 'properties': {'score': 0.914686381816864, 'page_number': 56}, 'text_representation': 'Vig, J., Madani, A., Varshney, L. R., Xiong, C., Socher, R., & Rajani, N. F. (2021). Bertology meets biology: Interpreting attention\\nIn 9th International Conference on Learning Representations, ICLR, Virtual Event, Austria, May 3-7.\\n in protein language models.\\nOpenReview.net.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.093774315329159, 0.8879410067471591, 0.9056056841681985, 0.910879960493608], 'properties': {'score': 0.8494846224784851, 'page_number': 56}, 'text_representation': 'Process., 26, 1702–1726.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09402481079101563, 0.9244131192294034, 0.9062449735753676, 0.9475653631036932], 'properties': {'score': 0.8656160235404968, 'page_number': 56}, 'text_representation': 'International Conference on Control, Robotics and Intelligent System, Xiamen, China, October 27-29 (pp. 176–184). ACM.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09362179026884192, 0.10399151888760653, 0.906045711741728, 0.13883346557617188], 'properties': {'score': 0.787153422832489, 'page_number': 57}, 'text_representation': 'Wang, J., Yang, Z., Hu, X., Li, L., Lin, K., Gan, Z., Liu, Z., Liu, C., & Wang, L. (2022a). GIT: A generative image-to-text transformer\\nfor vision and language. CoRR, abs/2205.14100. URL: https://doi.org/10.48550/arXiv.2205.14100. doi:10.48550/\\narXiv.2205.14100. arXiv:2205.14100.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0942831600413603, 0.1530766435102983, 0.905424373851103, 0.17590266834605825], 'properties': {'score': 0.7582940459251404, 'page_number': 57}}\n",
      "{'type': 'List-item', 'bbox': [0.09391021728515625, 0.18989199551669034, 0.9063459329044118, 0.2123390752618963], 'properties': {'score': 0.7667701244354248, 'page_number': 57}}\n",
      "{'type': 'List-item', 'bbox': [0.0935462682387408, 0.2267387528852983, 0.9069732306985294, 0.2617577292702415], 'properties': {'score': 0.7848407030105591, 'page_number': 57}, 'text_representation': 'Wang, T., Lai, Z., & Kong, H. (2021b). Tfnet: Transformer fusion network for ultrasound image segmentation. In C. Wallraven, Q. Liu,\\n& H. Nagahara (Eds.), Pattern Recognition - 6th Asian Conference, ACPR, Jeju Island, South Korea, November 9-12, Revised Selected\\nPapers, Part I (pp. 314–325). Springer volume 13188 of Lecture Notes in Computer Science.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09393325805664063, 0.27566109397194605, 0.9062691722196691, 0.29861608331853695], 'properties': {'score': 0.786891520023346, 'page_number': 57}}\n",
      "{'type': 'List-item', 'bbox': [0.09353563196518842, 0.31269262140447446, 0.9063047162224265, 0.3350689697265625], 'properties': {'score': 0.7967144250869751, 'page_number': 57}}\n",
      "{'type': 'List-item', 'bbox': [0.0933565476361443, 0.3491001475941051, 0.9054327033547794, 0.37193395441228694], 'properties': {'score': 0.7712907195091248, 'page_number': 57}}\n",
      "{'type': 'List-item', 'bbox': [0.09393492754767922, 0.38591089422052555, 0.905877254710478, 0.40881261652166195], 'properties': {'score': 0.7860762476921082, 'page_number': 57}, 'text_representation': 'Wang, Z., Yu, J., Yu, A. W., Dai, Z., Tsvetkov, Y., & Cao, Y. (2022d). Simvlm: Simple visual language model pretraining with weak\\nsupervision. In The Tenth International Conference on Learning Representations, ICLR, Virtual Event, April 25-29. OpenReview.net.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09385293399586397, 0.42253922895951707, 0.9062795122931985, 0.44498843106356534], 'properties': {'score': 0.7737049460411072, 'page_number': 57}}\n",
      "{'type': 'List-item', 'bbox': [0.09351193596335018, 0.458897177956321, 0.906354549632353, 0.4817462713068182], 'properties': {'score': 0.7974112033843994, 'page_number': 57}}\n",
      "{'type': 'List-item', 'bbox': [0.09232407513786765, 0.4958868408203125, 0.9064801384420956, 0.5187485573508522], 'properties': {'score': 0.8064874410629272, 'page_number': 57}}\n",
      "{'type': 'List-item', 'bbox': [0.0930938092400046, 0.5326291170987216, 0.9056389303768382, 0.5805926513671875], 'properties': {'score': 0.8763754367828369, 'page_number': 57}, 'text_representation': 'Xing, Y., Shi, Z., Meng, Z., Lakemeyer, G., Ma, Y., & Wattenhofer, R. (2021). KM-BART: knowledge enhanced multimodal BART\\nfor visual commonsense generation. In C. Zong, F. Xia, W. Li, & R. Navigli (Eds.), Proceedings of the 59th Annual Meeting of the\\nAssociation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP,\\n(Volume 1: Long Papers), Virtual Event, August 1-6 (pp. 525–535). Association for Computational Linguistics.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09314307717715993, 0.5943376020951705, 0.9063614430147059, 0.6170121626420455], 'properties': {'score': 0.8509846925735474, 'page_number': 57}}\n",
      "{'type': 'List-item', 'bbox': [0.09313479255227482, 0.6309818892045455, 0.9071564797794117, 0.6539362127130682], 'properties': {'score': 0.8662694096565247, 'page_number': 57}}\n",
      "{'type': 'List-item', 'bbox': [0.09363612455480239, 0.6675265780362216, 0.9076622817095589, 0.69083740234375], 'properties': {'score': 0.7994041442871094, 'page_number': 57}, 'text_representation': 'Sciences, 12, 4158.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09386953914866727, 0.7046379505504261, 0.9066437126608455, 0.7273326526988636], 'properties': {'score': 0.8343202471733093, 'page_number': 57}}\n",
      "{'type': 'List-item', 'bbox': [0.0937458891027114, 0.7409885475852273, 0.9057553280101103, 0.7638508744673296], 'properties': {'score': 0.8759927153587341, 'page_number': 57}}\n",
      "{'type': 'List-item', 'bbox': [0.0934942626953125, 0.77783447265625, 0.906272403492647, 0.800850830078125], 'properties': {'score': 0.7807304859161377, 'page_number': 57}, 'text_representation': 'images. Expert Syst. Appl., Volume 213, Part B, 119024.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09365171544692096, 0.8143953080610795, 0.9055146340762867, 0.8374760298295455], 'properties': {'score': 0.8235106468200684, 'page_number': 57}, 'text_representation': 'Medicine, 150, 106115.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09351008695714615, 0.8509024325284091, 0.9051388729319852, 0.8865131170099432], 'properties': {'score': 0.8960825800895691, 'page_number': 57}, 'text_representation': 'Yeh, C., Mahadeokar, J., Kalgaonkar, K., Wang, Y., Le, D., Jain, M., Schubert, K., Fuegen, C., & Seltzer, M. L. (2019). Transformer-\\ntransducer: End-to-end speech recognition with self-attention. CoRR, abs/1910.12977. URL: http://arxiv.org/abs/1910.\\n12977. arXiv:1910.12977.\\n'}\n",
      "{'type': 'Text', 'bbox': [0.09417713838465074, 0.9006840376420454, 0.5520104262408089, 0.9109889914772727], 'properties': {'score': 0.5086295008659363, 'page_number': 57}, 'text_representation': 'Yu, D., & Deng, L. (2016). Automatic speech recognition volume 1. Springer.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09354728249942555, 0.9245824640447443, 0.9054426125919117, 0.9472157981178977], 'properties': {'score': 0.8432511687278748, 'page_number': 57}}\n",
      "{'type': 'List-item', 'bbox': [0.0934600830078125, 0.10309140985662286, 0.9072264188878676, 0.13896741000088778], 'properties': {'score': 0.7536620497703552, 'page_number': 58}, 'text_representation': 'Yu, S., Wang, X., & Langar, R. (2017). Computation offloading for mobile edge computing: A deep learning approach. In 28th IEEE\\nAnnual International Symposium on Personal, Indoor, and Mobile Radio Communications, PIMRC, Montreal, QC, Canada, October\\n8-13 (pp. 1–6). IEEE.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.0935458553538603, 0.151061748157848, 0.9063044289981618, 0.1871537225896662], 'properties': {'score': 0.757622480392456, 'page_number': 58}, 'text_representation': 'Yuan, L., Chen, D., Chen, Y., Codella, N., Dai, X., Gao, J., Hu, H., Huang, X., Li, B., Li, C., Liu, C., Liu, M., Liu, Z., Lu, Y., Shi, Y., Wang,\\nL., Wang, J., Xiao, B., Xiao, Z., Yang, J., Zeng, M., Zhou, L., & Zhang, P. (2021). Florence: A new foundation model for computer\\nvision. CoRR, abs/2111.11432. URL: https://arxiv.org/abs/2111.11432. arXiv:2111.11432.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09363117891199449, 0.19889672019264915, 0.9061936322380515, 0.23485143488103694], 'properties': {'score': 0.7766037583351135, 'page_number': 58}, 'text_representation': 'Yun, S., Jeong, M., Kim, R., Kang, J., & Kim, H. J. (2019). Graph transformer networks. In H. M. Wallach, H. Larochelle, A. Beygelzimer,\\nF. d’Alch´e-Buc, E. B. Fox, & R. Garnett (Eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural\\nInformation Processing Systems, NeurIPS, December 8-14, Vancouver, BC, Canada (pp. 11960–11970).\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09274989408605239, 0.24682914040305398, 0.9068800982306985, 0.2824419611150568], 'properties': {'score': 0.7700691819190979, 'page_number': 58}, 'text_representation': 'Zellers, R., Bisk, Y., Farhadi, A., & Choi, Y. (2019). From recognition to cognition: Visual commonsense reasoning. In IEEE Conference\\non Computer Vision and Pattern Recognition, CVPR, Long Beach, CA, USA, June 16-20 (pp. 6720–6731). Computer Vision Foundation\\n/ IEEE.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09304236916934742, 0.29489782159978695, 0.9065395938648897, 0.31760420365767045], 'properties': {'score': 0.8083584308624268, 'page_number': 58}}\n",
      "{'type': 'List-item', 'bbox': [0.0930472609576057, 0.3299587180397727, 0.9064463177849265, 0.3654684170809659], 'properties': {'score': 0.8207060694694519, 'page_number': 58}, 'text_representation': 'Zhang, J., Zhao, Y., Saleh, M., & Liu, P. J. (2020a). PEGASUS: pre-training with extracted gap-sentences for abstractive summarization.\\nIn Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July, Virtual Event (pp. 11328–11339).\\nPMLR volume 119 of Proceedings of Machine Learning Research.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09311941707835478, 0.37741776899857954, 0.9066228889016544, 0.41354223077947444], 'properties': {'score': 0.8338773250579834, 'page_number': 58}, 'text_representation': 'Zhang, Q., Lu, H., Sak, H., Tripathi, A., McDermott, E., Koo, S., & Kumar, S. (2020b). Transformer transducer: A streamable speech\\nIn IEEE International Conference on Acoustics, Speech and Signal\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09258347455193015, 0.42527815385298295, 0.9058857996323529, 0.47373468572443184], 'properties': {'score': 0.844271719455719, 'page_number': 58}, 'text_representation': 'Zhang, Y., Park, D. S., Han, W., Qin, J., Gulati, A., Shor, J., Jansen, A., Xu, Y., Huang, Y., Wang, S., Zhou, Z., Li, B., Ma, M., Chan, W.,\\nYu, J., Wang, Y., Cao, L., Sim, K. C., Ramabhadran, B., Sainath, T. N., Beaufays, F., Chen, Z., Le, Q. V., Chiu, C., Pang, R., & Wu, Y.\\n(2022). Bigssl: Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition. IEEE J. Sel. Top. Signal\\nProcess., 16, 1519–1532.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09329980289234835, 0.48573797052556816, 0.9058990119485294, 0.5088469904119318], 'properties': {'score': 0.7577639818191528, 'page_number': 58}}\n",
      "{'type': 'List-item', 'bbox': [0.0927268263872932, 0.5211692116477272, 0.9064012235753677, 0.5570870694247159], 'properties': {'score': 0.8550427556037903, 'page_number': 58}, 'text_representation': 'Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T., Torr, P. H. S., & Zhang, L. (2021). Rethinking\\nsemantic segmentation from a sequence-to-sequence perspective with transformers. In IEEE Conference on Computer Vision and Pattern\\nRecognition, CVPR, virtual, June 19-25 (pp. 6881–6890). Computer Vision Foundation / IEEE.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09299108168658088, 0.5689546342329546, 0.9057010426240809, 0.6046832275390625], 'properties': {'score': 0.8207105398178101, 'page_number': 58}, 'text_representation': 'Zheng, Y., Li, X., Xie, F., & Lu, L. (2020). Improving end-to-end speech synthesis with local recurrent neural network enhanced trans-\\nformer. In 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May 4-8,\\n2020 (pp. 6734–6738). IEEE.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09305328369140625, 0.6168277809836648, 0.905931827320772, 0.6399976695667614], 'properties': {'score': 0.8066617846488953, 'page_number': 58}}\n",
      "{'type': 'List-item', 'bbox': [0.09285932653090533, 0.6522005948153409, 0.9061473891314338, 0.6751963112571022], 'properties': {'score': 0.8387038111686707, 'page_number': 58}}\n",
      "{'type': 'List-item', 'bbox': [0.09291482364430147, 0.6872701748934659, 0.9070932904411765, 0.7105914306640625], 'properties': {'score': 0.8251208066940308, 'page_number': 58}}\n",
      "{'type': 'List-item', 'bbox': [0.09263863956227023, 0.7227804287997159, 0.9074618709788603, 0.7709769509055397], 'properties': {'score': 0.8517671823501587, 'page_number': 58}, 'text_representation': 'Zhu, X., Hu, H., Wang, H., Yao, J., Li, W., Ou, D., & Xu, D. (2022). Region aware transformer for automatic breast ultrasound tumor\\nsegmentation. In E. Konukoglu, B. H. Menze, A. Venkataraman, C. F. Baumgartner, Q. Dou, & S. Albarqouni (Eds.), International\\nConference on Medical Imaging with Deep Learning, MIDL, 6-8 July, Zurich, Switzerland (pp. 1523–1537). PMLR volume 172 of\\nProceedings of Machine Learning Research.\\n'}\n",
      "{'type': 'List-item', 'bbox': [0.09231781903435202, 0.7828274258700284, 0.9077789665670956, 0.8062521639737216], 'properties': {'score': 0.7361984848976135, 'page_number': 58}}\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "for doc in partitioned_docset.take_all():\n",
    "    for doci in doc.elements:\n",
    "        if doci.type == \"table\":\n",
    "            text +=  doci['table'].to_csv()\n",
    "        elif doci.text_representation:\n",
    "            text +=  doci.text_representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445147c2-4cbd-4a9e-8a2e-0576e869611e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/sycamore-monorepo-XJJYX36S-py3.10/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py:55\u001b[0m, in \u001b[0;36mdependable_faiss_import\u001b[0;34m(no_avx2)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m chunks \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_text(text)\n\u001b[1;32m     10\u001b[0m embedding \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings()\n\u001b[0;32m---> 11\u001b[0m faiss_index \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/sycamore-monorepo-XJJYX36S-py3.10/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py:931\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    930\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[0;32m--> 931\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/sycamore-monorepo-XJJYX36S-py3.10/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py:883\u001b[0m, in \u001b[0;36mFAISS.__from\u001b[0;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__from\u001b[39m(\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    882\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[0;32m--> 883\u001b[0m     faiss \u001b[38;5;241m=\u001b[39m \u001b[43mdependable_faiss_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m distance_strategy \u001b[38;5;241m==\u001b[39m DistanceStrategy\u001b[38;5;241m.\u001b[39mMAX_INNER_PRODUCT:\n\u001b[1;32m    885\u001b[0m         index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatIP(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/sycamore-monorepo-XJJYX36S-py3.10/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py:57\u001b[0m, in \u001b[0;36mdependable_faiss_import\u001b[0;34m(no_avx2)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import faiss python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install faiss-gpu` (for CUDA supported GPU) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `pip install faiss-cpu` (depending on Python version).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m faiss\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version)."
     ]
    }
   ],
   "source": [
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_overlap = 200,\n",
    "    chunk_size = 1000,\n",
    "    length_function = len \n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "faiss_index = FAISS.from_texts(chunks, embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae029c87-c20d-41a7-b437-54c6da8be46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_question = input()\n",
    "    docs = faiss_index.similarity_search(user_question, k=5)\n",
    "\n",
    "    llm = OpenAI()\n",
    "    chain = load_qa_chain(llm, chain_type= \"stuff\")\n",
    "    with get_openai_callback() as cb:\n",
    "        response = chain.run(input_documents=docs, question=user_question)\n",
    "        print(cb)\n",
    "        print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
