{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from typing import Optional\n",
    "from pyarrow.filesystem import FileSystem\n",
    "from pyarrow import fs\n",
    "from sycamore.connectors.file.file_scan import JsonManifestMetadataProvider\n",
    "from sycamore.functions import HuggingFaceTokenizer\n",
    "from sycamore.llms import OpenAI, OpenAIModels\n",
    "from sycamore.materialize_config import MaterializeSourceMode\n",
    "from sycamore.reader import DocSetReader\n",
    "from sycamore.transforms.embed import SentenceTransformerEmbedder\n",
    "from sycamore.transforms import COALESCE_WHITESPACE\n",
    "from sycamore.transforms.merge_elements import GreedyTextElementMerger\n",
    "from sycamore.transforms.partition import ArynPartitioner\n",
    "import sycamore\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "\n",
    "from ray.data import ActorPoolStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve AWS credentials to return S3 filesystem\n",
    "def get_s3_fs():\n",
    "    session = boto3.session.Session()\n",
    "    credentials = session.get_credentials()\n",
    "    from pyarrow.fs import S3FileSystem\n",
    "\n",
    "    fs = S3FileSystem(\n",
    "        secret_key=credentials.secret_key,\n",
    "        access_key=credentials.access_key,\n",
    "        region=session.region_name,\n",
    "        session_token=credentials.token,\n",
    "    )\n",
    "\n",
    "    return fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_client_args = {\n",
    "    \"hosts\": [{\"host\": \"localhost\", \"port\": 9200}],\n",
    "    \"http_compress\": True,\n",
    "    \"http_auth\": ('admin', 'admin'),\n",
    "    \"use_ssl\": True,\n",
    "    \"verify_certs\": False,\n",
    "    \"ssl_assert_hostname\": False,\n",
    "    \"ssl_show_warn\": False,\n",
    "    \"timeout\": 120\n",
    "}\n",
    "\n",
    "index_settings = {\n",
    "    \"body\": {\n",
    "        \"settings\": {\n",
    "            \"index.knn\": True,\n",
    "            \"number_of_shards\": 5,\n",
    "            \"number_of_replicas\": 1\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"embedding\": {\n",
    "                  \"dimension\": 768,\n",
    "                  \"method\": {\n",
    "                    \"engine\": \"faiss\",\n",
    "                    \"space_type\": \"l2\",\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"parameters\": {}\n",
    "                  },\n",
    "                  \"type\": \"knn_vector\"\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"financebench-etl\"\n",
    "manifest_path = \"/Users/aanyapratapneni/Documents/Aryn/manifest.json\" # Note: AWS credentials were not working for the S3 manifest path, so this is a local file\n",
    "\n",
    "hf_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "openai_llm = OpenAI(OpenAIModels.GPT_4O.value)\n",
    "tokenizer = HuggingFaceTokenizer(hf_model)\n",
    "embedder = SentenceTransformerEmbedder(model_name=hf_model, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "ctx = sycamore.init()\n",
    "reader = DocSetReader(ctx)\n",
    "ds = (\n",
    "    reader.manifest(binary_format=\"pdf\", metadata_provider=JsonManifestMetadataProvider(manifest_path), filesystem=get_s3_fs())\n",
    "    .partition(partitioner=ArynPartitioner(extract_table_structure=True, threshold=0.35, use_ocr=True))\n",
    "    .materialize(path=\"/Users/aanyapratapneni/Documents/Aryn/materialize\", source_mode=MaterializeSourceMode.IF_PRESENT)\n",
    "    .regex_replace(COALESCE_WHITESPACE)\n",
    "    .merge(merger=GreedyTextElementMerger(tokenizer, 512))\n",
    "    .spread_properties([\"path\", \"company\", \"year\", \"doc-type\"])\n",
    "    .explode()\n",
    "    .embed(embedder=embedder)\n",
    ")\n",
    "\n",
    "end = time()\n",
    "print(f\"Took {(end - start) / 60} mins\")\n",
    "\n",
    "###########################\n",
    "\n",
    "start = time()\n",
    "\n",
    "ds.write.opensearch(\n",
    "    os_client_args=os_client_args,\n",
    "    index_name=index,\n",
    "    index_settings=index_settings,\n",
    ")\n",
    "\n",
    "end = time()\n",
    "print(f\"Took {(end - start) / 60} mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "import datasets\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from sycamore.connectors.file.file_writer import JSONEncodeWithUserDict\n",
    "from sycamore.data import Element\n",
    "from sycamore.data.document import Document\n",
    "from sycamore.evaluation import EvaluationDataPoint\n",
    "from sycamore.evaluation.datasets import EvaluationDataSetReader\n",
    "from sycamore.evaluation.pipeline import EvaluationPipeline\n",
    "from sycamore.transforms.query import OpenSearchQueryExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV uploaded to s3://aryn-datasets-us-east-1/financebench/financebench_sample_150.csv\n",
    "# CSV of 10 questions is at https://www.notion.so/FinanceBench-Documents-f19756a506184caf8491d5ad54b29862?pvs=4#d0055e19ebd9443f877b62a069ddeba8\n",
    "hf_dataset = datasets.load_dataset(\"csv\", data_files='/Users/aanyapratapneni/Documents/Aryn/financebench_sample_10.csv', split=\"train\") # local path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year extraction\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_year(question, company):\n",
    "    pattern = r'\\bFY\\d{2}\\b|\\b\\d{4}\\b|\\bFY\\d{4}\\b'\n",
    "    yrs = (re.findall(pattern, question))\n",
    "\n",
    "    yrs = [yr[-2:] for yr in yrs]\n",
    "\n",
    "    year = ('20' + max(yrs)) if len(yrs) != 0 else ''\n",
    "    \n",
    "    return '' if not year else doc_exists(year, company)\n",
    "\n",
    "# check all documents to see if filters are valid\n",
    "def doc_exists(year, company):\n",
    "    df = hf_dataset.filter(lambda entry: entry[\"doc_name\"].startswith([company, year].join(\"_\")))\n",
    "    \n",
    "    return '' if len(df) == 0 else year\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert each question in FinanceBench to an EvaluationDataPoint for future query construction\n",
    "def _hf_to_qa_datapoint(datapoint: dict[str, Any]) -> dict[str, Any]:\n",
    "    document = EvaluationDataPoint()\n",
    "\n",
    "    page_numbers = [int(num.strip()) for num in datapoint[\"page_number\"].split(\",\")]\n",
    "\n",
    "    document.question = datapoint[\"question\"]\n",
    "    document.ground_truth_answer = datapoint[\"answer\"]\n",
    "    document.ground_truth_source_documents = [Element({\n",
    "        \"properties\": {\n",
    "            \"_location\": datapoint[\"doc_link\"],\n",
    "            \"page_numbers\": page_numbers\n",
    "        }\n",
    "    })]\n",
    "\n",
    "    company = datapoint[\"doc_name\"].split(\"_\")[0]\n",
    "\n",
    "    document.filters = {\n",
    "        \"properties.company\": company,\n",
    "    }\n",
    "    year = extract_year(document.question, company)\n",
    "    if year:\n",
    "        document.filters[\"properties.year\"] = year\n",
    "    \n",
    "    document[\"raw\"] = datapoint\n",
    "    return {\"doc\": document.serialize()}\n",
    "\n",
    "INDEX = \"financebench-etl\"\n",
    "\n",
    "if os.path.exists(\"/.dockerenv\"):\n",
    "    opensearch_host = \"opensearch\"\n",
    "    print(\"Assuming we are in a sycamore jupyter container, using opensearch for opensearch host\")\n",
    "else:\n",
    "    opensearch_host = \"localhost\"\n",
    "    print(\"Assuming we are running outside of a container, using localhost for opensearch host\")\n",
    "\n",
    "OS_CLIENT_ARGS = {\n",
    "    \"hosts\": [{\"host\": opensearch_host, \"port\": 9200}],\n",
    "    \"http_compress\": True,\n",
    "    \"http_auth\": (\"admin\", \"admin\"),\n",
    "    \"use_ssl\": True,\n",
    "    \"verify_certs\": False,\n",
    "    \"ssl_assert_hostname\": False,\n",
    "    \"ssl_show_warn\": False,\n",
    "    \"timeout\": 120,\n",
    "}\n",
    "\n",
    "OS_CONFIG = {\n",
    "    \"size\": 10,\n",
    "    \"neural_search_k\": 200,\n",
    "    \"embedding_model_id\": \"7-KAu5EBeZTJZhOyaQFc\",\n",
    "    \"search_pipeline\": \"hybrid_rag_pipeline\",\n",
    "    \"llm\": \"gpt-4o\",\n",
    "    \"context_window\": \"10\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/Users/aanyapratapneni/Documents/Aryn/myresultdir/etl.json\" # local path to save results\n",
    "\n",
    "reader = EvaluationDataSetReader(ctx)\n",
    "input_docset = reader.huggingface(hf_dataset, doc_extractor=_hf_to_qa_datapoint)\n",
    "\n",
    "data = {\n",
    "    \"experiment_name\": \"FinanceBench gpt-4o ocr + filters\",\n",
    "    \"description\": \"gpt-4o\",\n",
    "    \"created_by\": \"aanyapratapneni\",\n",
    "    \"index\": INDEX,\n",
    "    \"os_client_args\": OS_CLIENT_ARGS,\n",
    "    \"os_config\": OS_CONFIG,\n",
    "    \"qa_path\": [\"huggingface: PatronusAI/financebench\"]\n",
    "}\n",
    "\n",
    "pipeline = EvaluationPipeline(\n",
    "    index=INDEX,\n",
    "    os_config=OS_CONFIG,\n",
    "    metrics=[],\n",
    "    query_executor=OpenSearchQueryExecutor(OS_CLIENT_ARGS),\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "start = time()\n",
    "query_level_metrics = pipeline.execute(input_docset)[0]\n",
    "data[\"query_level_data\"] = query_level_metrics.take_all()\n",
    "data[\"evaluation_time\"] = f'{\"{:.2f}\".format(time() - start)} seconds'\n",
    "with open(output_path, \"w+\") as outfile:\n",
    "    json.dump(data, outfile, cls=JSONEncodeWithUserDict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sycamore-monorepo-E-M94jC3-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
